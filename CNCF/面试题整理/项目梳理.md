

# 技术栈

熟练运用 Cobra、Viper、Pflag 等构建企业级 CLI 应用框架，实现配置的集中化与多源化管理。

miniblog 项目为了方便日志记录，降低开发者理解日志记录方法的负担，只实现了结构化记录方法。基于zap并结合业务需求定制化开发日志包。统一了项目的日志规范，极大提升了代码的可读性与一致性。

实现了包含 HTTP/gRPC 状态码、业务错误码、错误信息及元数据的错误返回机制。此外，还为 ErrorX 提供了便捷的字段设置方法，方便开发者快速构造和返回错误。



具备大型项目性能调优经验：使用pprof定位协程泄漏、使用trace分析调度瓶颈、通过benchmark进行关键路径优化

悉CI/CD流程设计（GitLab CI/ArgoCD），具备基础设施即代码（Terraform）能力

熟练使用Prometheus+Grafana监控体系，具备Tracing（Jaeger）日志聚合（ELK）落地经验

**技术栈：Go+Gin+K8s+Istio+Redis+Kafka**

- 核心职责：
  - 主导订单和支付模块重构，设计基于Go的微服务架构，接口响应时间降低40%
  - 开发K8s Operator实现自动化扩缩容，支撑大促期间50万QPS流量
  - 通过Istio实现金丝雀发布和故障注入，降低生产环境发布风险30%
- 性能优化：
  - 利用pprof定位GC瓶颈，通过池化技术和sync.Pool减少60%内存分配
  - 设计Redis分布式锁+本地缓存二级架构，商品详情页TP99从200ms降至80ms



- [x] 梳理订阅安全产品的过程
- [x] 安全服务的基本介绍：UCWI、SWG、DSG、SEG、DSA、终端DLP。DLP的基本内容
- [x] 熟悉UCSS平台的基本内容
- [x] 扩缩容HPA

首先由云平台管理人员通过customer租户管理平台创建租户并填写基本信息，然后根据租户的订阅要求授权相应的安全服务及其规格，主要的安全服务有......

介绍 UCWI 时，由于它是提供RESTful接口，那流量是如何导入到集群中的呢？如何按每日查询数、查询并发数进行限流的呢？

```
OPS、UCSS、POP集群，LVS机器通过IPVS
IPVS - 内核空间的“数据平面”
ipvsadm - 用户空间的“控制平面”

go模板生成租户自己的消费者配置consumer（定义客户端如何认证和限速访问）、上游配置upstream
```

介绍SWG时，导流方式为NodePort。

介绍SEG时，导流方式为LVS到Node节点，然后MTA服务直接NodePort方式暴露。

介绍DSA时，数据发现通过SLAB内网穿透实现访问内网资源的能力。

```
1. 由内向外建立控制通道 (握手)
内网中的穿透客户端（如SLAB Agent）主动向位于公网的穿透服务器（中继服务器）发起一个持久的、稳定的TCP连接。因为这个连接是从内网“向外”建立的，它符合防火墙的出站规则，通常不会被阻拦。
这个连接一旦建立，就成为一个控制通道。客户端会通过这个通道告诉服务器：“你好，我在这里，我愿意为某个端口（比如映射到内网的AD服务的445端口）提供代理服务”。
2. 公网服务器作为中介 (代理)
穿透服务器拥有一个固定的公网IP和域名，它是一个双方都能访问的“中介”。
当公网客户端（如SASE云服务）需要访问企业内网资源时，它并不直接连接企业防火墙，而是去连接这个“中介”服务器。
3. 隧道转发与数据中继
穿透服务器收到公网客户端的请求后，立即通过之前建立好的控制通道，“通知”内网的穿透客户端：“现在有一个外部请求，需要你帮忙访问一下你内部的AD服务”。
内网穿透客户端收到指令后，在本机内部访问目标服务（如127.0.0.1:445或内网IP），获取响应。
最后，内网客户端将响应数据通过控制通道发回给公网服务器，公网服务器再最终返回给公网客户端。
整个过程，企业防火墙只看到一个从内网到指定公网服务器的出站SSL加密连接，没有任何端口暴露在公网上。公网客户端访问的也是这个公网服务器，而不是企业地址。
```



UCSS统一内容安全管理平台：

- 数据看板：系统健康状况（用户目录、URL分类更新、邮件服务器连接）、热点命中策略
- 报告：用户行为、网络事件、发现事件、终端事件
- 监控与日志：
- DLP管理、各种安全服务的配置管理、终端管理



sk-proxy-go、sk-service-controller、sk-gatorcloud-operator三者之间的联动，以及各个项目的特点与难点

## sk-proxy-go

```
import "k8s.io/apimachinery/pkg/util/wait"

func (c *TenantCacher) Start(ctx context.Context) {
	go wait.JitterUntilWithContext(ctx, c.sync, time.Duration(c.cfg.SyncTenantInfoInterval)*time.Second, 0.1, true)
}
帮我详细解释一下wait.JitterUntilWithContext的使用与原理

import "k8s.io/client-go/util/workqueue"
s.queue = workqueue.NewNamedRateLimitingQueue(workqueue.NewItemExponentialFailureRateLimiter(400*time.Millisecond, 10*time.Minute), "notify-task-queue")
帮我详细解释一下 workqueue.NewNamedRateLimitingQueue的使用与原理
```

- interface用于缓存租户的策略
  - interface实现了通知接口，因此SPS在租户策略变更之后，将会调用通知接口，interface即可实时获取租户策略
  - 另外，为了防止SPS的通知机制失败，interface做了定时拉取补偿机制，因此interface可以定时租户策略
- service用于对外提供接口，其它组件可以通过service服务获取租户策略
- interface还有一个很核心的功能，就是会缓存租户授权信息到ETCD当中，后续service-controller以及operator看到租户授权信息之后将会创建租户的授权产品（主要涉及etcd的增删改查）

重点解释两个队列：queue、syncQueue的使用，对于两种方式都是通过队列来实现生产者消费者模型的。



## sk-service-controller

用于监听ETCD，监听`/tenant/info`，与此同时对于所有的`/tenant/info/<tenant id>/rs/<product>`产品的变动都会创建/修改/删除对应的CR

集群之间通信需要通过APISIX的认证，根据popId、popCode、tenantId、时间戳（检查 `x-timestamp`是否在合理的时间窗口内（例如，与服务器时间相差不超过 ±5 分钟），以防止重放攻击）信息生成认证头。不管是HTTP还是GRPC可以通过APISIX的认证插件。通过apisix/plugins/token-auth.lua来自定义认证插件。

删除安全产品时，设置删除传播策略为：metav1.DeletePropagationOrphan，只删除父资源，其管理的所有子资源都会被保留，成为“孤儿”资源。

这通常基于一种特定的架构设计：

1. 一个“产品”或“租户”资源（由 `BaseProduct`管理）可能创建并管理着很多其他底层资源（如 Deployments, Services, ConfigMaps 等）。
2. 当删除这个“产品”时，你可能不希望立即删除所有这些底层资源。
3. 你可能有一个更高级别的控制器或清理流程，需要在这些资源成为“孤儿”后，再进行更复杂、更安全的清理操作。
4. 使用 `Orphan`策略可以避免误删，实现分阶段、可控的删除过程。

更新CR时：

| 特性           | 说明                                              | 优点                                                         |
| :------------- | :------------------------------------------------ | :----------------------------------------------------------- |
| 智能合并       | 使用三路合并算法计算差异                          | 大幅减少不必要的 API 调用，避免无意义的资源更新和控制器重建。 |
| 关注点分离     | 使用 `IgnoreStatusFields`忽略状态字段             | 避免因状态变化触发业务逻辑更新，只关注期望状态（spec）的变更。 |
| 元数据安全     | 精心保留 `resourceVersion`, `creationTimestamp`等 | 确保更新操作符合 Kubernetes 的并发控制要求，避免基础元数据被破坏。 |
| 注解合并       | 合并而非覆盖现有注解                              | 保证其他系统设置的注解不会丢失，体现了良好的协同工作设计。   |
| 优雅的错误处理 | 捕获特定错误并尝试删除后重建                      | 实现了自我修复。能够自动处理一些棘手的更新冲突（如字段不可变性验证失败），极大地增强了系统的鲁棒性。 |



```go
	// 它使用了一个patch库来计算 currObj（当前状态）和 newObj（目标状态）之间的差异
	// 它告诉差异计算器忽略 .status 字段的任何不同。我们只关心 spec 的变更，避免因状态字段的变化而触发不必要的更新。
	patchResult, err := patch.DefaultPatchMaker.Calculate(currObj, newObj, patch.IgnoreStatusFields())
	if err != nil {
		return errors.Wrapf(err, "calculate diff by three-way merge error")
	}
	// 如果计算出的差异是空的（patchResult.IsEmpty()），说明新的 spec与当前的 spec没有实质性区别
	if patchResult.IsEmpty() {
		s.Log.Infof("%s not modify, ingnoed", name)
		return nil
	}
	// 这类似于 kubectl apply的工作方式，在对象上设置一个注解（通常是 kubectl.kubernetes.io/last-applied-configuration）来记录本次更新的完整配置。
	// 这为未来的三路合并提供了“原始状态”。
	if err := patch.DefaultAnnotator.SetLastAppliedAnnotation(newObj); err != nil {
		return errors.Wrapf(err, "unable to patch %s with comparison object", name)
	}

三路合并引入了第三个关键的参考点：最初的共同版本（也叫“祖先”或“基础”版本）。
现在你有三个版本：
O (Original)：原始文件（你们最开始一起看的那个版本）
A (Your Version)：你的修改版
B (Friend‘s Version)：你朋友的修改版
算法的思路非常简单：
“看看你（A）在原始文件（O）基础上改了啥，再看看你朋友（B）在原始文件（O）基础上改了啥。只要你们修改的不是同一个地方，就把你们俩的修改合在一起。如果修改了同一个地方，那就冲突了，需要人工来处理。”

在你提供的 Patch方法中：
O (Original)：并不是一个真实存在的文件，而是由 LastAppliedAnnotation等机制推断出的“上一次应用的状态”。
A (Your Version)：currObj（Kubernetes 中当前的资源对象）。
B (Friend’s Version)：newObj（你希望资源变成的新状态）。
```



## sk-gatorcloud-operator

```shell
# swg这种产品Operator被部署到gator-cloud命名空间中，其中的各个组件被部署到租户的命名空间中
root@POP-LVS-172-30-3-242:~# kubectl get swgs -n gator-cloud
NAME          TENANTNAME     TENANTID   TENANTCODE   STORAGE   REPLICAS   VERSION   STARTTIME    ENDTIME      AGE    DESCRIPTION
swg-1000006   zhangyong      1000006    c5cb27eb     80Gi      13         202       2024-06-18   2028-12-31   280d   ca-init 容器修改
swg-1000009   yuhongyan      1000009    b078e46e     80Gi      2          200       2024-09-01   2028-09-03   406d   spe更新到3.10.0_133

# 产品的组件可共享、启动具有优先级
```

0、定义finalizer，添加删除逻辑hook

有 Finalizer：当你执行 `kubectl delete`时，Kubernetes API Server 只会做两件事：

1. 设置资源的 `deletionTimestamp`字段（标记为正在删除）。
2. 阻止资源被真正删除，直到所有 Finalizer 被移除。



1、根据 spec 中的 version 字段获取 产品配置大清单 productConfig。

2、检查产品配置中是否存在产品配置 如果存在，对当前产品依赖进行排序。

3、根据产品组件依赖的优先级进行分桶，相同优先级的资源可以同时创建 与此同时，渲染模板。

4、如果CR被删除，执行删除产品逻辑（资源已经被标记删除 `AND`本控制器的 Finalizer 还未被移除时，返回 `true`。）

```
删除产品，非产品级共享组件直接删除即可，若为产品级共享组件，则需判断是否有其它产品在使用该组件，若无，则直接删除
删除逻辑按照优先级倒序进行

非租户级共享资源，直接删除组件
租户级别共享资源，判断租户的引用计数是否为一，如果为一，那么删除资源，否则引用计数减一


```

5、更新CR以及组件资源状态

6、创建或者更新资源，一个桶中的资源可以一起创建，下一个桶的资源在创建之前必须要等到前一个桶中的所有资源全部Ready

```
创建后立即检测当前桶中的所有资源是否已经Ready, 这样做的好处时可以判断产品是否已经全部创建完成
如果当前桶的资源还未Ready，就丢到延时队列中

创建完成后，有一个后处理逻辑，可以创建apisix相关的路由信息
```





# OPS运维管理平台

项目描述： GatorCloud 项目旨在将天空卫视的核心产品 DLP（数据防泄漏） 能力从传统的物理设备销售模式
转变为基于云的服务模式。 在过去，天空卫士主要通过物理设备以容器化部署的方式向大型企业提供 DLP服务。 随着互联网的发展， 数据变得与来越重要，许多中小型企业也开始需要保护敏感数据的能力。GatorCloud 的目标就是利用公有云灵活性、 可扩展性、 按需付费等优势， 将天空卫士的 DLP能力提供给各种规模的企业。 无论是规模仅有十几人的小公司， 还是拥有数十万员工的大型企业， 都可以通过 GatorCloud 轻松获取 DLP 保护服务。  



多集群管理、多租户管理、安全服务管理

具体功能：集群基本信息管理、资源监控、**跨集群认证**、日志收集配置、Web 终端；租户的实时资源使用收集、各服务运行监控；安全产品服务**资源定义**、版本控制、接口测试；租户独立升级、降级安全产品服务；K8s 资源可视化展示、修改。

集成 Grafana，对产品服务进行监控可视化。上报业务监控指标到 Prometheus，结合 Grafana 实现服务异常告警。

集群问题排查，解决容器资源占用异常、容器性能差等问题。

> 资源限制不合理，没有考虑 pagecache，memory cgroup。request 太小
>
> ApiSix，不同集群，内存，oom，节点资源丰富。大量nginx进程，limit cpu，top，/proc/state。深入学习 cgroup，k8s cadvisor，lxcfs。

涉及技术：Client-go，Operator，Kubebuilder，K8s，Prometheus，Grafana，ApiSix，Kratos，Gorm，Wire，Grpc，Postgresql 等。

task-pop多副本导致的分布式锁问题





# Operator模块

 项目描述： 通过 Operator 封装租户产品业务逻辑， 实现租户不同 DLP 产品的订购、 修改、 取消订购等逻辑。
 技 术 栈： kubernetes + kubebuilder + operator + controller -runtime + apisix + etcd
 技术要点：
o 租户 DLP 产品订购、 修改、 取消订购以及产品到期问题
o 不同 DLP 产品容器依赖问题
o 不同 DLP 产品容器共享问题
o 后续不同 DLP 产品资源清单维护问题
o 后续不同 DLP 服务灰度升级、 回滚问题
o 不同 DLP 产品 apisix 路由维护问题  

# 私有化部署

响应一汽集团数字化转型战略，将传统硬件部署的UCWI-11000安全产品改造为Kubernetes云原生架构。实现从物理设备到容器化K8S部署的转型，满足客户对弹性伸缩、高可用和统一管理的核心需求。



原来采用docker-compose的方式部署，了解原来如何实现容器启动顺序、网络访问等原理



## 相关改造

appliance容器化改造

- 之前的设备注册流程需要人工操作，该流程需要自动化，依赖的信息可通过环境变量或者configmap的方式获取；注册成功后获取的证书等信息通过存入指定的存储让其他组件可以感知到并应用

各组件间调用方式修改（Cloud Team）

- 设备版是docker-compose方式部署，服务通过固定IP来调用。
- 各组件以单独的Pod来部署，通过暴露的service name来访问。
- 目前组件的依赖服务地址基本支持配置文件，并将依赖服务地址修改为对应服务的service name。

UCWI与UCSS的交互（Cloud Team）

- UCWI的apache会暴露到集群外，供外部服务访问。在设备注册时，将apache的访问地址上传到UCSS。
- 整个交互和当前设备版类似，改动较小。

容器依赖关系（Cloud Team）

- 组件之间存在依赖关系，在docker-compose中是可以控制启动顺序的，在pod中不行。
- 解决方案：可通过init container检查依赖pod的探针，就绪后才启动。

该方案可支持同一个K8S集群部署多套UCWI来支持扩容和负载均衡

- 使用 namespace 隔离，类似多个UCWI设备，保持原有逻辑。
- 目前方案为了简化部署，选择通过HostPort暴露，并选择一个work节点的IP作为对外访问的入口。具体选择集群中的哪一个节点，在安装部署时需要指定。
- 没有使用 NodePort 暴露，因为 NodePort 默认端口范围是 30000 以上，而 UCSS 固定了对 UCWI 的访问端口为 9443 和 5443，因此采用 HostPort 暴露。
- 每个 UCWI 实例需要绑定一个 work 节点，用于暴露 UCWI 服务网关，集群外通过此节点的 IP 访问 UCWI。

交付方式（Cloud Team）

- 通过导出镜像+部署文件+相关脚本
- 升级和bug修复：导出新的镜像+部署文件+相关脚本
- 镜像通过公网镜像仓库发布（后续镜像的更新由成都团队负责）
- 部署程序会通过公网镜像仓库拉取需要的镜像

## 具体实现

容器调度

1、appliance & ucwi 容器部署到同一个 Pod

appliance 访问 ucwi 时，由于 ucwi 授信 IP 功能，需要固定 appliance 访问时的客户端 IP，才能授信。因此将这两个容器放到同一个 Pod 里，appliance 通过 127.0.0.1 的方式去访问 ucwi，ucwi 只需要授信 127.0.0.1 即可。

2、appliance Pod（包含 ucwi 容器）需要调度到固定某个节点

通过 hostPort 的形式暴露 appliance 9443 端口，用于集群外的 ucss 访问。通过 hostPort 的形式暴露 ucwi 5443 端口，用于集群外的客户端访问。

3、dcrp 和 rde 容器部署到同一个 Pod

因为这两个容器内部通过 127.0.0.1 的形式相互访问，所以需要在同一个 network namespace。

4、spe & kvserver 容器部署到同一个 Pod

spe 和 kvserver 挂载了同一个PV，spe 运行时，会挂载 tmpfs 文件系统到该PV。因此 spe 和 kvserver 必须在同一个 Pod，并配置挂载传播特性，才能保证 spe 挂载的文件系统对 kvserver 可见。

5、appliance 部署后和 ucss 通信需要默认证书，此证书由成都团队以挂载文件的形式提供。后续 appliance 会申请新的证书进行覆盖。

## 代码实现

使用 pflag、viper、cobra 三个强大的 Go 包来构建应用的命令行选项、配置文件和应用启动框架。

version：给应用添加版本号打印功能：通过编译时指定 -ldflags -X importpath.name=value 参数，来为程序自动注入版本信息。

status：查看所有部署产品实例的相关信息，注意要读取产品命名空间下的config这个configmap，config中的data为config: |- xxxx，以此获得 网关对外地址：bs.Appliance.ServerOutIp。是否所有产品都是此种方式呢？

config：查看选择的产品实例的配置信息，也就是读取产品命名空间下的config这个configmap，config中的data为config: |- xxxx。

deploy：部署实例（创建、更新、删除、回滚），

创建过程：

- 校验命名空间（设备名、实例名）是否已经存在、校验node节点状态（污点、已被部署实例）Appliance.Node、校验Appliance.ServerOutIp通过部署nginx的hostNetwork方式、校验storageClass（当前并没有校验只是检查了kubectl version）、校验UcssAddr（当前并没有校验）。

- 创建命名空间，命名空间中增加annotation：device、status、product。

- 用户配置完成之后，将用户修改的配置存到命名空间中的configMap中。

- 生成部署的资源清单文件，然后将资源清单与配置文件一起进行备份，configmap名为config-backup，data有两个key：manifest-backup、config-backup。方便后期回滚。

- 创建完成之后更新命名空间的status注解为Running 运行中。

删除过程：

- 用户选择要删除的设备名
- 删除对应的命名空间
- 如果删除失败，更新相应的状态到命名空间的注解中

更新过程：

- 用户选择要更新的设备名。
- 合并之前保存的configmap配置与最新安装包中嵌入的配置，镜像使用嵌入配置文件，其余的使用configmap的配置。
- 将合并的配置存到命名空间中的configMap中。
- 生成部署的资源清单文件，然后将资源清单与配置文件一起进行备份，configmap名为config-backup，data有两个key：manifest-backup、config-backup。方便后期回滚。
- 创建完成之后更新命名空间的status注解为Running 运行中。

  回滚过程：

- 用户选择要回滚的设备名。
- 读取备份configmap中的manifest-backup资源清单。
- 创建完成之后更新命名空间的status注解为Running 运行中。



# ETCD

## watch机制

> https://cloud.tencent.com/developer/article/2485343

Watch 是 etcd 提供的一种订阅机制，允许客户端监听特定键或键前缀的变化。当被监听的键发生增删改操作时，etcd 会向所有订阅者发送事件通知。Watch 机制的核心特点包括：

- 实时性：变更发生后，几乎实时地通知客户端。
- 持久性：支持长时间监听，客户端可以持续接收事件。
- 高效性：仅传输变化的数据，减少网络开销。

etcd Watch机制的基本原理

1. 订阅：客户端向 etcd 发送 Watch 请求，指定要监听的键或前缀。
2. 监听：etcd 监控指定范围内的键值变化。
3. 通知：当有变更发生时，etcd 将事件推送给所有订阅者。

**实际应用场景**

在微服务架构中，服务的配置经常需要动态调整。通过 etcd 的 Watch 机制，服务可以实时感知配置的变化，无需重启即可应用新配置。

服务实例的上线、下线可以通过 etcd 注册与注销。通过 Watch 机制，其他服务可以实时获知服务实例的变动，动态调整调用策略。

在分布式环境下，协调多个实例的操作需要分布式锁的支持。通过 Watch 机制，客户端可以监听锁的释放，及时获取锁权限。

## 租约机制

在 etcd 中，租约是一个有时间限制的授权。你可以将一个或多个键（Key）绑定到一个租约上。当租约到期后，所有绑定到该租约上的键都会被 etcd 自动删除。

```shell
etcdctl lease grant 240
# etcd 成功创建了租约，并返回了租约的ID：5a578262b3f6f545。这个ID是后续操作这个租约的凭证。
lease 5a578262b3f6f545 granted with TTL(240s)

# 查询了 ID 为5a578262b3f6f545的租约的详细信息
etcdctl lease timetolive 5a578262b3f6f545
lease 5a578262b3f6f545 granted with TTL(240s), remaining(198s)

# 将键绑定到租约
etcdctl put --lease=5a578262b3f6f545 /abc  111
OK

# 对指定的租约进行续约
etcdctl lease keep-alive 5a578262b3f6f545
# 输出示例：（会持续阻塞并输出续约结果，直到你 Ctrl+C）
# lease 5a578262b3f6f545 keepalived with TTL(240)
# lease 5a578262b3f6f545 keepalived with TTL(240)
# ...
# lease keep-alive命令默认会一直阻塞运行并定期（通常是 TTL 的 1/3 时间）续约一次。对于程序来说，通常使用客户端的自动异步续约接口。

# 撤销指定租约
etcdctl lease revoke 5a578262b3f6f545
# 输出示例：
# lease 5a578262b3f6f545 revoked
# 此时再查询绑定在该租约上的键 /abc，会发现已经被删除了
etcdctl get /abc
# （无输出，键已不存在）

# 查看租约详情，并显示其绑定的所有键
etcdctl lease timetolive 5a578262b3f6f545 --keys
# 输出示例：（如果租约还存在且绑定了键）
# lease 5a578262b3f6f545 granted with TTL(240s), remaining(232s), attached keys([/abc])
#                                                              ^^^^^^^^^^^^^^^^
# 输出示例：（如果租约已过期或被撤销）
# lease 5a578262b3f6f545 already expired
```

**典型应用场景：**

1. 服务注册与发现：服务节点启动时，用租约注册到 etcd（例如 `key=/services/order/192.168.1.100`）。服务需要定期续约（心跳）。如果服务节点宕机，心跳停止，租约到期后 key 自动被删除，客户端就知道这个节点不可用了。
2. 分布式锁：获取锁时可以绑定一个租约，如果持有锁的客户端崩溃，能保证锁最终会被自动释放，防止死锁。



## MVCC实现



## 分布式锁

# CA认证中心



# 分布式任务调度


# 第1章 可靠、可扩展与可维护的应用系统

当今许多新型应用都属于数据密集型（data-intensive），而不是计算密集型(compute-intensive）。对于这些类型应用，CPU的处理能力往往不是第一限制性因素，关键在于数据量、数据的复杂度及数据的快速多变性。  

数据密集型应用通常也是基于标准模块构建而成，每个模块负责单一的常用功能。例如，许多应用系统都包含以下模块：  

- 数据库：用以存储数据，这样之后应用可以再次访问。
- 高速缓存：缓存那些复杂或操作代价昂贵的结果，以加快下一次访问。
- 索引：用户可以按关键字搜索数据并支持各种过滤。
- 流式处理：持续发送消息至另一个进程，处理采用异步方式。
- 批处理：定期处理大量的累积数据。

这些模块也许看上去习以为常，主要是因为这些“数据处理系统”（data systems）

已经做了非常漂亮的抽象，以至于我们已经习惯于拿来即用，而没有做太多深入地思考。由于目前业界已有很多数据方案可供选择，当需要构建一个新应用时，相信大多数开发者不会从头开始，例如写一个全新的数据存储引擎。  

但情况并非如此简单。确实已有很多数据库系统，但因为需求和设计目标的差异，个中精妙都不尽相同。缓存和索引方案与之类似。因此在构建某个特定应用时，我们总是需要弄清楚哪些组件、哪些方法最适合自己，并且当单个组件无法满足需求而必须组合使用时，总要面临更多的技术挑战。  

本书既介绍系统原理，也注重具体实践，通过二者结合的章节之旅慢慢展示如何构建数据密集型应用。我们还将探索现有工具的相同之处、差异所在，以及它们如何达成预设目标。  

在本章，我们将首先探讨所关注的核心设计目标：可靠、可扩展与可维护的数据系统。澄清本源，解析处理之道，建立后续章节所需的基本要点。在接下来的章节中，我们将层层推进，深入探讨系统设计时所面临的种种抉择与权衡。  



## 认识数据系统

我们通常将数据库、队列、高速缓存等视为不同类型的系统。虽然数据库和消息队列存在某些相似性，例如两者都会保存数据（至少一段时间），但他们却有着截然不同的访问模式，这就意味着不同的性能特征和设计实现。  

那么为什么本书将它们归为一大类即“数据系统”（data system）呢？  

首先，近年来出现了许多用于数据存储和处理的新工具。它们针对各种不同的应用场景进行优化，不适合再归为传统类型。例如，Redis 既可以用于数据存储也适用于消息队列，Apache Kafka作为消息队列也具备了持久化存储保证。系统之间的界限正在变得模糊。  

其次，越来越多的应用系统需求广泛，单个组件往往无法满足所有数据处理与存储需求。因而需要将任务分解，每个组件负责高效完成其中一部分，多个组件依靠应用层代码驱动有机衔接起来。  

举个例子，假定某个应用包含缓存层（例如Memcached）与全文索引服务器（如Elasticsearch或Solr），二者与主数据库保持关联，通常由应用代码负责缓存、索引与主数据库之间的同步，如图1-1所示（具体技术将在后面的章节中详细介绍）。  

<img src="image/image-20250521141118001.png" alt="image-20250521141118001" style="zoom:50%;" />

设计数据系统或数据服务时， 一定会碰到很多棘手的问题。例如，当系统内出现了局部失效时，如何确保数据的正确性与完整性？当发生系统降级（ degrade ）时，该如何为客户提供一致的良好表现？负载增加时，系统如何扩展？友好的服务API该如何设计？

影响数据系统设计的因素有很多，其中包括相关人员技能和经验水平、遗留系统依赖性、交付周期、对不同风险因素的容忍度、监管合规等。这些因素往往因时因地而异。本书将专注于对大多数软件系统都极为重要的三个问题：

- 可靠性 （Reliability）

当出现意外情况如硬件、软件故障、人为失误等，系统应可以继续正常运转：虽然性能可能有所降低，但确保功能正确。具体请参阅本章后面的“可靠性” 一节 。

- 可扩展性（Scalability）

随着规模的增长 ，例如数据量 、流量或复杂性，系统应以合理的方式来匹配这种增长，具体请参阅本章后面的 “可扩展性 ” 一节。

- 可维护性 （Maintainability）

随着时间的推移，许多新的人员参与到系统开发和运维， 以维护现有功能或适配新场景等，系统都应高效运转。具体请参阅本章后面的“可维护性”一节。

### **可靠性**

每个人脑子里都有一个直观的认识 ， 即什么意味着可靠或者不可靠。对于软件，典型的期望包括：

- 应用程序执行用户所期望的功能。
- 可以容忍用户出现错误或者不正确的软件使用方怯 。
- 性能可以应对典型场景 、 合理负载压力和数据量。
- 系统可防止任何未经授权的访问和滥用。

如果所有上述目标都要支持才算 “正常工作”，那么我们可以认为可靠性大致意味着 ：即使发生了某些错误，系统仍可以继续正常工作。

可能出错的事情称为错误（ faults ）或故障，系统可应对错误则称为容错（ fault-­tolerant ）或者弹性（ resilient ）。 前一个词略显误导 ：似乎暗示着系统可以容忍各种可能的故障类型，显然实际中这是不可能的。举一个夸张一些的例子，如果整个地球（及其上的所有服务器）都被黑洞吞噬，那么要在这个级别容错就意味着必须在宇宙范围内进行系统冗余。试想，这将是天价的预算。因此，容错总是指特定类型的故障 ，这样的系统才更有实际意义。

注意， 故障与失效（ failure ） 不完全一 致 。故障通常被定义为组件偏离其正常规格，而失效意味系统作为一个整体停止，无法向用户提供所需的服务。我们不太可能将故障概率降低到零，因此通常设计容错机制来避免从故障引发系统失效。本书将介绍在不可靠组件基础上构建可靠性系统的相关技术。

在这种容错系统中，用于测试目的，可以故意提高故障发生概率，例如通过随机杀死某个进程，来确保系统仍保持健壮。很多关键的bug实际上正是由于错误处理不当而造成的。通过这种故意引发故障的方式，来持续检验、测试系统的容错机制，增加对真实发生故障时应对的信心。 Netflix的 Chaos Monkey系统就是这种测试的典型例子。

虽然我们通常倾向于容忍故障而不是预防故障，但是也存在“预防胜于治疗”的情况，安全问题就是一例，例如 ： 如果攻击者破坏了系统并窃取了敏感数据，则该事件造成的影响显然无法被撤销。然而，本书主要针对那些影响可以被消除的故障类型，接下来详细介绍。

#### **硬件故障**

当我们考虑系统故障时，对于硬件故障总是很容易想到 ： 硬盘崩溃，内存故障，电网停电，甚至有人误拔掉了网线。任何与大型数据中心合作过的人都可以告诉你，当有很多机器时，这类事情迟早会发生。

有研究证明硬盘的平均无故障时间（ MTTF ）约为 10 ～ 50年。因此，在一个包括10 000个磁盘的存储集群中，我们应该预期平均每天有一个磁盘发生故障。

我们的第一个反应通常是为硬件添加冗余来减少系统故障率。 例如对磁盘配置RAID ，服务器配备双电源，甚至热插拔CPU， 数据中心添加备用电源、发电机等。当一个组件发生故障，元余组件可以快速接管，之后再更换失效的组件。这种方法可能并不能完全防止硬件故障所引发的失效，但还是被普遍采用，且在实际中也确实可以让系统不间断运行长达数年。

#### **软件错误**

我们通常认为硬件故障之间多是相互独立的： 一台机器的磁盘出现故障并不意味着另一台机器的磁盘也要失效。除非存在某种弱相关（例如一些共性原因，如服务器机架中的温度过高），否则通常不太可能出现大量硬件组件同时失效的情况。

另一类故障则是系统内的软件问题。这些故障事先更加难以预料，而且因为节点之间是由软件关联的，因而往往会导致更多的系统故障 。 例如：

- 由于软件错误，导致当输入特定值时应用服务器总是崩溃。 例如， 2012年6月30日发生闰秒，由于Linux 内核中的一个bug ，导致了很多应用程序在该时刻发生挂起。
- 一个应用进程使用了某些共享如CPU、内存、磁盘或网络带宽 ，但却不幸失控跑飞了。
- 系统依赖于某些服务，但该服务突然变慢，甚至无响应或者开始返回异常的响应。
- 级联故障，其中某个组件的小故障触发另一个组件故障，进而引发更多的系统问题。

导致软件故障的bug通常会长时间处于引而不发的状态，直到碰到特定的触发条件。这也意味着系统软件其实对使用环境存在某种假设，而这种假设多数情况都可以满足，但是在特定情况下，假设条件变得不再成立 。

软件系统问题有时没有快速解决办法，而只能仔细考虑很多细节，包括认真检查依赖的假设条件与系统之间交互 ，进行全面的测试，进程隔离，允许进程崩愤并自动重启，反复评估，监控并分析生产环节的行为表现等。如果系统提供某些保证，例如，在消息队列中，输出消息的数量应等于输入消息的数量， 则可以不断地检查确认 ，如发现差异则立即告警。

#### **人为失误**

设计和构建软件系统总是由人类完成， 也是由人来运维这些系统。即使有时意图是好的，但人却无法做到万无一失。例如，一项针对大型互联网服务的调查发现，运维者的配置错误居然是系统下线的首要原因，而硬件问题（服务器或网络）仅在10%～25% 的故障中有所影响。

#### **可靠性的重要性**

可靠性绝不仅仅针对的是核电站和空中交管软件之类的系统，很多应用都需要可靠工作。商业软件中的错误会导致效率下降（如数据报告错误，甚至带来法律风险），电子商务网站的暂停会对营收和声誉带来巨大损失。

即使在所谓“非关键”应用中，我们也应秉持对用户负责的态度。例如一对父母，将其所有的照片以及他们孩子的视频存放在你的照片应用中。 如果不幸发生了数据库损坏，他们的感受可想而知，他们是否知道该如何从备份数据来执行恢复？

当然， 也会存在其他一些情况，例如面对不太确定的市场开发原型系统，或者服务的利润微薄，有时也会牺牲一些可靠性来降低开发成本或者运营开销，对此， 我们总是建议务必三思后行。

### **可扩展性**

即使系统现在工作可靠 ， 并不意味着它将来一定能够可靠运转。发生退化的一个常见原因是负载增加：例如也许并发用户从最初的 10 000个增长到 100 000个，或从 100万到 1000万；又或者系统目前要处理的数据量超出之前很多倍。

可扩展性是用来描述系统应对负载增加能力的术语。但是请注意，它并不是衡量一个系统的一维指标， 谈论“ X是可扩展 ”或“ Y不扩展”没有太大意义。相反，讨论可扩展性通常要考虑这类问题 ： “如果系统以某种方式增长，我们应对增长的措施有哪些”， “我们该如何添加计算资源来处理额外的负载” 。

#### **描述负载**

首先，我们需要简洁地描述系统当前的负载 ， 只有这样才能更好地讨论后续增长问题（例如负载加倍会意味着什么）。负载可以用称为负载参数的若干数字来描述。参数的最佳选择取决于系统的体系结构，它可能是Web服务器的每秒请求处理次数，数据库中写入的比例 ，聊天室的同时活动用户数量， 缓存命中率等。 有时平均值很重要 ，有时系统瓶颈来自于少数峰值 。

#### **描述性能**

描述系统负载之后，接下来设想如果负载增加将会发生什么。有两种考虑方式：

- 负载增加，但系统资源（如CPU、内存、网络带宽等）保持不变，系统性能会发生什么变化？
- 负载增加，如果要保持性能不变，需要增加多少资源？

这两个问题都会关注性能指标，所以我们先简要介绍一下如何描述系统性能。

在批处理系统如Hadoop 中 ，我们通常关心吞吐量 （ throughput ），即每秒可处理的记录条数 ，或者在某指定数据集上运行作业所需的总时间的 ；而在线系统通常更看重服务的响应时间（ response time），即客户端从发送请求到接收响应之间的间隔 。

即使是反复发送、处理相同的请求，每次可能都会产生略微不同的响应时间。实际情况往往更复杂，由于系统要处理各种不同的请求，晌应时间可能变化很大。因此，最好不要将响应时间视为一个固定的数字，而是可度量的一种数值分布。

我们经常考察的是服务请求的平均响应时间（严格来说，术语“平均值”并没有明确采用何种具体公式，但通常被理解为算术平均值：给定n个值，将所有值相加，并除以n ） 。然而，如果想知道更典型的响应时间，平均值并不是合适的指标 ，因为它掩盖了一些信息，无法告诉有多少用户实际经历了多少延迟。

因此最好使用百分位数（ percentiles ）。如果已经搜集到了响应时间信息，将其从最快到最慢排序，中位数（ median ）就是列表中间的响应时间。例如，如果中位数响应时间为200 ms ，那意味着有一半的请求响应不到 200 ms ，而另一半请求则需要更长的时间。

中位数指标非常适合描述多少用户需要等待多长时间： 一半的用户请求的服务时间少于中位数响应时间，另一半则多于中位数的时间。因此中位数也称为50百分位数，有时缩写为p50 。请注意，中位数对应单个请求 ：这也意味着如果某用户发了多个请求，那么它们中至少一个比中位数慢的概率远远大于50% 。

......

#### **应对负载增加的方法**

我们已经讨论了描述负载的参数以及衡量性能的相关指标，接下来讨论可扩展性：即当负载参数增加时， 应如何保持良好性能？

首先， 针对特定级别负载而设计的架构不太可能应付超出预设目标10倍的实际负载 。如果目标服务处于快速增长阶段，那么需要认真考虑每增加一个数量级的负载，架构应如何设计。

现在谈论更多的是如何在垂直扩展（即升级到更强大的机器）和水平扩展（即将负载分布到多个更小的机器）之间做取舍。在多台机器上分配负载也被称为无共享体系结构。在单台机器上运行的系统通常更简单，然而高端机器可能非常昂贵，且扩展水平有限，最终往往还是无法避免需要水平扩展。实际上，好的架构通常要做些实际取舍，例如，使用几个强悍的服务器仍可以比大量的小型虚拟机来得更简单、便宜。

某些系统具有弹性特征，它可以自动检测负载增加，然后自动添加更多计算资源，而其他系统则是手动扩展（人工分析性能表现，之后决定添加更多计算）。如果负载高度不可预测，则自动弹性系统会更加高效 ，但或许手动方式可以减少执行期间的意外情况（参阅第6章的“分区再平衡”）。

把无状态服务分布然后扩展至多台机器相对比较容易，而有状态服务从单个节点扩展到分布式多机环境的复杂性会大大增加。出于这个原因，直到最近通常的做法一直是，将数据库运行在一个节点上（采用垂直扩展策略）， 直到高扩展性或高可用性的要求迫使不得不做水平扩展。

然而，随着相关分布式系统专门组件和编程接口越来越好 ， 至少对于某些应用类型来讲，上述通常做法或许会发生改变。可以乐观设想 ，即使应用可能并不会处理大量数据或流量，但未来分布式数据系统将成为标配。在本书后续部分，我们将介绍多种分布式数据系统，不仅可以帮助提高可扩展性， 也会提高易用性与可维护性。

超大规模的系统往往针对特定应用而高度定制，很难有一种通用的架构。背后取舍因素包括数据读取量、写入量、待存储的数据量、数据的复杂程度、 响应时间要求、访问模式等，或者更多的是上述所有因素的叠加，再加上其他更复杂的问题。

例如，即使两个系统的数据吞吐量折算下来是一样的，但是为每秒处理 100000次请求（每个大小为 1KB ）而设计的系统，与为每分钟3个请求（每个大小为 2GB ）设计的系统会大不相同。

对于特定应用来说，扩展能力好的架构通常会做出某些假设，然后有针对性地优化设计，如哪些操作是最频繁的，哪些负载是少数情况。如果这些假设最终发现是错误的，那么可扩展性的努力就白费了，甚至会出现与设计预期完全相反的情况。对于早期的初创公司或者尚未定型的产品，快速迭代推出产品功能往往比投入精力来应对不可知的扩展性更为重要。

可扩展架构通常都是从通用模块逐步构建而来，背后往往有规律可循，所以本书将会讨论这些通用模块和常见模式，希望对读者有所借鉴。

### **可维护性**

众所周知，软件的大部分成本并不在最初的开发阶段，而是在于整个生命周期内持续的投入，这包括维护与缺陷修复，监控系统来保持正常运行、故障排查、适配新平台、搭配新场景、技术缺陷的完善以及增加新功能等。

不幸的是，许多从业人根本不喜欢维护这些所谓的遗留系统，例如修复他人埋下的错误，或者使用过时的开发平台，或者被迫做不喜欢的工作。坦白说，每一个遗留系统总有其过期的理由，所以很难给出一个通用的建议该如何处理它们。

但是，换个角度，我们可以从软件设计时开始考虑，尽可能较少维护期间的麻烦，甚至避免造出容易过期的系统。为此，我们将特别关注软件系统的三个设计原则 ：

- 可运维性

方便运营团队来保持系统平稳运行。

- 简单性

简化系统复杂性，使新工程师能够轻松理解系统。注意这与用户界面的简单性并不一样。

- 可演化性

后续工程师能够轻松地对系统进行改进，并根据需求变化将其适配到非典型场景，也称为可延伸性、易修改性或可塑性。

与可靠性和可扩展性类似，实现上述这些目标也没有简单的解决方案。接下来，我们首先建立对这三个特性的理解。

#### **可运维性：运维更轻松**

有人认为， “良好的操作性经常可以化解软件的局限性，而不规范的操作则可以轻松击垮软件”。虽然某些操作可以而且应该是自动化的 ，但最终还是需要人来执行配置并确保正常工作。

运营团队对于保持软件系统顺利运行至关重要。 一个优秀的运营团队通常至少负责以下内容:

- 监视系统的健康状况，并在服务出现异常状态时快速恢复服务。
- 追踪问题的原因，例如系统故障或性能下降。
- 保持软件和平台至最新状态， 例如安全补丁方面。
- 了解不同系统如何相互影响，避免执行带有破坏性的操作 。
- 预测未来可能的问题，并在问题发生之前及时解决（例如容量规划）。
- 建立用于部署、配置管理等良好的实践规范和工具包 。
- 执行复杂的维护任务， 例如将应用程序从一个平台迁移到另一个平台。
- 当配置更改时，维护系统的安全稳健。
- 制定流程来规范操作行为，并保持生产环境稳定 。
- 保持相关知识的传承（如对系统理解），例如发生团队人员离职或者新员工加入等。

良好的可操作性意味着使日常工作变得简单，使运营团队能够专注于高附加值的任务。数据系统设计可以在这方面贡献很多， 包括 ：

- 提供对系统运行时行为和内部的可观测性，方便监控。
- 支持自动化， 与标准工具集成 。
- 避免绑定特定的机器，这样在整个系统不间断运行的同时，允许机器停机维护。
- 提供良好的文档和易于理解的操作模式，诸如“如果我做了X ，会发生Y”。
- 提供良好的默认配置，且允许管理员在需要时方便地修改默认值。
- 尝试自我修复，在需要时让管理员手动控制系统状态 。
- 行为可预测，减少意外发生。

#### **简单性：简化复杂度**

小型软件项目通常可以写出简单而漂亮的代码 ，但随着项目越来越大，就会越来越复杂和难以理解。这种复杂性拖慢了开发效率，增加了维护成本。一个过于复杂的软件项目有时被称为一个“大泥潭”。

复杂性有各种各样的表现方式 ： 状态空间的膨胀，模块紧搞合，令人纠结的相互依赖关系， 不一致的命名和术语，为了性能而采取的特殊处理，为解决某特定问题而引人的特殊框架等。在参考文献中有很多这方面的讨论。

复杂性使得维护变得越来越困难， 最终会导致预算超支和开发进度滞后。对于复杂的软件系统，变更而引人潜在错误的风险会显著加大，最终开发人员更加难以准确理解、评估或者更加容易忽略相关系统行为，包括背后的假设，潜在的后果，设计之外的模块交互等。相反 ，降低复杂性可以大大提高软件的可维护性，因此简单性应该是我们构建系统的关键目标之一 。

简化系统设计并不意味着减少系统功能，而主要意味着消除意外方面的复杂性，正如Moseley和Marks把复杂性定义为一种“意外”，即它并非软件固有、被用户所见或感知，而是实现本身所衍生出来的问题。

消除意外复杂性最好手段之一是抽象。 一个好的设计抽象可以隐藏大量的实现细节，并对外提供干净、易懂的接口。 一个好的设计抽象可用于各种不同的应用程序。这样，复用远比多次重复实现更有效率；另一方面，也带来更高质量的软件，而质量过硬的抽象组件所带来的好处，可以使运行其上的所有应用轻松获益。

然而，设计好的抽象还是很有挑战性。在分布式系统领域中，虽然已有许多好的算法可供参考，但很多时候我们并不太清楚究竟该如何利用他们，封装到抽象接口之中，最终帮助将系统的复杂性降低到可靠控的级别。本书我们将广泛考察如何设计好的抽象，这样至少能够将大型系统的一部分抽象为定义明确、可重用的组件。

#### **可演化性：易于改变**

一成不变的系统需求几乎没有，想法和目标经常在不断变化：适配新的外部环境，新的用例，业务优先级的变化，用户要求的新功能，新平台取代旧平台，法律或监管要求的变化，业务增长促使架构的演变等。

在组织、流程方面 ，敏捷开发模式为适应变化提供了很好的参考。敏捷社区还发布了很多技术工具和模式，以帮助在频繁变化的环境中开发软件，例如测试驱动开发(TDD)和重构。

这些敏捷开发技术目前多数还只是针对小规模、本地模式（例如同一应用程序中的几个源代码文件）环境。本书将探索在更大的数据系统层面上提高敏捷性，系统由多个不同特性的应用或者服务协作而成 。 

我们的目标是可以轻松地修改数据系统，使其适应不断变化的需求，这和简单性与抽象性密切相关 ： 简单易懂的系统往往比复杂的系统更容易修改。这是一个非常重要的理念，我们将采用另一个不同的词来指代数据系统级的敏捷性 ， 即可演化性。

### **小结**

这一章我们探讨了一些关于数据密集型应用的基本原则，这些原则将指导如何阅读本书的其余部分。

一个应用必须完成预期的多种需求，主要包括功能性需求（即应该做什么，比如各种存储、检索、搜索和处理数据）和一些非功能性需求（ 即常规特性、例如安全性 、可靠性、合规性、可伸缩性、兼容性和可维护性） 。本章我们着重梳理讨论了可靠性、可扩展性和可维护性。

可靠性意味着即使发生故障，系统也可以正常工作。故障包括硬件（通常是随机的，不相关的）、软件（缺陷通常是系统的，更加难以处理）以及人为（总是很难避免时不时会出错）方面。容错技术可以很好地隐藏某种类型故障，避免影响最终用户。

可扩展性是指负载增加时， 有效保持系统性能的相关技术策略。为了讨论可扩展性，我们首先探讨了如何定量描述负载和性能。对于可扩展的系统，增加处理能力的同时，还可以在高负载情况下持续保持系统的高可靠性。

可维护性则意味着许多方面，但究其本质是为了让工程和运营团队更为轻松。良好的抽象可以帮助降低复杂性， 并使系统更易于修改和适配新场景。良好的可操作性意味着对系统健康状况有良好的可观测性和有效的管理方法。

然而知易行难，使应用程序可靠、可扩展或可维护并不容易。考虑到一些重要的模式和技术在很多不同应用中普遍适用，在接下来的几章中，我们就一些数据密集系统例子，分析它们如何实现上述这些目标。

# 第2章 数据模型与查询语言





# 第3章 数据存储与检索

从最基本的层面看，数据库只需做两件事情 ： 向它插入数据时，它就保存数据：之后查询时，它应该返回那些数据。

在第2章中，我们讨论了数据模型和查询语言，即关于应用开发人员向数据库指明数据格式并在之后如何查询的机制。本章我们主要从数据库的角度再来探讨同样的问题，即如何存储输入的数据，并在收到查询请求时，怎样重新找到数据。

作为一名应用系统开发人员，为什么要关注数据库内部的存储和检索呢？首先，你不太可能从头开始实现一套自己的存储引擎，往往只需要从众多现有的存储引擎中选择一个适合自己应用的存储引擎。 因此，为了针对你特定的工作负载而对数据库调优时，最好对存储引擎的底层机制有一个大概的了解。

特别地，针对事务型工作负载和针对分析型负载的存储引擎优化存在很大的差异。本章“事务处理与分析处理”和“面向列的存储”部分，将讨论一系列针对分析型进行优化的存储引擎。

我们首先讨论存储引擎 ，这些存储引擎用于大家比较熟悉的两种数据库，即传统的关系数据库和大多数所谓的NoSQL数据库。我们将研究两个存储引擎家族 ，即日志结构的存储引擎和面向页的存储引擎，比如B-tree 。

### **数据库核心：数据结构**

我们来看一个世界上最简单的数据库， 它由两个Bash 函数实现：

\#!/bin/bash db_set () {  echo "$1,$2" >> database } db_get () {  grep "^$1," database | sed -e "s/^$1,//" | tail -n 1 }

这两个函数实现了键值存储的功能。执行 db_set key value 会将 键（key） 和 值（value） 存储在数据库中。键和值（几乎）可以是你喜欢的任何东西，例如，值可以是 JSON 文档。然后调用 db_get key 会查找与该键关联的最新值并将其返回。

麻雀虽小，五脏俱全：

$ db_set 123456 '{"name":"London","attractions":["Big Ben","London Eye"]}' $ db_set 42 '{"name":"San Francisco","attractions":["Golden Gate Bridge"]}' $ db_get 42 {"name":"San Francisco","attractions":["Golden Gate Bridge"]}

底层的存储格式非常简单：一个文本文件，每行包含一条逗号分隔的键值对。每次对 db_set 的调用都会向文件末尾追加记录，所以每次更新键的时候旧版本的值不会被覆盖 —— 因而查找最新值的时候，需要找到文件中键最后一次出现的位置（因此 db_get 中使用了 tail -n 1 )。

$ db_set 42 '{"name":"San Francisco","attractions":["Exploratorium"]}' $ db_get 42 {"name":"San Francisco","attractions":["Exploratorium"]} $ cat database 123456,{"name":"London","attractions":["Big Ben","London Eye"]} 42,{"name":"San Francisco","attractions":["Golden Gate Bridge"]} 42,{"name":"San Francisco","attractions":["Exploratorium"]}

db_set 函数对于极其简单的场景其实有非常好的性能，因为在文件尾部追加写入通常是非常高效的。与 db_set 相似，许多数据库内部都使用日志（log），日志是一个仅支持追加（append-only）式更新的数据文件。虽然真正的数据库有更多更为复杂的问题需要考虑（例如并发控制、回收硬盘空间以控制日志文件大小、处理错误与部分完成写记录等），但基本原理是相同的。日志机制极其有用，我们还将在本书的其它部分多次提到它。

日志这个词通常指的是应用程序的运行输出日志，来记录发生了什么事情。在本书中，日志则是一个更为通用的含义，表示一个仅能追加的记录序列集合。它可能是人类不可读的，可能是二进制格式而只能被其他程序来读取。

另一方面，如果日志文件保存了大量的记录，那么 db_get 函数的性能会非常差。每次想查找一个键，db_get 必须从头到尾扫描整个数据库文件来查找键的出现位置。在算法术语中，查找的开销是 O(n） ，即如果数据库的记录条数加倍，则查找需要两倍的时间。这一点并不好。

为了高效地查找数据库中特定键的值 ， 需要新的数据结构： 索引。在本章中，我们将介绍一些索引结构并对它们进行比较 ；它们背后的基本想法都是保留一些额外的元数据，这些元数据作为路标，帮助定位想要的数据。如果希望用几种不同的方式搜索相同的数据，在数据的不同部分，我们可能定义多种不同的索引。

索引是基于原始数据派生而来的额外数据结构。很多数据库允许单独添加和删除索引，而不影响数据库的内容，它只会影响查询性能。维护额外的结构势必会引入开销，特别是在新数据写入时。对于写入，它很难超过简单地追加文件方式的性能，因为那已经是最简单的写操作了。由于每次写数据时，需要更新索引，因此任何类型的索引通常都会降低写的速度。

这里涉及存储系统中重要的权衡设计 ： 适当的索引可以加速读取查询，但每个索引都会减慢写速度。为此，默认情况下，数据库通常不会对所有内容进行索引 ，它需要应用开发人员或数据库管理员，基于对应用程序典型查询模式的了解，来手动选择索引。目的是为应用程序提供最有利加速的同时，避免引入过多不必要的开销 。

#### **哈希索引**

让我们从 键值数据（key-value Data） 索引开始。这不是你可以构建索引的唯一数据类型，但键值数据是很常见的。而且是其他更复杂索引的基础构造模块。

键值存储与大多数编程语言所内置的字典结构非常相似，通常采用hash map (或者hash table，哈希表)来实现。许多算法资料中都介绍过hash map，所以这里不再详细介绍它们工作的细节。既然我们已经可以用散列映射来表示内存中的数据结构，为什么不使用它来索引硬盘上的数据呢？

假设数据存储全部采用追加式文件组成，如之前的例子所示。那么最简单的索引策略就是:保存内存中的hash map，把每个键一一映射到数据文件中特定的字节偏移量这样就可以找到每个值的位置，如图3-1所示。每当在文件中追加新的key-value对时，还要更新hash map来反映刚刚写入数据的偏移量 （包括插入新的键和更新已有的键）。 当查找某个值时，使用 hash map来找到文件中的偏移量 ，即存储位置，然后读取其内容 。

​    ![0](https://note.youdao.com/yws/res/18447/WEBRESOURCE4d9f42910732ea7c44be44c4cf1fa0f0)

这听起来可能过于简单，但它的确是一个可行的方法 。事实上，这就是Bitcask (Riak中的默认存储引擎) 所采用的核心做法。 Bitcask可以提供高性能的读和写，只要所有的key可以放入内存（因为hash map需要保存在内存中） 。 而value数据量则可以超过内存大小，只需一次磁盘寻址，就可以将value从磁盘加载到内存。如果那部分数据文件已经在文件系统的缓存中，则读取根本不需要任何的磁盘I/O 。

像Bitcask这样的存储引擎非常适合每个键的值频繁更新的场景。例如， key可能是某个关于猫的视频URL, value是它播放的次数（每次有人单击播放按钮时就增加）。对于这种工作负载，有很多写操作，但是没有太多不同的key ，即每个key都有大量的写操作，但将所有key保存在内存中是可行的。

如上所述，只追加到一个文件，那么如何避免最终用尽磁盘空间？ 一个好的解决方案是将日志分解成一定大小的段，当文件达到一定大小时就关闭它，并将后续写入到新的段文件中。然后可以在这些段上执行压缩，如图 3-2所示 。 压缩意味着在日志中丢弃重复的键，并且只保留每个键最近的更新。

​    ![0](https://note.youdao.com/yws/res/18467/WEBRESOURCE13d33711c6089b7e79e538a6d7d48506)

此外，由于压缩往往使得段更小（假设键在段内被覆盖多次），也可以在执行压缩的同时将多个段合并在一起，如图 3-3 所示。由于段在写入后不会再进行修改，所以合并的段会被写入另一个新的文件。对于这些冻结段的合并和压缩过程可以在后台线程中完成，而且运行时，仍然可以用旧的段文件继续正常读取和写请求。当合并过程完成后，将读取请求切换到新的合并段上，而旧的段文件可以安全删除 。

​    ![0](https://note.youdao.com/yws/res/18472/WEBRESOURCEce07911844053f34fed532a2bdc9af54)

每个段现在都有自己的内存哈希表 ，将键映射到文件的偏移量。 为了找到键的值，首先检查最新的段的 hash map ；如果键不存在，检查第二最新的段，以此类推。由于合并过程可以维持较少的段数量 ，因此查找通常不需要检查很多 hash map 。还有很多细节方面的考虑才能使得这个简单的想法在实际中行之有效。简而言之，在真正地实现中有以下重要问题 ：

- 文件格式

csv不是日志的最佳格式 。 更快更简单的方法是使用二进制格式，首先以字节为单位来记录字符串的长度，之后跟上原始字符串（不需要转义）。

- 删除记录

如果要删除键和它关联的值，则必须在数据文件中追加一个特殊的删除记录（有时候称为墓碑）。当合并日志段时， 一旦发现墓碑标记，则会丢弃这个己删除键的所有值 。

- 崩溃恢复

如果数据库重新启动，则内存中的hash map将丢失 。原则上，可以通过从头到尾读取整个段文件，然后记录每个键的最新值的偏移量，来恢复每个段的 hash map 。 但是， 如果分段文件很大，可能扫描需要很长时间，这将使服务器重启变得缓慢。 Bitcask通过将每个段的 hash map的快照存储在磁盘上，可以更快地加载到内存中，以此加快恢复速度。

- 部分写入的记录

数据库随时可能崩溃，包括将记录追加到日志的过程中。 Bitcask文件包括校验值，这样可以发现损坏部分并丢弃。

- 并发控制

由于写入以严格的先后顺序追加到日志中，通常的实现选择是只有一个写线程。数据文件段是追加的， 并且是不可变的， 所以他们可以被多个线程同时读取。

一个追加的日志乍看起来似乎很浪费空间 ： 为什么不原地更新文件，用新值覆盖旧值？但是，结果证明追加式的设计非常不错，主要原因有以下几个 ：

- 追加和分段合并主要是顺序写，它通常比随机写入快得多，特别是在旋转式磁性硬盘上。在某种程度上，顺序写入在基于闪存的固态硬盘（ solid state drives,SSD ）上也是适合的。 我们将在本章后面的“比较B-tree和LSM-Trees”部分进一步讨论此问题 。
- 如果段文件是追加的或不可变的，则并发和崩溃恢复要简单得多。例如，不必担心在重写值时发生崩溃的情况，留下一个包含部分旧值和部分新值混杂在一起的文件。
- 合并旧段可以避免随着时间的推移数据文件出现碎片化的问题 。

但是，哈希表索引也有其局限性 ：

- 哈希表必须全部放入内存，所以如果有大量的键，就没那么幸运了。原则上，可以在磁盘上维护 hash map ，但不幸的是，很难使磁盘上的 hash map表现良好。它需要大量的随机访问 I/O ，当哈希变满时，继续增长代价昂贵，井且哈希冲突时需要复杂的处理逻辑。
- 区间查询效率不高。例如，不能简单地支持扫描kitty0000和 kitty9999 区间内的所有键，只能采用逐个查找的方式查询每一个键 。

在下一节中，我们将看到摆脱这些限制的其他索引结构。

#### **SSTables和LSM-Tree**

在图3-3中，每个日志结构的存储段都是一组key-value对的序列。这些 key-value对按照它们的写入顺序排列，并且对于出现在 日志中的同一个键，后出现的值优于之前的值。除此之外，文件中 key-value对的顺序并不重要。

现在简单地改变段文件的格式：要求key-value对的顺序按键排序。乍一看，这个要求似乎打破了顺序写规则，我们稍后会解释。

这种格式称为排序字符串表，或简称为SSTable。它要求每个键在每个合并的段文件中只能出现一次（压缩过程已经确保了）。 SSTable相比哈希索引的日志段，具有以下优点：

1、合并段更加简单高效，即使文件大于可用内存。方法类似于合并排序算法中使用的方法，如图3-4所示。并发读取多个输入段文件，比较每个文件的第一个键，把最小的键（根据排序顺序）拷贝到输出文件，并重复这个过程。这会产生一个新的按键排序的合并段文件。如果相同的键出现在多个输入段怎么办？请记住，每个段包含在某段时间内写入数据库的所有值。这意味着一个输入段中的所有值肯定比其他段中的所有值更新（假设总是合并相邻的段）。当多个段包含相同的键时，可以保留最新段的值，并丢弃旧段中的值。

2、在文件中查找特定的键时，不再需要在内存中保存所有键的索引。 以图3-5为例，假设正在查找键handiwork ，且不知道该键在段文件中的确切偏移。但是，如果知道键handbag和键handsome的偏移量 ，考虑到根据键排序，则键handiwork一定位于它们两者之间。这意味着可以跳到handbag的偏移，从那里开始扫描，直到找到handiwork。

所以，仍然需要一个内存索引来记录某些键的偏移，但它可以是稀疏的，由于可以很快扫描几千字节，对于段文件中每几千字节，只需要一个键就足够了。

​    ![0](https://note.youdao.com/yws/res/18550/WEBRESOURCE805a1f0827161220eee9e7bca03c9a4e)

​    ![0](https://note.youdao.com/yws/res/18552/WEBRESOURCEf9c86d4b026102e6b0759bd4dd4e63df)

3、由于读请求往往需要扫描请求范围内的多个key value对，可以考虑将这些记录保存到一个块中并在写磁盘之前将其压缩（如图 3-5 中阴影区域所示）。然后稀疏内存索引的每个条目指向压缩块的开头。除了节省磁盘空间，压缩还减少了I/O带宽的占用。

**构建和维护 SSTables**

到目前为止，似乎一切还算顺利，但是，考虑到写入可能以任意顺序出现，首先该如何让数据按键排序呢？在磁盘上维护排序结构是可行的（参阅本章后面的“ B-trees”），不过，将其保存在内存中更容易。内存排序有很多广为人知的树状数据结构，例如红黑树或AVL树。使用这些数据结构，可以按任意顺序插入键并以排序后的顺序，读取它们 。

存储引擎的基本工作流程如下 ：

- 当写入时，将其添加到内存中的平衡树数据结构中（例如红黑树）。这个内存中的树有时被称为内存表。
- 当内存表大于某个阈值（通常为几兆字节）时，将其作为 SSTable 文件写入磁盘。由于树已经维护了按键排序的 key - value对， 写磁盘可以比较高效。新的SSTable文件成为数据库的最新部分。当 SSTable写磁盘的同时 ，写入可以继续添加到一个新的内存表实例 。
- 为了处理读请求，首先尝试在内存表中查找键，然后是最新的磁盘段文件，接下来是次新的磁盘段文件，以此类推，直到找到目标（或为空）。
- 后台进程周期性地执行段合并与压缩过程，以合并多个段文件，并丢弃那些已被覆盖或删除的值 。

上述方案可以很好地工作。但它还存在一个问题 ： 如果数据库崩溃，最近的写入（在内存表中但尚未写入磁盘）将会丢失。为了避免该问题，可以在磁盘上保留单独的日志，每个写入都会立即追加到该日志，就像上一节所述。那个日志文件不需要按键排序，这并不重要，因为它的唯一目的是在崩溃后恢复内存表。每当将内存表写入SSTable时，相应的日志可以被丢弃。

**从SSTables到LSM-Tree**

这里描述的算法本质上是LevelDB 和RocksDB 所使用的，主要用于嵌入到其他应用程序的 key-value 存储引擎库。除此之外，LevelDB可以在Riak中用作Bitcask的替代品。在Cassandra和HBase中使用了类似的存储引擎，这两种引擎都受到了Google的Bigtable论文（它引入了 SSTable和内存表这两个术语）的启发。	

最初这个索引结构 由Patrick O'Neil等人以日志结构的合井树（ Log-Structured MergeTree ，或LSM-Tree) [IOJ命名，它建立在更早期的日志结构文件系统之上 l 门 l 。因此，基于合井和压缩排序文件原理的存储引擎通常都被称为LSM存储引擎。

#### **B-trees**

目前讨论的日志结构索引正在逐渐受到更多的认可，但它们还不是最常见的索引类型。最广泛使用的索引结构是另一种不同的：B-tree。  

B-tree始见于1970年[17]，不到十年便被冠以“普遍存在”[18]，B-tree经受了长久的时间考验。时至今日，它仍然是几乎所有关系数据库中的标准索引实现，许多非关系型数据库也经常使用。  

像SSTable一样，B-tree保留按键排序的key-value对，这样可以实现高效的key-value查找和区间查询。但相似仅此而已：B-tree本质上具有非常不同的设计理念。  

之前看到的日志结构索引将数据库分解为可变大小的段，通常大小为几兆字节或更大，并且始终按顺序写入段。相比之下，B-tree将数据库分解成固定大小的块或页，传统上大小为4KB（有时更大），页是内部读/写的最小单元。这种设计更接近底层硬件，因为磁盘也是以固定大小的块排列。  

每个页面都可以使用地址或位置进行标识，这样可以让一个页面引用另一个页面，类似指针，不过是指向磁盘地址，而不是内存。可以使用这些页面引用来构造一个树状页面，如图3-6所示。  

某一页被指定为B-tree的根；每当查找索引中的一个键时，总是从这里开始。该页面包含若干个键和对子页的引用。每个孩子都负责一个连续范围内的键，相邻引用之间的键可以指示这些范围之间的边界。  

<img src="image/image-20250604212104407.png" alt="image-20250604212104407" style="zoom:67%;" />

在图3-6的例子中，假定正在查找键251，因此需要沿着 $2 0 0 { \sim } 3 0 0$ 间的页引用，到达类似的页，它进一步将200\~300范围分解成子范围。最终，我们到达一个包含单个键的页（叶子页），该页包含每个内联键的值或包含可以找到值的页的引用。  

B-tree中一个页所包含的子页引用数量称为分支因子。例如，在图3-6中，分支因子为6。在实际中，分支因素取决于存储页面引用和范围边界所需的空间总量，通常为几百个。  

如果要更新B-tree中现有键的值，首先搜索包含该键的叶子页，更改该页的值，并将页写回到磁盘（对该页的任何引用仍然有效）。如果要添加新键，则需要找到其范围包含新键的页，并将其添加到该页。如果页中没有足够的可用空间来容纳新键，则将其分裂为两个半满的页，并且父页也需要更新以包含分裂之后的新的键范围，如图3-7所示注2。 

<img src="image/image-20250604212201178.png" alt="image-20250604212201178" style="zoom:67%;" />

该算法确保树保持平衡：具有n个键的B-tree总是具有 $o$ （log $n$ ）的深度。大多数数据库可以适合3\~4层的B-tree，因此不需要遍历非常深的页面层次即可找到所需的页（分支因子为500的4KB页的四级树可以存储高达256TB）。

**使B-tree可靠**

B-tree底层的基本写操作是使用新数据覆盖磁盘上的旧页。它假设覆盖不会改变页的磁盘存储位置，也就是说，当页被覆盖时，对该页的所有引用保持不变。这与日志结构索引（如LSM-tree）形成鲜明对比，LSM-tree仅追加更新文件（并最终删除过时的文件），但不会修改文件。  

可以认为磁盘上的页覆盖写对应确定的硬件操作。在磁性硬盘驱动器上，这意味着将磁头首先移动到正确的位置，然后旋转盘面，最后用新的数据覆盖相应的扇区。对于SSD，由于SSD必须一次擦除并重写非常大的存储芯片块，情况会更为复杂。  

此外，某些操作需要覆盖多个不同的页。例如，如果插入导致页溢出，因而需分裂页，那么需要写两个分裂的页，并且覆盖其父页以更新对两个子页的引用。这是个比较危险的操作，因为如果数据库在完成部分页写入之后发生崩溃，最终会导致索引破坏（例如，可能有一个孤儿页，没有被任何其他页所指向）。  

为了使数据库能从崩溃中恢复，常见B-tree的实现需要支持磁盘上的额外的数据结构：预写日志（write-aheadlog，WAL），也称为重做日志。这是一个仅支持追加修改的文件，每个B-tree的修改必须先更新WAL然后再修改树本身的页。当数据库在崩溃后需要恢复时，该日志用于将B-tree恢复到最近一致的状态[5,20]。  

原地更新页的另一个复杂因素是，如果多个线程要同时访问B-tree，则需要注意并发控制，否则线程可能会看到树处于不一致的状态。通常使用锁存器（轻量级的锁）保护树的数据结构来完成。在这方面，日志结构化的方法显得更简单，因为它们在后台执行所有合并，而不会干扰前端的查询，并且会不时地用新段原子地替换旧段。  

**优化B-tree**

由于B-tree已经存在了很长时间，自然多年来开发了许多优化措施。这里只列举一些：  

- 一些数据库（如LMDB）不使用覆盖页和维护WAL来进行崩溃恢复，而是使用写时复制方案[21]。修改的页被写入不同的位置，树中父页的新版本被创建，并指向新的位置。这种方法对于并发控制也很有帮助，详见后面第7章“快照隔离与可重复读”。
- 保存键的缩略信息，而不是完整的键，这样可以节省页空间。特别是在树中间的页中，只需要提供足够的信息来描述键的起止范围。这样可以将更多的键压入到页中，让树具有更高的分支因子，从而减少层数注。
- 一般来说，页可以放在磁盘上的任何位置；没有要求相邻的页需要放在磁盘的相邻位置。如果查询需要按照顺序扫描大段的键范围，考虑到每个读取的页都可能需要磁盘I/O，所以逐页的布局可能是低效的。因此，许多B-tree的实现尝试对树进行布局，以便相邻叶子页可以按顺序保存在磁盘上。然而，随着树的增长，维持这个顺序会变得越来越困难。相比之下，由于LSM-tree在合并过程中一次重写大量存储段，因此它们更容易让连续的键在磁盘上相互靠近。
- 添加额外的指针到树中。例如，每个叶子页面可能会向左和向右引用其同级的兄弟页，这样可以顺序扫描键，而不用跳回到父页。
- B-tree的变体如分形树[22]，借鉴了一些日志结构的想法来减少磁盘寻道（与分形无关）。  

#### **对比B-tree和LSM-tree**

尽管B-tree的实现比LSM-tree的实现更为成熟，然而由于LSM-tree的性能特点，LSM-tree目前很有吸引力。根据经验，LSM-tree通常对于写入更快，而B-tree被认为对于读取更快[23]。读取通常在LSM-tree上较慢，因为它们必须在不同的压缩阶段检查多个不同的数据结构和SSTable。  

然而，基准测试通常并不太确定，而且取决于很多工作负载的具体细节。最好测试特定工作负载，这样方便进行更有效的比较。在本节中，将简要讨论在测量存储引擎性能时值得考虑的一些要点。  

**LSM-tree的优点**

B-tree索引必须至少写两次数据：一次写入预写日志，一次写入树的页本身（还可能发生页分裂）。即使该页中只有几个字节更改，也必须承受写整个页的开销。一些存储引擎甚至覆盖相同的页两次，以避免在电源故障的情况下最终出现部分更新的页[24,25]  

由于反复压缩和SSTable的合并，日志结构索引也会重写数据多次。这种影响（在数据库内，由于一次数据库写入请求导致的多次磁盘写）称为写放大。对于SSD，由于只能承受有限次地擦除覆盖，因此尤为关注写放大指标。  

对于大量写密集的应用程序，性能瓶颈很可能在于数据库写入磁盘的速率。在这种情况下，写放大具有直接的性能成本：存储引擎写入磁盘的次数越多，可用磁盘带宽中每秒可以处理的写入越少。  

此外，LSM-tree通常能够承受比B-tree更高的写入吞吐量，部分是因为它们有时具有较低的写放大（尽管这取决于存储引擎的配置和工作负载），部分原因是它们以顺序方式写入紧凑的SSTable文件，而不必重写树中的多个页[26]。这种差异对于磁盘驱动器尤为重要，原因是磁盘的顺序写比随机写要快得多。  

LSM-tree可以支持更好地压缩，因此通常磁盘上的文件比B-tree小很多。由于碎片，B-tree存储引擎使某些磁盘空间无法使用：当页被分裂或当一行的内容不能适合现有页时，页中的某些空间无法使用。由于LSM-tree不是面向页的，并且定期重写SSTables以消除碎片化，所以它们具有较低的存储开销，特别是在使用分层压缩时[27]。  

在许多SSD上，固件内部使用日志结构化算法将随机写入转换为底层存储芯片上的顺序写入，所以存储引擎写入模式的影响不那么明显[19]。然而，更低的写放大和碎片减少对于SSD上仍然有益，以更紧凑的方式表示数据，从而在可用的I/O带宽中支持更多的读写请求。

**LSM-tree的缺点**

日志结构存储的缺点是压缩过程有时会干扰正在进行的读写操作。即使存储引擎尝试增量地执行压缩，并且不影响并发访问，但由于磁盘的并发资源有限，所以当磁盘执行昂贵的压缩操作时，很容易发生读写请求等待的情况。这对吞吐量和平均响应时间的影响通常很小，但是如果观察较高的百分位数（参看第1章“描述性能”），日志结构化存储引擎的查询响应时间有时会相当高，而B-tree的响应延迟则更具确定性[28]  

高写入吞吐量时，压缩的另一个问题就会冒出来：磁盘的有限写入带宽需要在初始写入（记录并刷新内存表到磁盘）和后台运行的压缩线程之间所共享。写入空数据库时，全部的磁盘带宽可用于初始写入，但数据库的数据量越大，压缩所需的磁盘带宽就越多。  

如果写入吞吐量很高并且压缩没有仔细配置，那么就会发生压缩无法匹配新数据写入速率的情况。在这种情况下，磁盘上未合并段的数量不断增加，直到磁盘空间不足，由于它们需要检查更多的段文件，因此读取速度也会降低。通常，即使压缩不能跟上，基于SSTable的存储引擎也不会限制到来的写入速率，因此需要额外的监控措施来及时发现这种情况[29.30]。  

B-tree的优点则是每个键都恰好唯一对应于索引中的某个位置，而日志结构的存储引擎可能在不同的段中具有相同键的多个副本。如果数据库希望提供强大的事务语义，这方面B-tree显得更具有吸引力：在许多关系数据库中，事务隔离是通过键范围上的锁来实现的，并且在B-tree索引中，这些锁可以直接定义到树中[5]。在第7章，我们将更详细地讨论这一点。  

B-tree在数据库架构中已经根深蒂固，为许多工作负载提供了一贯良好的性能，所以不太可能在短期内会消失。对于新的数据存储，日志结构索引则越来越受欢迎。不存在快速和简单的规则来确定哪种存储引擎更适合你的用例，因此，实地的测试总是需要的。  

#### **其他索引结构**

到目前为止，只讨论了key-value索引，它们像关系模型中的主键（primarykey）索引。主键唯一标识关系表中的一行，或文档数据库中的一个文档，或图形数据库中的一个顶点。数据库中的其他记录可以通过其主键（或ID）来引用该行/文档/顶点，该索引用于解析此类引用。  

二级索引也很常见。在关系数据库中，可以使用CREATEINDEX命令在同一个表上创建多个二级索引，并且它们通常对于高效地执行联结操作至关重要。例如，在第2章的图2-1中，很可能在user_id列上有一个二级索引，以便可以在每个表中找到属于同一用户的所有行。  

二级索引可以容易地基于key-value索引来构建。主要区别在于它的键不是唯一的，即可能有许多行（文档，顶点）具有相同键。这可以通过两种方式解决：使索引中的每个值成为匹配行标识符的列表（像全文索引中的postinglist），或者追加一些行标识符来使每个键变得唯一。无论哪种方式，B-tree和日志结构索引都可以用作二级索引。 

**在索引中存储值**

索引中的键是查询搜索的对象，而值则可以是以下两类之一：它可能是上述的实际行（文档，顶点），也可以是对其他地方存储的行的引用。在后一种情况下，存储行的具体位置被称为堆文件，并且它不以特定的顺序存储数据（它可以是追加的，或者记录删掉的行以便用新数据在之后覆盖它们）。堆文件方法比较常见，这样当存在多个二级索引时，它可以避免复制数据，即每个索引只引用堆文件中的位置信息，实际数据仍保存在一个位置。  

当更新值而不更改键时，堆文件方法会非常高效：只要新值的字节数不大于旧值，记录就可以直接覆盖。如果新值较大，则情况会更复杂，它可能需要移动数据以得到一个足够大空间的新位置。在这种情况下，所有索引都需要更新以指向记录的新的堆位置，或者在旧堆位置保留一个间接指针[5]。  

在某些情况下，从索引到堆文件的额外跳转对于读取来说意味着太多的性能损失，因此可能希望将索引行直接存储在索引中。这被称为聚集索引。例如，在MySQLInnoDB存储引擎中，表的主键始终是聚集索引，二级索引引用主键（而不是堆文件位置）[31]。在SQL Server中，可以为每个表指定一个聚集索引[32]聚集索引（在索引中直接保存行数据）和非聚集索引（仅存储索引中的数据的引用）之间有一种折中设计称为覆盖索引或包含列的索引，它在索引中保存一些表的列值[33。它可以支持只通过索引即可回答某些简单查询（在这种情况下，称索引覆盖了查询）[32]。  

与任何类型的数据冗余一样，聚集和覆盖索引可以加快读取速度，但是它们需要额外的存储，并且会增加写入的开销。此外，数据库还需要更多的工作来保证事务性，这样应用程序不会因为数据冗余而得到不一致的结果。  

**多列索引**

迄今为止讨论的索引只将一个键映射到一个值。如果需要同时查询表的多个列（或文档中的多个字段），那么这是不够的。  

最常见的多列索引类型称为级联索引，它通过将一列追加到另一列，将几个字段简单地组合成一个键（索引的定义指定字段连接的顺序）。这就像一个老式的纸质电话簿，它提供从（lastname，firstname）到电话号码的索引。由于排序，索引可用于查找具有特定lastname的所有人，或所有具有特定lastname-firstname组合的人。但是，它无法查找具有特定firstname的所有人。  

多维索引是更普遍的一次查询多列的方法，这对地理空间数据尤为重要。例如，餐馆搜索网站可能有一个包含每个餐厅的纬度和经度的数据库。当用户在地图上查看餐馆时，网站需要搜索用户正在查看的矩形地图区域内的所有餐馆。这要求一个二维的范围查询，如下所示：  

SELECT\*FROM restaurants WHERE latitude >51.4946AND latitude <51.5079 AND longitude> -0.1162 AND longitude<-0.1004;  

标准B-tree或LSM-tree索引无法高效地应对这种查询，它只能提供一个纬度范围内（但在任何经度）的所有餐馆，或者所有经度范围内的餐厅（在北极和南极之间的任何地方），但不能同时满足。  

一种选择是使用空格填充曲线将二维位置转换为单个数字，然后使用常规的B-tree索引[34]。更常见的是使用专门的空间索引，如R树。例如，PostGIS使用PostgreSQL的广义搜索树索引[35]实现了地理空间索引作为R树。篇幅所限，这里无法详细描述R树，但有很多关于它们的参考文献。  

一个有趣的想法是，多维索引不仅仅适用于地理位置。例如，在电子商务网站上，可以使用颜色维度（红色，绿色，蓝色）上的三维索引来搜索特定颜色范围内的产品，或在天气观测数据库中，可以在（日期，温度）上创建二维索引，以便高效地搜索在2013年中，温度在 $2 5 { \sim } 3 0 ^ { \circ } \mathrm { C }$ 之间的所有观测值。使用一维索引，将不得不从2013年扫描所有记录（无论温度如何），然后按温度进行过滤，反之亦然。二维索引可以通过时间戳和温度同时缩小查询范围。HyperDex使用了这种技术[36]。  

**全文搜索和模糊索引**

到目前为止讨论的所有索引都假定具有确切的数据，并允许查询键的确切值或排序的键的取值范围。它们不支持搜索类似的键，如拼写错误的单词。这种模糊查询需要不同的技术。  

例如，全文搜索引擎通常支持对一个单词的所有同义词进行查询，并忽略单词语法上的变体，在同一文档中搜索彼此接近的单词的出现，并且支持多种依赖语言分析的其他高级功能。为了处理文档或查询中的拼写错误，Lucene能够在某个编辑距离内搜索文本（编辑距离为1表示已经添加、删除或替换了一个字母）。  

如上节“从SSTable到LSM-tree”所述，Lucene对其词典使用类似SSTable的结构。此结构需要一个小的内存索引来告诉查询，为了找到一个键，需要排序文件中的哪个偏移量。在LevelDB中，这个内存中的索引是一些键的稀疏集合，但是在Lucene中，内存中的索引是键中的字符序列的有限状态自动机，类似字典树[38]。这个自动机可以转换成Levenshtein自动机，它支持在给定编辑距离内高效地搜索单词[39]。  

其他模糊搜索技术则沿着文档分类和机器学习的方向发展。有关更多细节，请参阅相关信息检索教程[40]。

**在内存中保存所有内容**

本章迄今为止讨论的数据结构都是为了适应磁盘限制。与内存相比，磁盘更难以处理。使用磁盘和SSD，如果要获得良好的读写性能，需要精心地安排磁盘上的数据布局。然而这些工作是值得的，因为磁盘有两个显著的优点：数据保存持久化（如果电源关闭，内容不会丢失），并且每GB容量的成本比内存低很多。  

随着内存变得更便宜，每GB成本被摊薄。而许多数据集不是那么大，可以将它们完全保留在内存中，或者分布在多台机器上。这推动了内存数据库的发展。  

一些内存中的key-value存储（如Memcached），主要用于缓存，如果机器重启造成的数据丢失是可以接受的。但是其他内存数据库旨在实现持久性，例如可以通过用特殊硬件（如电池供电的内存），或者通过将更改记录写入磁盘，或者将定期快照写入磁盘，以及复制内存中的状态到其他机器等方式来实现。  

当内存数据库重启时，它需要重新载入其状态，无论是从磁盘还是通过网络从副本（除非使用特殊硬件）。尽管写入磁盘，但磁盘仅仅用作为了持久性目的的追加日志，读取完全靠内存服务。此外，写入磁盘还具有一些运维方面优势：磁盘上的文件可以容易地通过外部工具来执行备份、检查和分析。  

诸如VoltDB、MemSQL和OracleTimesTen的产品是具有关系模型的内存数据库，相关供应商声称通过移除所有与管理磁盘数据结构相关的开销，它们可以获得极大的性能提升[41，42]。RAMCloud是一个开源的、具有持久性的内存key-value存储（对内存和磁盘上的数据使用日志结构）[43]。而Redis和Couchbase通过异步写入磁盘提供较弱的持久性。  

与直觉相反，内存数据库的性能优势并不是因为它们不需要从磁盘读取。如果有足够的内存，即使是基于磁盘的存储引擎，也可能永远不需要从磁盘读取，因为操作系统将最近使用的磁盘块缓存在内存中。相反，内存数据库可以更快，是因为它们避免使用写磁盘的格式对内存数据结构编码的开销[44]  

除了性能外，内存数据库的另一个有意思的地方是，它提供了基于磁盘索引难以实现的某些数据模型。例如，Redis为各种数据结构（如优先级队列和集合）都提供了类似数据库的访问接口。由于所有的数据都保存在内存中，所以实现可以比较简单。  

最近的研究表明，内存数据库架构可以扩展到支持远大于可用内存的数据集，而不会导致以磁盘为中心架构的开销[45]。所谓的反缓存方法，当没有足够的内存时，通过将最近最少使用的数据从内存写到磁盘，并在将来再次被访问时将其加载到内存。这与操作系统对虚拟内存和交换文件的操作类似，但数据库可以在记录级别而不是整个内存页的粒度工作，因而比操作系统更有效地管理内存。不过，这种方法仍然需要索引完全放入内存（如本章开头的Bitcask示例）。  

如果将来非易失性存储（non-volatilememory，NVM）技术得到更广泛普及，可能还需要进一步改变存储引擎设计[46]。目前这是一个新的研究领域，但值得密切关注。  

### **事务处理与分析处理**

#### **数据仓库**

#### **星型与雪花型分析模式**

### **列式存储**

### **小结**

# 第4章 数据编码与演化










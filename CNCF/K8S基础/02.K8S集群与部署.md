# K8S 简介

由于容器技术的兴起，导致大量的容器应用出现，所以就出现了一些用来支持应用程序容器化部署和组织的容器编排技术，一些流行的开源容器编排工具有 Docker Swarm、Kubernetes 等，但是在发展过程中 Kubernetes 成为了容器编排领域事实上的标准。

Kubernetes 是 Google 团队发起的一个开源项目，它的目标是管理跨多个主机的容器，用于自动部署、扩展和管理容器化的应用程序，主要实现语言为 Go 语言，它的理论基础来源与 Google 内部的 Borg 项目，所以 Kubernetes 项目的理论基础就比其他开源项目要“先进”很多，因为 Borg 系统一直以来就被称为 Google 公司内部最强大的“私密武器”。

## 架构

Kubernetes 项目依托着 Borg 项目的理论优势，确定了一个如下图所示的全局架构图：

<img src="image/image-20240926223256459.png" alt="image-20240926223256459" style="zoom: 33%;" />

从上面可以看出 Kubernetes 由 Master 和 Node 两种节点组成，这两种角色分别对应着控制节点和工作节点。

其中 Master 节点由三个独立的组件组成，它们分别是负责整个集群通信的 kube-apiserver、负责容器调度的 kube-scheduler 以及负责维护集群状态的 kube-controller-manager 组件。整个集群的数据都是通过 kube-apiserver 保存到 etcd 数据库中的。

工作节点上最核心的组件就是 kubelet，当然还有容器运行时，比如 Docker；其中 kubelet 就是主要来实现和容器运行时进行通信的，这个通信的过程也被 Kubernetes 抽象成了 CRI（Container Runtime Interface）。对于 Kubernetes 来说它根本不关心你使用什么容器运行时，只要这个容器运行时实现了 CRI 接口就可以被 Kubernetes 来管理。

**kubelet 的另外一个重要功能就是调用网络插件（CNI）和存储插件（CSI）为容器配置网络和存储功能，同样的 kubelet 也是把这两个重要功能通过接口暴露给外部了，所以如果我们想要实现自己的网络插件，只需要使用 CNI 就可以很方便的对接到 Kubernetes 集群当中去。**

可能下面的架构图看上去更清晰一些：

<img src="image/image-20240926223402800.png" alt="image-20240926223402800" style="zoom: 50%;" />



在 Kubernetes 集群中，**Master 节点同样需要安装 `kubelet`**。以下是关键原因和功能说明：

Master 节点的核心组件（如 `kube-apiserver`、`kube-scheduler`、`kube-controller-manager`）默认以静态 Pod（Static Pod）的形式运行，这些 Pod 由 `kubelet` 直接管理。`kubelet` 会监控 `/etc/kubernetes/manifests` 目录下的 YAML 文件，并自动启动这些组件容器。

`kubelet` 负责向 `kube-apiserver` 上报 Master 节点的资源状态（如 CPU、内存）、健康状态及运行中的 Pod 信息。若未安装，Master 节点将无法被集群识别为有效节点。



## 组件

上面介绍了 Kubernetes 集群的整体架构，下面再来更加详细的了解下这些组件的功能。

### kube-apiserver

API Server 提供了资源对象的唯一操作入口，其它所有组件都必须通过它提供的 API 来操作资源数据。只有 API Server 会与 etcd 进行通信，其它模块都必须通过 API Server 访问集群状态。API Server 作为 Kubernetes 系统的入口，封装了核心对象的增删改查操作。API Server 以 RESTFul 接口方式提供给外部客户端和内部组件调用，API Server 再对相关的资源数据进行操作，以达到实时完成相关的业务功能。

### kube-controller-manager

<img src="image/image-20240927113417549.png" alt="image-20240927113417549" style="zoom:67%;" />

Controller Manager 用于实现 Kubernetes 集群故障检测和恢复的自动化工作，主要负责管理各种控制器。

### kube-scheduler

Scheduler 是负责整个集群的资源调度的，主要的职责如下所示：

- 主要用于收集和分析当前 Kubernetes 集群中所有 Node 节点的资源 (包括内存、CPU 等) 负载情况，然后依据资源占用情况分发新建的 Pod 到 Kubernetes 集群中可用的节点
- 实时监测 Kubernetes 集群中未分发和已分发的 Pod
- 实时监测 Node 节点信息，由于会频繁查找 Node 节点，所以 Scheduler 同时会缓存一份信息在本地
- 在分发 Pod 到指定的 Node 节点后，会把 Pod 相关的 Binding 信息写回 API Server，以方便其它组件使用

### kubelet

kubelet 是负责容器运行的核心组件，主要的职责如下所示：

- 负责 Node 节点上 Pod 的创建、修改、监控、删除等全生命周期的管理
- 定时上报本地 Node 的状态信息给 API Server
- kubelet 是 Master 和 Node 之间的桥梁，接收 API Server 分配给它的任务并执行
- kubelet 通过 API Server 间接与 Etcd 集群交互来读取集群配置信息

### kube-proxy

kube-proxy是Kubernetes集群每个工作节点（Node）上的网络代理组件，核心职责是实现Service的抽象功能。它通过维护动态网络规则（如iptables/IPVS），解决以下关键问题：

- 服务发现：为Service提供稳定的虚拟IP（ClusterIP），屏蔽后端Pod IP动态变化的影响。
- 端口冲突规避：通过Service端口映射机制，允许多个Pod复用宿主机相同端口（如NodePort），避免端口冲突。
- 外部访问能力：提供NodePort、LoadBalancer等类型的Service，使集群外部流量可访问容器服务。

### kubectl

kubectl 是 Kubernetes 的集群管理命令行客户端工具集。通过 kubectl 命令对 API Server 进行操作，API Server 响应并返回对应的命令结果，从而达到对 Kubernetes 集群的管理。



## 核心资源对象

上面都是在架构层面了解 Kubernetes，但是似乎没有发现关于容器的说明，Kubernetes 作为容器编排引擎，那么它是怎么去对容器进行编排的呢？在 Kubernetes 集群中抽象了很多集群内部的资源对象，我们可以通过这些资源对象去操作容器的编排工作。

### Pod

Pod 是一组紧密关联的容器集合，它们共享 PID、IPC、Network 和 UTS namespace，是 Kubernetes 调度的基本单位。Pod 的设计理念是支持多个容器在一个 Pod 中共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。我们知道容器本质上就是进程，那么 Pod 实际上就是进程组了，只是这一组进程是作为一个整体来进行调度的。

在 Kubernetes 中，所有资源对象都使用资源清单（yaml 或 json）来定义，比如可以定义一个简单的 nginx 服务，它包含一个镜像为 nginx 的容器：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
```

定义了这样一个资源清单文件后，就可以利用上面提到的 kubectl 工具将这个 Pod 创建到 Kubernetes 集群中：

```shell
kubectl apply -f nginx-pod.yaml
```

Pod 在 Kubernetes 集群中被创建的基本流程如下所示：

<img src="image/image-20240927144114143.png" alt="image-20240927144114143" style="zoom: 50%;" />

- 用户通过 REST API 创建一个 Pod
- apiserver 将其写入 etcd
- scheduluer 检测到未绑定 Node 的 Pod，开始调度并更新 Pod 的 Node 绑定
- kubelet 检测到有新的 Pod 调度过来，通过 container runtime 运行该 Pod
- kubelet 通过 container runtime 取到 Pod 状态，并更新到 apiserver 中

### Label

Label 标签在 Kubernetes 资源对象中使用很多，是非常重要的一个属性，Label 是识别 Kubernetes 对象的标签，以 key/value 的方式附加到对象上（key 最长不能超过 63 字节，value 可以为空，也可以是不超过 253 字节的字符串）。

Label 不提供唯一性，并且实际上经常是很多对象（如 Pod）都使用相同的 Label 来标志具体的应用。Label 定义好后其他对象可以使用 Label Selector 来选择一组相同 Label 的对象（比如 Service 用 Label 来选择一组 Pod）。Label Selector 支持以下几种方式：

- 等式，如 `app=nginx` 和 `env!=production`
- 集合，如 `env in (production, qa)`
- 多个 Label（它们之间是AND关系），如`app=nginx,env=test`

### Namespace

Namespace（命名空间）是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的 Pods、Services、Deployments 等都是属于某一个 Namespace 的（默认是 default），比如上面的 Nginx Pod 没有指定 namespace，则默认就在 default 命名空间下面，而 Node、PersistentVolumes 等资源则不属于任何 Namespace，是全局的。

### Deployment

我们说了 Pod 是 Kubernetes 集群中的最基本的调度单元，但是如果想要创建同一个容器的多份拷贝，需要一个一个分别创建出来吗，那么能否将 Pods 划到一个逻辑组里面呢？Deployment 就是来管理 Pod 的资源对象。

Deployment 确保任意时间都有指定数量的Pod副本在运行。如果为某个 Pod 创建了 Deployment 并且指定 3 个副本，它会创建 3 个 Pod，并且持续监控它们。如果某个 Pod 不响应，那么 Deployment 会替换它，始终保持总数为 3。

如果之前不响应的 Pod 恢复了，现在就有 4 个 Pod 了，那么 Deployment 会将其中一个终止保持总数为 3。如果在运行中将副本总数改为 5，Deployment 会立刻启动 2 个新 Pod，保证总数为 5，支持回滚和滚动升级。

现在已经创建了 Pod 的一些副本，那么这些副本上如何进行负载呢？如何把这些 Pod 暴露出去呢？这个时候我们就需要用到 Service 这种资源对象了。

### Service

Service 是应用服务的抽象，通过 Labels 为应用提供负载均衡和服务发现。匹配 Labels 的 Pod IP 和端口列表组成 Endpoints，由 kube-proxy 负责将服务 IP 负载均衡到这些 Endpoints 上。

每个 Service 都会自动分配一个 cluster IP（仅在集群内部可访问的虚拟地址）和 DNS 名，其他容器可以通过该地址或 DNS 来访问服务，而不需要了解后端容器的运行。

# 集群部署

在对容器运行时和 K8s 有了一个整体上的认识后，接下来就需要来搭建一个 K8s 集群了，这里将会使用 Kubeadm 和 Kind 两种工具来搭建集群。

## kubeadm

Kubeadm 是一个安装 K8s 集群的工具包，可帮助你以更简单、合理安全和可扩展的方式引导最佳实践 Kubernetes 群集。它还支持为你管理 Bootstrap Tokens 并升级/降级集群。

Kubeadm 的目标是建立一个通过 Kubernetes 一致性测试的最小可行集群 ，但不会安装其他功能插件。在设计上并未安装网络解决方案，所以需要用户自行安装第三方符合 CNI 的网络解决方案（如 flanal，calico 等）。此外 Kubeadm 可以在多种设备上运行，可以是 Linux 笔记本电脑、虚拟机、物理/云服务器或 Raspberry Pi，这使得 Kubeadm 非常适合与不同种类的配置系统（例如 Terraform，Ansible 等）集成。

Kubeadm 在 2018 年 12 月 3 日发布的 [Kubernetes 1.13](https://kubernetes.io/blog/2018/12/03/kubernetes-1-13-release-announcement/) 版本中就已经宣布 GA 了，所以可以支持生产环境。

现在我们就来使用 Kubeadm 从头搭建一个使用 Containerd 作为容器运行时的 K8s 集群，这里我们安装的是最新稳定版 v1.25.4 版本。

### 环境准备

3 个节点，都是 Centos 7.6 系统，内核版本：3.10.0-1160.71.1.el7.x86_64，在每个节点上添加 hosts 信息：

```shell
[root@master ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.220.152 master
192.168.220.153 node1
192.168.220.154 node2
```

节点的 hostname 必须使用标准的 DNS 命名，另外千万别用默认 localhost 的 hostname，会导致各种错误出现的。在 Kubernetes 项目里，机器的名字以及一切存储在 Etcd 中的 API 对象，都必须使用标准的 DNS 命名（RFC 1123）。可以使用命令 `hostnamectl set-hostname xxx` 来修改 hostname。

下面是一些环境准备工作，需要在所有节点配置。

首先禁用防火墙：

```shell
➜  ~ systemctl stop firewalld
➜  ~ systemctl disable firewalld
```

禁用 SELINUX：

```shell
➜  ~ setenforce 0
➜  ~ cat /etc/selinux/config
SELINUX=disabled
```

如果使用的是云服务器，比如阿里云、腾讯云等，需要配置安全组，放开端口，如果只是为了测试方便可以直接全部放开，对于生产环境则只需要放开 K8S 要是使用到的一些端口，比如 6443 等等。

由于开启内核 ipv4 转发需要加载 `br_netfilter` 模块，所以加载下该模块：

```shell
➜  ~ modprobe br_netfilter
```

最好将上面的命令设置成开机启动，因为重启后模块失效，下面是开机自动加载模块的方式。在 `/etc/rc.d/rc.local` 文件末尾添加如下脚本内容：

```shell
for file in /etc/sysconfig/modules/*.modules ; do
[ -x $file ] && $file
done
```

然后在 `/etc/sysconfig/modules/` 目录下新建如下文件：

```shell
# mkdir -p /etc/sysconfig/modules/
# vi /etc/sysconfig/modules/br_netfilter.modules
modprobe br_netfilter
```

并给文件增加权限：

```shell
➜  ~ chmod 755 br_netfilter.modules
```

然后重启后，模块就可以自动加载了：

```shell
➜  ~ lsmod |grep br_netfilter
br_netfilter           22209  0
bridge                136173  1 br_netfilter
```

创建 `/etc/sysctl.d/k8s.conf` 文件，添加如下内容：

```shell
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
# 下面的内核参数可以解决ipvs模式下长连接空闲超时的问题
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_keepalive_probes = 10
net.ipv4.tcp_keepalive_time = 600
```

`bridge-nf` 使得 netfilter 可以对 Linux 网桥上的 IPv4/ARP/IPv6 包过滤。比如，设置`net.bridge.bridge-nf-call-iptables＝1`后，二层的网桥在转发包时也会被 iptables的 FORWARD 规则所过滤。常用的选项包括：

- net.bridge.bridge-nf-call-arptables：是否在 arptables 的 FORWARD 中过滤网桥的 ARP 包
- net.bridge.bridge-nf-call-ip6tables：是否在 ip6tables 链中过滤 IPv6 包
- net.bridge.bridge-nf-call-iptables：是否在 iptables 链中过滤 IPv4 包
- net.bridge.bridge-nf-filter-vlan-tagged：是否在 iptables/arptables 中过滤打了 vlan 标签的包。

执行如下命令使修改生效：

```shell
➜  ~ sysctl -p /etc/sysctl.d/k8s.conf
```

安装 ipvs：

```shell
➜  ~ cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
➜  ~ chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
```

上面脚本创建的 `/etc/sysconfig/modules/ipvs.modules` 文件，保证在节点重启后能自动加载所需模块。使用 `lsmod | grep -e ip_vs -e nf_conntrack_ipv4` 命令查看是否已经正确加载所需的内核模块。

接下来还需要确保各个节点上已经安装了 ipset 软件包，为了便于查看 ipvs 的代理规则，最好安装一下管理工具 ipvsadm：

```shell
➜ yum install ipset
➜ yum install ipvsadm
```

然后记得一定要同步服务器时间，这里我们使用 chrony 来进行同步，其他工具也可以：

```shell
➜  ~ yum install chrony -y
➜  ~ systemctl enable chronyd
➜  ~ systemctl start chronyd
➜  ~ chronyc sources
210 Number of sources = 4
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^+ sv1.ggsrv.de                  2   6    17    32   -823us[-1128us] +/-   98ms
^- montreal.ca.logiplex.net      2   6    17    32    -17ms[  -17ms] +/-  179ms
^- ntp6.flashdance.cx            2   6    17    32    -32ms[  -32ms] +/-  161ms
^* 119.28.183.184                2   6    33    32   +661us[ +357us] +/-   38ms
➜  ~ date
Tue Aug 31 14:36:14 CST 2021
```

关闭 swap 分区：

```shell
➜  ~ swapoff -a
```

修改 `/etc/fstab`文件，注释掉 SWAP 的自动挂载，使用 `free -m` 确认 swap 已经关闭。swappiness 参数调整，修改 `/etc/sysctl.d/k8s.conf` 添加下面一行：

```shell
[root@master ~]# cat /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
# 下面的内核参数可以解决ipvs模式下长连接空闲超时的问题
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_keepalive_probes = 10
net.ipv4.tcp_keepalive_time = 600
# 添加最后一行
vm.swappiness=0
```

执行 `sysctl -p /etc/sysctl.d/k8s.conf` 使修改生效。当然如果是生产环境使用还可以先对内核参数进行统一的调优。

### 安装Containerd

接下来需要安装 Containerd 容器运行时

> 如果这安装集群的过程出现了容器运行时的问题，启动不起来，可以尝试使用 yum install containerd.io 来安装 Containerd。

首先需要在节点上安装 seccomp 依赖，这一步很重要：

```shell
➜  ~ rpm -qa |grep libseccomp
libseccomp-2.3.1-4.el7.x86_64
# 如果没有安装 libseccomp 包则执行下面的命令安装依赖
➜  ~ yum install wget -y
➜  ~ wget http://mirror.centos.org/centos/7/os/x86_64/Packages/libseccomp-2.3.1-4.el7.x86_64.rpm
➜  ~ yum install libseccomp-2.3.1-4.el7.x86_64.rpm -y
```

由于 Containerd 需要依赖底层的 runc 工具，所以我们也需要先安装 runc，不过 Containerd 提供了一个包含相关依赖的压缩包 `cri-xxxx-${VERSION}.${OS}-${ARCH}.tar.gz`，可以直接使用这个包来进行安装，强烈建议使用该安装包，不然可能因为 runc 版本问题导致不兼容。

首先从 [release 页面](https://github.com/containerd/containerd/releases)下载最新的 1.6.10 版本的压缩包，直接将压缩包解压到系统的各个目录中：

```shell
➜ tar -C / -xzf cri-containerd-1.6.10-linux-amd64.tar.gz
```

记得将 `/usr/local/bin` 和 `/usr/local/sbin` 追加到 PATH 环境变量中：

```shell
[root@master ~]# echo $PATH
/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin
[root@master ~]# containerd -v
containerd github.com/containerd/containerd v1.6.10 770bd0108c32f3fb5c73ae1264f7e503fe7b2661

[root@node2 ~]# runc -h
NAME:
   runc - Open Container Initiative runtime
USAGE:
   runc [global options] command [command options] [arguments...]

VERSION:
   1.1.4
commit: v1.1.4-0-g5fd4c4d1
spec: 1.0.2-dev
go: go1.18.8
libseccomp: 2.5.1
```

可以正常执行 `containerd -v` 命令证明 Containerd 安装成功了，但是执行 `runc -h` 命令的时候却出现了类似 `runc: undefined symbol: seccomp_notify_respond` 的错误，这是因为当前系统默认安装的 libseccomp 是 2.3.1 版本，该版本已经不能满足我们这里的 v1.6.10 版本的 containerd 了（从 1.5.7 版本开始就不兼容了），需要 2.4 以上的版本，所以需要重新安装一个高版本的 libseccomp。

```shell
☸ ➜ rpm -qa | grep libseccomp
libseccomp-2.3.1-4.el7.x86_64
# 先卸载原来的版本，有可能会把依赖的库也被删除，比如时间同步的chrony，所以需要重新下载
# --nodeps是一个选项，它告诉rpm在删除软件包时忽略依赖性检查。通常，当你试图删除一个软件包时，rpm会检查其他包是否依赖于它。如果有依赖关系，它默认会阻止删除操作以防止系统中断。使用--nodeps选项会强制删除软件包，即使这样做会破坏依赖。
rpm -e libseccomp-2.3.1-4.el7.x86_64 --nodeps

# 下载高于 2.4 以上的包
# wget https://mirrors.aliyun.com/centos-vault/8.5.2111/BaseOS/x86_64/os/Packages/libseccomp-2.5.1-1.el8.x86_64.rpm
☸ ➜ wget http://rpmfind.net/linux/centos/8-stream/BaseOS/x86_64/os/Packages/libseccomp-2.5.1-1.el8.x86_64.rpm
☸ ➜ rpm -ivh libseccomp-2.5.1-1.el8.x86_64.rpm
☸ ➜ rpm -qa | grep libseccomp
libseccomp-2.5.1-1.el8.x86_64
```

现在 runc 命令就可以正常使用了。

Containerd 的默认配置文件为 `/etc/containerd/config.toml`，我们可以通过如下所示的命令生成一个默认的配置：

```shell
☸ ➜ mkdir -p /etc/containerd
☸ ➜ containerd config default > /etc/containerd/config.toml
```

对于使用 systemd 作为 init system 的 Linux 的发行版，使用 systemd 作为容器的 cgroup driver 可以确保节点在资源紧张的情况更加稳定，所以推荐将 containerd 的 cgroup driver 配置为 systemd。

修改前面生成的配置文件 `/etc/containerd/config.toml`，在`plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options` 配置块下面将 `SystemdCgroup` 设置为 `true`：

```shell
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
  ...
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    # SystemdCgroup = false      
    SystemdCgroup = true
    ....
```

然后再为镜像仓库配置一个加速器，需要在 cri 配置块下面的 registry 配置块下面进行配置 `registry.mirrors`：

```toml
# 关于镜像仓库的配置，请参考01章节的介绍，下面的做法不再推荐
[plugins."io.containerd.grpc.v1.cri"]
  ...
  # sandbox_image = "registry.k8s.io/pause:3.6", k8s 1.25版本使用的3.8这个版本
  sandbox_image = "registry.aliyuncs.com/k8sxio/pause:3.8"
  ...
  [plugins."io.containerd.grpc.v1.cri".registry]
    [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
      [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
        endpoint = ["https://atckcdf3.mirror.aliyuncs.com"]
```

现在社区已经将 K8s 默认的镜像仓库从 `k8s.gcr.io` 迁移到了 `registry.k8s.io` ，不过国内正常情况下还是不能使用，所以如果节点不能正常获取 `registry.k8s.io` 的镜像，那么我们需要在上面重新配置 `sandbox_image` 镜像。

由于上面下载的 containerd 压缩包中包含一个 `etc/systemd/system/containerd.service` 的文件，这样我们就可以通过 systemd 来配置 containerd 作为守护进程运行，现在就可以启动 containerd 了，直接执行下面的命令即可：

```shell
➜  ~ systemctl daemon-reload
➜  ~ systemctl enable containerd --now
```

启动完成后就可以使用 containerd 的本地 CLI 工具 ctr 和 crictl 了，比如查看版本：

```shell
[root@master ~]# ctr version
Client:
  Version:  v1.6.10
  Revision: 770bd0108c32f3fb5c73ae1264f7e503fe7b2661
  Go version: go1.18.8

Server:
  Version:  v1.6.10
  Revision: 770bd0108c32f3fb5c73ae1264f7e503fe7b2661
  UUID: d620def4-303d-4303-88e1-4d95abc1706c
[root@master ~]# crictl version
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  v1.6.10
RuntimeApiVersion:  v1
```



### 初始化集群

上面的相关环境配置完成后，接着就可以来安装 Kubeadm 了，这里是通过指定 yum 源的方式来进行安装的：

```shell
➜  ~ cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
```

当然了，上面的 yum 源是需要科学上网的，如果不能科学上网的话，可以使用阿里云的源进行安装：

```shell
➜  ~ cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

然后安装 kubeadm、kubelet、kubectl：所有节点都需要安装，对于kubeadm来说node节点需要执行kubeadm join命令来加入集群，对于kubectl来说如果不想在node节点执行kubectl命令则不用安装

```shell
# --disableexcludes 禁掉除了kubernetes之外的别的仓库
➜  ~ yum makecache fast
➜  ~ yum install -y kubelet-1.25.4 kubeadm-1.25.4 kubectl-1.25.4 --disableexcludes=kubernetes
[root@master ~]# kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.4", GitCommit:"872a965c6c6526caa949f0c6ac028ef7aff3fb78", GitTreeState:"clean", BuildDate:"2022-11-09T13:35:06Z", GoVersion:"go1.19.3", Compiler:"gc", Platform:"linux/amd64"}
[root@node2 yum.repos.d]# kubelet --version
Kubernetes v1.25.4
[root@node2 yum.repos.d]# kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.4", GitCommit:"872a965c6c6526caa949f0c6ac028ef7aff3fb78", GitTreeState:"clean", BuildDate:"2022-11-09T13:36:36Z", GoVersion:"go1.19.3", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.4", GitCommit:"872a965c6c6526caa949f0c6ac028ef7aff3fb78", GitTreeState:"clean", BuildDate:"2022-11-09T13:29:58Z", GoVersion:"go1.19.3", Compiler:"gc", Platform:"linux/amd64"}
```

可以看到我们这里安装的是 v1.25.4 版本，然后将所有节点的 kubelet 设置成开机启动：

```shell
➜  ~ systemctl enable --now kubelet
```

**到这里为止上面所有的操作都需要在所有节点执行配置**，在云环境上面的话可以将当前环境直接做成一个镜像，然后创建新节点的时候直接使用该镜像即可，这样可以避免重复的工作。

当执行 `kubelet --help` 命令的时候可以看到原来大部分命令行参数都被 DEPRECATED了，这是因为官方推荐使用 `--config` 来指定配置文件，在配置文件中指定原来这些参数的配置，可以通过官方文档 [Set Kubelet parameters via a config file](https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/) 了解更多相关信息，这样 Kubernetes 就可以支持动态 Kubelet 配置（Dynamic Kubelet Configuration）了，参考 [Reconfigure a Node’s Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/)。

接下来可以通过下面的命令在 master 节点上输出集群初始化默认使用的配置：

```shell
➜  ~ kubeadm config print init-defaults --component-configs KubeletConfiguration > kubeadm.yaml
```

然后根据自己的需求修改配置，比如修改 `imageRepository` 指定集群初始化时拉取 Kubernetes 所需镜像的地址，kube-proxy 的模式为 ipvs，另外需要注意的是这里是准备安装 flannel 网络插件的，需要将 `networking.podSubnet` 设置为 10.244.0.0/16：

```shell
[root@master ~]# cat kubeadm.yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.220.146 # 指定master节点内网IP
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  name: master # 修改为master
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/k8sxio # 替换镜像
kind: ClusterConfiguration
kubernetesVersion: 1.25.4 # 更改版本
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
  podSubnet: 10.244.0.0/16 # 指定 pod 子网
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs # kube-proxy 模式
---
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
```

对于上面的资源清单的文档比较杂，要想完整了解上面的资源对象对应的属性，可以查看对应的 godoc 文档，地址:https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta3。

在开始初始化集群之前可以使用 `kubeadm config images pull --config kubeadm.yaml` 预先在各个服务器节点上拉取所 k8s 需要的容器镜像。

配置文件准备好之后，可以使用如下命令先将相关镜像 pull 下来：

```shell
[root@master ~]# kubeadm config images pull --config kubeadm.yaml
[config/images] Pulled registry.aliyuncs.com/k8sxio/kube-apiserver:v1.25.4
[config/images] Pulled registry.aliyuncs.com/k8sxio/kube-controller-manager:v1.25.4
[config/images] Pulled registry.aliyuncs.com/k8sxio/kube-scheduler:v1.25.4
[config/images] Pulled registry.aliyuncs.com/k8sxio/kube-proxy:v1.25.4
[config/images] Pulled registry.aliyuncs.com/k8sxio/pause:3.8
[config/images] Pulled registry.aliyuncs.com/k8sxio/etcd:3.5.5-0
failed to pull image "registry.aliyuncs.com/k8sxio/coredns:v1.9.3": output: E0219 02:47:34.236315    4679 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = NotFound desc = failed to pull and unpack image \"registry.aliyuncs.com/k8sxio/coredns:v1.9.3\": failed to resolve reference \"registry.aliyuncs.com/k8sxio/coredns:v1.9.3\": registry.aliyuncs.com/k8sxio/coredns:v1.9.3: not found" image="registry.aliyuncs.com/k8sxio/coredns:v1.9.3"
time="2024-02-19T02:47:34-05:00" level=fatal msg="pulling image: rpc error: code = NotFound desc = failed to pull and unpack image \"registry.aliyuncs.com/k8sxio/coredns:v1.9.3\": failed to resolve reference \"registry.aliyuncs.com/k8sxio/coredns:v1.9.3\": registry.aliyuncs.com/k8sxio/coredns:v1.9.3: not found"
, error: exit status 1
To see the stack trace of this error execute with --v=5 or higher
[root@master ~]#
```

上面在拉取 coredns 镜像的时候出错了，没有找到这个镜像，我们可以手动 pull 该镜像，然后重新 tag 下镜像地址即可：

```shell
# ctr -n k8s.io i pull docker.io/coredns/coredns:1.9.3
docker.io/coredns/coredns:1.9.3:                                                  resolved       |++++++++++++++++++++++++++++++++++++++|
index-sha256:8e352a029d304ca7431c6507b56800636c321cb52289686a581ab70aaa8a2e2a:    done           |++++++++++++++++++++++++++++++++++++++|
manifest-sha256:bdb36ee882c13135669cfc2bb91c808a33926ad1a411fee07bd2dc344bb8f782: done           |++++++++++++++++++++++++++++++++++++++|
layer-sha256:f2401d57212f95ea8e82ff8728f4f99ef02d4b39459837244d1b049c5d43de43:    done           |++++++++++++++++++++++++++++++++++++++|
config-sha256:5185b96f0becf59032b8e3646e99f84d9655dff3ac9e2605e0dc77f9c441ae4a:   done           |++++++++++++++++++++++++++++++++++++++|
layer-sha256:d92bdee797857f997be3c92988a15c196893cbbd6d5db2aadcdffd2a98475d2d:    done           |++++++++++++++++++++++++++++++++++++++|
elapsed: 10.8s                                                                    total:  14.2 M (1.3 MiB/s)
unpacking linux/amd64 sha256:8e352a029d304ca7431c6507b56800636c321cb52289686a581ab70aaa8a2e2a...
done: 793.185387ms
# ctr -n k8s.io i tag docker.io/coredns/coredns:1.9.3 registry.aliyuncs.com/k8sxio/coredns:v1.9.3
```

在每一个节点上面都应该手动拉取一下coredns的镜像，当然像kube-proxy、pause这类也需要在node节点上面拉取。

然后就可以使用上面的配置文件在 master 节点上进行初始化：

```shell
# 认真阅读下面的输出，其实可以了解到很多细节
[root@master ~]#  kubeadm init --config kubeadm.yaml
[init] Using Kubernetes version: v1.25.4 
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull' # 这里就说明了可以提前拉取初始化集群需要的镜像
# 这是证书相关的流程
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master] and IPs [10.96.0.1 192.168.220.146]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost master] and IPs [192.168.220.146 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost master] and IPs [192.168.220.146 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
# kubeconfig相关的流程
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
# kubelet-start相关的流程
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
# 控制平面相关的流程
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 6.003178 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node master as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: abcdef.0123456789abcdef
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user: # 作为普通用户执行下面的命令

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run: # 作为root用户执行下面的命令

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

# 使用root用户执行下面的命令，可以添加任意数量的worker节点
Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.220.146:6443 --token abcdef.0123456789abcdef \
        --discovery-token-ca-cert-hash sha256:e7b33bd9084141b29f720babe2a055d8aa80d993eb4d1356d1299490fe775838
[root@master ~]#
```

如果`kubeadm.yaml`配置文件修改有问题，比如我之前忘记修改`name: master`，`init` 时会给出警告，这个时候需要先执行`kubeadm reset` 命令重置，然后再次执行上面的 init 命令。

根据安装提示拷贝 kubeconfig 文件：

```shell
➜  ~ mkdir -p $HOME/.kube
➜  ~ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
➜  ~ sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

`/etc/kubernetes/admin.conf`:  这是Kubernetes集群的管理员配置文件的路径。这个文件包含了连接到集群所需的全部信息，包括API服务器的地址、证书、和密钥等。

`$HOME/.kube/config`: 这是kubectl默认查找Kubernetes集群配置信息的位置。`$HOME`是当前用户的主目录，所以这个路径指的是当前用户主目录下的`.kube/config`文件。

执行这条命令后，你就可以使用kubectl命令行工具来管理你的Kubernetes集群了。kubectl会自动读取`.kube/config`文件中的配置信息，以便知道如何连接和认证到你的Kubernetes集群。

然后可以使用 kubectl 命令查看 master 节点是否已经初始化成功了：

```shell
[root@master ~]# kubectl get node
NAME     STATUS     ROLES           AGE   VERSION
master   NotReady   control-plane   14m   v1.25.4
[root@master ~]#
```

现在节点还处于 NotReady 状态，是因为还没有安装 CNI 插件，我们可以先添加一个 Node 节点，再部署网络插件。

### 添加节点

将 master 节点上面的 `$HOME/.kube/config` 文件拷贝到 node 节点对应的文件中（如果想在 node 节点上操作 kubectl），然后执行上面初始化完成后提示的 join 命令即可：

```shell
[root@node1 ~]# kubeadm join 192.168.220.146:6443 --token hl19cf.bqsb66wxhzi5zk5h --discovery-token-ca-cert-hash sha256:e7b33bd9084141b29f720babe2a055d8aa80d993eb4d1356d1299490fe775838
[preflight] Running pre-flight checks
        [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

[root@node1 ~]#
```

根据提示可以看到需要将 kubelet 服务启动，如果忘记了上面的 join 命令可以使用命令 `kubeadm token create --print-join-command` 重新获取。

```shell
[root@master ~]# kubectl get node
NAME     STATUS     ROLES           AGE   VERSION
master   NotReady   control-plane   18m   v1.25.4
node1    NotReady   <none>          57s   v1.25.4
[root@master ~]#
```

这个时候其实集群还不能正常使用，因为还没有安装网络插件，接下来安装网络插件，可以在文档 [https://kubernetes.io/docs/concepts/cluster-administration/addons](https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy) 中选择我们自己的网络插件，这里我们安装 flannel:

```shell
➜  ~ wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
# 如果有节点是多网卡，则需要在资源清单文件中指定内网网卡
# 搜索到名为 kube-flannel-ds 的 DaemonSet，在 kube-flannel 容器下面
➜  ~ vi kube-flannel.yml
......
containers:
- name: kube-flannel
  image: docker.io/flannel/flannel:v0.24.2
  command:
  - /opt/bin/flanneld
  args:
  - --ip-masq
  - --kube-subnet-mgr
  - --iface=ens33  # 如果是多网卡的话，指定内网网卡的名称
......
➜  ~ kubectl apply -f kube-flannel.yml  # 安装 flannel 网络插件
```

隔一会儿查看 Pod 运行状态：

```shell
[root@master ~]# kubectl get pods -A -o wide
```

当部署完网络插件后执行 `ip addr` 命令，正常会看到新增的 `cni0` 与 `flannel.1` 这两个虚拟设备，但是如果没有看到 `cni0` 这个设备也不用太担心，可以观察 `/var/lib/cni` 目录是否存在，如果不存在并不是说部署有问题，而是该节点上暂时还没有应用运行，只需要在该节点上运行一个 Pod 就可以看到该目录会被创建，并且 `cni0` 设备也会被创建出来。

用同样的方法添加另外一个节点即可。

### Dashboard

直接执行下面的命令一键安装即可：

```shell
# 推荐使用下面这种方式
➜  ~ wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
➜  ~ vi recommended.yaml
# 修改Service为NodePort类型
......
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  ports:
    - port: 443
      targetPort: 8443
  selector:
    k8s-app: kubernetes-dashboard
  type: NodePort  # 加上type=NodePort变成NodePort类型的服务
......
```

在 YAML 文件中可以看到新版本 Dashboard 集成了一个 metrics-scraper 的组件，可以通过 Kubernetes 的 Metrics API 收集一些基础资源的监控信息，并在 web 页面上展示，所以要想在页面上展示监控信息就需要提供 Metrics API，比如安装 Metrics Server。

直接创建：

```shell
➜  ~ kubectl apply -f recommended.yaml
```

发现镜像拉取非常慢，这个时候我们到指定的 node下面手动拉取：`ctr -n k8s.io i pull docker.io/kubernetesui/dashboard:v2.7.0`，新版本的 Dashboard 会被默认安装在 `kubernetes-dashboard` 这个命名空间下面：

```shell
[root@master ~]# kubectl get pods -n kubernetes-dashboard -o wide
NAME                                         READY   STATUS    RESTARTS   AGE   IP           NODE    NOMINATED NODE   READINESS GATES
dashboard-metrics-scraper-64bcc67c9c-4wcbg   1/1     Running   0          10m   10.244.2.3   node2   <none>           <none>
kubernetes-dashboard-5c8bd6b59-brr2g         1/1     Running   0          10m   10.244.2.2   node2   <none>           <none>
```

然后查看 Dashboard 的 NodePort 端口：

```shell
[root@master ~]# kubectl get svc -n kubernetes-dashboard
NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
dashboard-metrics-scraper   ClusterIP   10.110.118.94   <none>        8000/TCP        11m
kubernetes-dashboard        NodePort    10.99.173.133   <none>        443:31818/TCP   11m
[root@master ~]#
```

然后可以通过上面的 31818 端口去访问 Dashboard，要记住使用 https，Chrome 不生效可以使用 Firefox 测试，如果 Firefox 打不开页面，可以点击下页面中的信任证书即可，信任后就可以访问到 Dashboard 的登录页面了。

然后创建一个具有全局所有权限的用户来登录 Dashboard：(admin.yaml)

```yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: admin-user
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
subjects:
  - kind: ServiceAccount
    name: admin-user
    namespace: kubernetes-dashboard
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
```

直接创建即可：

```shell
~ kubectl apply -f admin.yaml
```

现在我们需要找到可以用来登录的令牌，可以使用 `kubectl create token` 命令来请求一个 service account token：

```shell
# 请求创建一个 token 作为 kubernetes-dashboard 命名空间中的 admin-user 这个 sa 对 kubeapiserver 进行身份验证
[root@master ~]# kubectl -n kubernetes-dashboard create token admin-user
eyJhbGciOiJSUzI1NiIsImtpZCI6IjRMcGFKbEFtZmlKWEtNQ09VZTd4NEN3S0pSN3NCYUZoMkloYlJDU0Z4ZnMifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzA4MzQwNzE0LCJpYXQiOjE3MDgzMzcxMTQsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiOWZiZDM3ZmItNmIwMi00NjhiLWJjNDItZjhjY2MxYzBhNzQwIn19LCJuYmYiOjE3MDgzMzcxMTQsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbi11c2VyIn0.fySqfx2bcrY_4Y7PyuBTZX1f5LjH63A9p0iTqacorafjC470DxAaRwOewDChYFPy9G4P-0aJcrr-QGW2PIBRlyyCKqiou3KUM0QmouctpuqrfY53tYGPb90nArvuovgeLlw1bpEpIxlIFZNNE2nHZ-MPUSM4Auxuheo0AXOLpnfKXPKRIVkemVah5p5nZ_nbRgem05Vj3chiFIziZ8Z35zgYVIw8HcO5Z8KOrpEEx7N45ohISZajzneyC3scTcBK0jREDaDTLBd0Qh75OQqq_E7Wo0K6eFapNNzkPHgwDY6vvZF8Y-IJ9x_8up2xOmafKSWi27mllKSgLT6Gm5Q1QQ
[root@master ~]#
```

上面的命令执行后会打印出token，然后用上面的字符串作为 token 登录 Dashboard 即可。

![image-20240927181949991](image/image-20240927181949991.png)

到这里就完成了使用 kubeadm 搭建 v1.25.4 版本的 kubernetes 集群。

### 清理

如果集群安装过程中遇到了其他问题，可以使用下面的命令来进行重置：

```shell
☸ ➜ kubeadm reset
☸ ➜ ifconfig cni0 down && ip link delete cni0
☸ ➜ ifconfig flannel.1 down && ip link delete flannel.1
☸ ➜ rm -rf /var/lib/cni/
```



## Ubuntu22.04

> https://blog.csdn.net/m0_53928179/article/details/139068769
>

按照上面的教程基本没有问题，但是需要注意的是，containerd安装完成之后需要

- 镜像加速：https://blog.csdn.net/IOT_AI/article/details/131975562
- 修改/etc/containerd/config.toml中sanbox_image的内容，从原来的registry.k8s.io/pause:3.8修改registry.aliyuncs.com/google_containers/pause:3.9

`registry.aliyuncs.com/google_containers` 是国内常用镜像源，但需确认其是否包含 `v1.30.14` 镜像

```shell
root@master:~/yamlDir/01-pod# kubeadm config images list --kubernetes-version="v1.30.14" \
  --image-repository=registry.aliyuncs.com/google_containers
registry.aliyuncs.com/google_containers/kube-apiserver:v1.30.14
registry.aliyuncs.com/google_containers/kube-controller-manager:v1.30.14
registry.aliyuncs.com/google_containers/kube-scheduler:v1.30.14
registry.aliyuncs.com/google_containers/kube-proxy:v1.30.14
registry.aliyuncs.com/google_containers/coredns:v1.11.3
registry.aliyuncs.com/google_containers/pause:3.9
registry.aliyuncs.com/google_containers/etcd:3.5.15-0

```



## Kind

[Kind](https://kind.sigs.k8s.io) 是 Kubernetes in Docker 的简写，是一个使用 Docker 容器作为 Node 节点，在本地创建和运行 Kubernetes 集群的工具。使用 Kind 搭建的集群无法在生产中使用，但是如果你只是想在本地简单的玩玩 K8s，不想占用太多的资源，那么使用 Kind 是你不错的选择。

Kind 内部也是使用 Kubeadm 创建和启动集群节点，并使用 Containerd 作为容器运行时，所以弃用 dockershim 对 Kind 没有什么影响。

Kind 的架构图如下所示，它将 Docker 容器作为 Kubernetes 的 Node 节点，并在该 Node 中安装 Kubernetes 组件，包括一个或者多个 Control Plane 和一个或者多个 Work Nodes。这就解决了在本机运行多个 Node 的问题，而不需要虚拟化。

![image-20240927182701536](image/image-20240927182701536.png)



### 安装

要使用 Kind 的前提是提供一个 Docker 环境，可以使用下面的命令快速安装。

```shell
sudo sh -c "$(curl -fsSL https://get.docker.com)"
```

Docker 安装之后接下来可以安装一个 kubectl 工具，Kind 本身不需要 kubectl，安装 kubectl 可以在本机直接管理 Kubernetes 集群。

```shell
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
#验证版本
kubectl version --client
```

接下来就可以安装 Kind 了，最简单的方式是在 [Github Release 页面](https://github.com/kubernetes-sigs/kind/releases) 直接下载对应的安装包即可，比如我们这里安装最新的 v0.17.0 版本：

```shell
☸ ➜ wget https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-amd64
☸ ➜ chmod +x kind-linux-amd64
☸ ➜ sudo mv kind-linux-amd64 /usr/local/bin/kind
# 验证版本
☸ ➜ kind version
kind v0.17.0 go1.19.2 darwin/amd64
```

### 版本约束

使用kind创建k8s集群时一定要注意kind版本与k8s集群版本之间的关系，具体可以在 https://github.com/kubernetes-sigs/kind/releases 中查看，不然会出现莫名其妙的问题。



### 操作

要了解 Kind 的操作，最简单的方式就是直接执行一条 kind 命令，或者 kind -h，这也是我们了解任何 CLI 工具最基础的方法：

```shell
[root@localhost ~]# kind
kind creates and manages local Kubernetes clusters using Docker container 'nodes'

Usage:
  kind [command]

Available Commands:
  build       Build one of [node-image]
  completion  Output shell completion code for the specified shell (bash, zsh or fish)
  create      Creates one of [cluster]
  delete      Deletes one of [cluster]
  export      Exports one of [kubeconfig, logs]
  get         Gets one of [clusters, nodes, kubeconfig]
  help        Help about any command
  load        Loads images into nodes
  version     Prints the kind CLI version

Flags:
  -h, --help              help for kind
      --loglevel string   DEPRECATED: see -v instead
  -q, --quiet             silence all stderr output
  -v, --verbosity int32   info log verbosity, higher value produces more output
      --version           version for kind

Use "kind [command] --help" for more information about a command.
[root@localhost ~]#
```

从上面的命令可以看出 kind 工具包含很多可用的命令，比如 build、create、delete、load 等等，其中最重要的应该属于 create 命令了，该命令可以用来创建一个集群，用同样的方式可以继续查看 `kind create` 命令的使用方法：

```shell
[root@localhost ~]# kind create -h
Creates one of local Kubernetes cluster (cluster)

Usage:
  kind create [flags]
  kind create [command]

Available Commands:
  cluster     Creates a local Kubernetes cluster

Flags:
  -h, --help   help for create

Global Flags:
      --loglevel string   DEPRECATED: see -v instead
  -q, --quiet             silence all stderr output
  -v, --verbosity int32   info log verbosity, higher value produces more output

Use "kind create [command] --help" for more information about a command.
[root@localhost ~]#
```

可以看出只有一个 cluster 子命令可用，但是该子命令后面如何操作呢？

```shell
[root@localhost ~]# kind create cluster  -h
Creates a local Kubernetes cluster using Docker container 'nodes'

Usage:
  kind create cluster [flags]

Flags:
      --config string       path to a kind config file
  -h, --help                help for cluster
      --image string        node docker image to use for booting the cluster
      --kubeconfig string   sets kubeconfig path instead of $KUBECONFIG or $HOME/.kube/config
  -n, --name string         cluster name, overrides KIND_CLUSTER_NAME, config (default kind)
      --retain              retain nodes for debugging when cluster creation fails
      --wait duration       wait for control plane node to be ready (default 0s)

Global Flags:
      --loglevel string   DEPRECATED: see -v instead
  -q, --quiet             silence all stderr output
  -v, --verbosity int32   info log verbosity, higher value produces more output
[root@localhost ~]#
```

可以看到 `create cluster` 后面没有可用的子命令了，但是有一些 Flags 标志可以传递，但其实不传递任何的参数也可以创建一个 K8s 集群，这属于最简单创建 K8s 的方式，只需要执行如下所示的命令即可创建一个默认的集群（目前最新版本只支持到 K8s v1.25.3 版本。kind默认创建的k8s集群node镜像版本可以在release中查看）：

```shell
[root@localhost ~]# kind create cluster
Creating cluster "kind" ... # 默认名字为kind
⠈⠑ Ensuring node image (kindest/node:v1.25.3) 🖼
 ✓ Ensuring node image (kindest/node:v1.25.3) 🖼
 ✓ Preparing nodes 📦
 ✓ Writing configuration 📜
 ✓ Starting control-plane 🕹️
 ✓ Installing CNI 🔌
 ✓ Installing StorageClass 💾
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Not sure what to do next? 😅  Check out https://kind.sigs.k8s.io/docs/user/quick-start/
[root@localhost ~]# kubectl cluster-info --context kind-kind125
Kubernetes control plane is running at https://127.0.0.1:39925
CoreDNS is running at https://127.0.0.1:39925/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
[root@localhost ~]#
```

创建完成后就可以直接使用 kubectl 命令管理该 K8S 集群了：

```shell
[root@localhost ~]# kubectl get nodes -owide
NAME                 STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane   37m   v1.25.3   172.19.0.2    <none>        Ubuntu 22.04.1 LTS   3.10.0-1160.59.1.el7.x86_64   containerd://1.6.9
[root@localhost ~]# kubectl get pods -A -owide
NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE   IP           NODE                 NOMINATED NODE   READINESS GATES
kube-system          coredns-565d847f94-dzh6b                     1/1     Running   0          37m   10.244.0.3   kind-control-plane   <none>           <none>
kube-system          coredns-565d847f94-vm88g                     1/1     Running   0          37m   10.244.0.2   kind-control-plane   <none>           <none>
kube-system          etcd-kind-control-plane                      1/1     Running   0          37m   172.19.0.2   kind-control-plane   <none>           <none>
kube-system          kindnet-9qqlb                                1/1     Running   0          37m   172.19.0.2   kind-control-plane   <none>           <none>
kube-system          kube-apiserver-kind-control-plane            1/1     Running   0          37m   172.19.0.2   kind-control-plane   <none>           <none>
kube-system          kube-controller-manager-kind-control-plane   1/1     Running   0          37m   172.19.0.2   kind-control-plane   <none>           <none>
kube-system          kube-proxy-vrfpf                             1/1     Running   0          37m   172.19.0.2   kind-control-plane   <none>           <none>
kube-system          kube-scheduler-kind-control-plane            1/1     Running   0          37m   172.19.0.2   kind-control-plane   <none>           <none>
local-path-storage   local-path-provisioner-684f458cdd-pb6nv      1/1     Running   0          37m   10.244.0.4   kind-control-plane   <none>           <none>
[root@localhost ~]#
```

默认的集群名称为 kind，在创建的时候可以使用参数 `--name` 指定创建的集群名称，可以创建多个群集：

```shell
kind create cluster --name kind-2
```

此外还可以指定启动集群的 Node 镜像：

```shell
kind create cluster --name kind-3 --image kindest/node:v1.23.4 # 这个node版本不能随便填，需参考对应kind版本的release中的说明
```

然后可以使用 `kind get clusters` 命令来获取集群列表：

```shell
[root@localhost ~]# kind get clusters
kind
kind-2
[root@localhost ~]#
```

当有多个集群的时候，可以使用 kubectl 来切换要管理的集群：

```shell
# 切换到集群 `kind`
☸ ➜ kubectl config use-context kind-kind
# 切换到群集`kind-2`
☸ ➜ kubectl config use-context kind-kind-2
```

要删除集群也非常简单，比如要删除 kind-2 集群：

```shell
kind delete cluster --name kind-2
```

启动一个k8s集群时，这个集群一共有多少个节点就会启动多少个容器，进入到容器内部通过ctr操作容器运行时，可以看到集群下载的镜像。

```shell
[root@localhost ~]# docker ps # 下面两个容器表示两个集群
CONTAINER ID   IMAGE                  COMMAND                  CREATED             STATUS             PORTS                       NAMES
aac1349e6750   kindest/node:v1.25.3   "/usr/local/bin/entr…"   44 minutes ago      Up 44 minutes      127.0.0.1:37261->6443/tcp   kind-2-control-plane
2c9441f356c8   kindest/node:v1.25.3   "/usr/local/bin/entr…"   About an hour ago   Up About an hour   127.0.0.1:43440->6443/tcp   kind-control-plane
[root@localhost ~]# docker exec -it aac1349e6750 /bin/bash # 进入到容器中
root@kind-2-control-plane:/# ctr ns ls
NAME   LABELS
k8s.io
root@kind-2-control-plane:/# ctr -n k8s.io i ls -q
docker.io/kindest/kindnetd:v20221004-44d545d1
docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
import-2022-10-25@sha256:0dae4b69c2aa90e6c24691ebbe2e860e2a1ae68463a622c627fb58110153d950
......
registry.k8s.io/coredns/coredns:v1.9.3
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/pause:3.7
sha256:221177c6082a88ea4f6240ab2450d540955ac6f4d5454f0e15751b653ebda165
......
root@kind-2-control-plane:/#
```

Kind 集群中的镜像可以从互联网直接拉取，有时候可能比较缓慢，我们可以将本机镜像导入到 Kind 集群中去，比如使用如下命令可以将镜像导入到 kind-control-plane 节点去：

```shell
[root@localhost ~]# kubectl config use-context kind-kind
Switched to context "kind-kind".
[root@localhost ~]# kind load docker-image --nodes kind-control-plane nginx:mainline-alpine
Image: "" with ID "sha256:cc44224bfe208a46fbc45471e8f9416f66b75d6307573e29634e7f42e27a9268" not yet present on node "kind-control-plane", loading...
[root@localhost ~]# docker exec -it 2c9441f356c8 /bin/bash
root@kind-control-plane:/#  ctr -n k8s.io i ls -q
docker.io/kindest/kindnetd:v20221004-44d545d1
docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
docker.io/library/nginx:mainline-alpine # 可以看到被导入
import-2022-10-25@sha256:0dae4b69c2aa90e6c24691ebbe2e860e2a1ae68463a622c627fb58110153d950
......
sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
root@kind-control-plane:/#
```



### 配置集群

上面介绍的是 kind 命令的一些常用操作，此外我们还可以通过一个文件来配置要创建的 K8s 集群，比如定义一个如下所示的 config.yaml 文件：

```yaml
# config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: demo
nodes:
  - role: control-plane
  - role: worker
  - role: worker
```

该配置文件表示一共要创建 3 个节点，一个控制节点，两个工作节点，在创建集群的时候只需要通过 `--config` 参数指定该文件即可：

```shell
kind create cluster --config config.yaml
```

创建后的集群名称为 demo，一共包括 3 个节点：

```shell
[root@localhost ~]# docker ps
CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS         PORTS                       NAMES
4ea1388b90af   kindest/node:v1.25.3   "/usr/local/bin/entr…"   2 minutes ago   Up 2 minutes                               demo-worker
66d186535b98   kindest/node:v1.25.3   "/usr/local/bin/entr…"   2 minutes ago   Up 2 minutes   127.0.0.1:41158->6443/tcp   demo-control-plane
1999e538b85b   kindest/node:v1.25.3   "/usr/local/bin/entr…"   2 minutes ago   Up 2 minutes                               demo-worker2
[root@localhost ~]#
```

如果想创建一个 HA 模式的控制平面，那么可以定义如下所示的配置文件，只需要指定 3 个(奇数个) control-plane 角色的节点即可：

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: control-plane
  - role: control-plane
  - role: worker
  - role: worker
  - role: worker
```

此外还可以将 Node 的端口映射到宿主机上，通过配置文件中的 `extraPortMappings` 属性可以实现该功能，如下所示配置可以将 control-plane 节点 80 端口映射到宿主机的 80 端口上：

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
    extraPortMappings:
      - containerPort: 80
        hostPort: 80
        listenAddress: "0.0.0.0" # Optional, defaults to "0.0.0.0"
        protocol: udp # Optional, defaults to tcp
```

如果要将端口映射与 NodePort 一起使用，kind 节点的 containerPort 和 Service 的 nodePort 需要相等：

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
    extraPortMappings:
      - containerPort: 30950
        hostPort: 80
```

然后将 nodePort 设置为 30950：

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: foo
  labels:
    app: foo
spec:
  containers:
  - name: foo
    image: hashicorp/http-echo:0.2.3
    args:
      - "-text=foo"
    ports:
      - containerPort: 5678
---
apiVersion: v1
kind: Service
metadata:
  name: foo
spec:
  type: NodePort
  ports:
    - name: http
      nodePort: 30950
      port: 5678
  selector:
    app: foo
```

同样也可以在配置文件中指定 Node 的容器镜像版本运行指定版本的 Kubernetes 群集。可以在[官方 release 页面](https://github.com/kubernetes-sigs/kind/releases)中查找需要镜像 tag，带上 sha256 shasum（非必须），例如：

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
    image: kindest/node:v1.18.15@sha256:5c1b980c4d0e0e8e7eb9f36f7df525d079a96169c8a8f20d8bd108c0d0889cc4
  - role: worker
    image: kindest/node:v1.18.15@sha256:5c1b980c4d0e0e8e7eb9f36f7df525d079a96169c8a8f20d8bd108c0d0889cc4
```

此外还有一些其他定制操作，比如 Kind 创建的集群默认自带一个轻量级的 CNI 插件 kindnetd，我们也可以禁用默认设置来安装其他 CNI，比如 Calico：

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  # the default CNI will not be installed
  disableDefaultCNI: true
```

还可以在 iptables 和 ipvs 之间配置将要使用的 kube-proxy 模式，默认情况下使用 iptables：

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  kubeProxyMode: "ipvs"
```

另外可以将宿主机的路径挂载到某个节点上用于数据持久化：

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  # add a mount from /path/to/my/files on the host to /files on the node
    extraMounts:
    - hostPath: /path/to/my/files
      containerPath: /files
```

我们还可以给节点定制不同的标签，这对于节点筛选非常有用，只需要在节点中添加 labels 配置即可：

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
    extraPortMappings:
      - containerPort: 30950
        hostPort: 80
    labels:
      tier: frontend
  - role: worker
    labels:
      tier: backend
```

Kind 使用 Kubeadm 来配置的集群节点，它会在第一个控制平面节点上运行 kubeadm init 命令，可以使用 kubeadm InitConfiguration 来进行一些定制：

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
    kubeadmConfigPatches:
    - |
      kind: InitConfiguration
      nodeRegistration:
        kubeletExtraArgs:
          node-labels: "my-label=true"
```

如果想进行更多的定制，那么在 kubeadm init 期间有四种配置类型可用: InitConfiguration、 ClusterConfiguration、 KubeProxyConfiguration、 KubeletConfiguration。例如，可以使用 kubeadm ClusterConfiguration 来覆盖 apiserver 标志：

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
    kubeadmConfigPatches:
      - |
        kind: ClusterConfiguration
        apiServer:
          extraArgs:
            enable-admission-plugins: NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook
```

在 Kind 集群中的 worker 或控制平面(在 HA 模式下)节点上，Kind 会执行 kubeadm join 命令，我们也可以使用 JoinConfiguration (spec) 来进行定制：

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
    kubeadmConfigPatches:
      - |
        kind: JoinConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "my-label2=true"
  - role: control-plane
    kubeadmConfigPatches:
      - |
        kind: JoinConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "my-label3=true"
```

此外 Kind 还有很多其他的实践方式，在后续课程中我们也会慢慢接触到。
在前面的学习中我们使用了一个 `kubectl scale` 命令可以来实现 Pod 的扩缩容功能，但是这个是完全手动操作的，要应对线上的各种复杂情况，我们需要能够做到自动化去感知业务，来自动进行扩缩容。为此，Kubernetes 也为我们提供了这样的一个资源对象：Horizontal Pod Autoscaling（Pod 水平自动伸缩），简称 HPA，HPA 通过监控分析一些控制器控制的所有 Pod 的负载变化情况来确定是否需要调整 Pod 的副本数量，在 Kubernetes 控制平面内运行的 HPA 控制器会定期调整其目标（例如：Deployment）的所需规模，以匹配观察到的指标，例如，平均 CPU 利用率、平均内存利用率或你指定的任何其他自定义指标。



# 工作原理

对于按 Pod 统计的资源指标（如 CPU），控制器从资源指标 API 中获取 HorizontalPodAutoscaler 指定的 Pod 的 metrics 值，如果设置了目标使用率，控制器获取每个 Pod 中的容器资源使用情况，并计算资源使用率。如果设置了 target 值，将直接使用原始数据（不再计算百分比）。接下来，控制器根据平均的资源使用率或原始值计算出扩缩的比例，进而计算出目标副本数。

------

对于上面这段话，个人认为说的不是很清楚，我们看看GPT如何阐述的

在 Kubernetes 中，Horizontal Pod Autoscaler (HPA) 是负责根据指定的性能指标自动调整 Pod 副本数量的控制器。当指标是逐个 Pod 汇报的资源（例如 CPU 使用率），HPA 的工作流程大致如下：

1. 控制器通过资源指标 API 获取每个 Pod 的指标数据，这些 Pod 是被 HPA 所监控的。
2. 如果 HPA 配置中指定了目标使用率（例如 CPU 使用率的目标值），控制器将会检查每个 Pod 内各个容器的资源消耗，并基于这些数据计算出平均的资源使用率。
3. 若 HPA 配置中设定了一个具体的目标值（target value），而不是使用率，控制器将使用这个原始指标数据而不进行任何百分比计算。
4. 控制器根据计算出的平均资源使用率或者原始指标数据来决定应该如何调整 Pod 的副本数量。这个调整取决于实际使用量与预设目标之间的差异。
5. 最后，控制器会根据这个比例计算出新的目标副本数，并相应地扩大或缩小 Pod 的副本数量，以满足当前的资源需求。

简而言之，HPA 会定期检查每个 Pod 的资源消耗情况，并根据设定的目标（使用率或原始值）自动调整 Pod 副本的数量，确保应用程序的性能和资源效率。

------

需要注意的是，如果 Pod 某些容器不支持资源采集，那么控制器将不会使用该 Pod 的 CPU 使用率。

当 Kubernetes 中的 Pod 使用自定义指标进行自动扩缩时，控制器遵循的机制与使用资源指标（例如 CPU 和内存）时类似，但有一个关键区别：自定义指标仅关注原始值，而不考虑使用率。这意味着，根据原始指标数据直接计算 Pod 的扩缩比例，而不是先将数据转换为使用率。

对于使用对象指标和外部指标的情况，Horizontal Pod Autoscaler (HPA) 直接将这些指标与设定的目标值进行比较，来确定是否需要扩缩Pod副本数。对象指标描述的是Kubernetes集群内的对象（例如服务的请求数），而外部指标可能来源于集群外部（例如来自云服务监控的指标）。

在 Kubernetes 的 autoscaling/v2beta2 API 版本中，这些指标不仅可以使用原始值进行比较，还可以根据 Pod 的数量均匀分配目标值后计算。这种方式允许控制器考虑到每个 Pod 应承担的目标指标份额，使得自动扩缩决策更加精细。

Metrics API 提供了对这些不同类型指标的支持，并为它们提供稳定性保证和维护状态。这是 HPA 能够访问和使用这些指标的基础。

HorizontalPodAutoscaler 控制器与 Kubernetes 集群中的工作负载资源（如 Deployment 和 StatefulSet）交互时，它们每个都暴露了一个名为 scale 的子资源接口。通过这个接口，HPA 可以动态调整目标工作负载的副本数量，并获取其当前状态。基本上，scale 接口使得控制器能够管理工作负载的扩缩，而不需要直接与具体的 Pod 交互。

总结来说，无论是使用资源指标、自定义指标、对象指标还是外部指标，HPA 都可以根据这些指标的实时数据来自动调整 Pod 副本的数量。这样做有助于保持应用程序性能和资源使用的最优化，确保只有在需要时才增加资源，并在不需要时减少资源。



# Metrics Server

Kubernetes 的 Pod 水平自动扩缩（Horizontal Pod Autoscaler，HPA）是通过一个周期性运行的控制回路来实现的，并非一个连续的过程。这个回路的运行间隔可以通过 kube-controller-manager 的 --horizontal-pod-autoscaler-sync-period 参数来设定，默认情况下，这个间隔是 15 秒。

在每次控制回路的周期中，控制器管理器会检查每个 HPA 对象，并根据这些对象中指定的指标来查询资源使用率。具体步骤如下：

1. 目标资源定位：控制器管理器会查找每个 HPA 对象中 scaleTargetRef 字段定义的目标资源。这个字段标识了要自动扩缩的 Kubernetes 资源，比如一个 Deployment 或 StatefulSet。
2. 选取 Pod：控制器利用目标资源的 .spec.selector 字段中定义的标签选择器来选取一组匹配的 Pod。这些 Pod 将用于计算平均资源使用率或其他指标。
3. 获取指标数据：

HPA 通常从三个聚合 API 中获取这些指标数据：

- metrics.k8s.io：这个 API 提供了每个 Pod 的资源使用数据，通常由一个名为 Metrics Server 的组件提供。Metrics Server 需要被单独部署并启动，它聚集了集群内部的监控数据，并通过标准的 Kubernetes API 暴露这些数据。
- custom.metrics.k8s.io：这个 API 用于提供自定义指标数据，是适用于不基于每个 Pod 的指标。
- external.metrics.k8s.io：这个 API 用于提供来自集群外部的指标，比如来自其他监控系统的数据。

有了 Metrics Server，集群管理员和用户就可以通过 Kubernetes 标准的 API 接口来访问监控数据。这样做的好处是可以使用 Kubernetes 生态内的工具和客户端库来获取和处理监控数据，而不需要依赖于特定的监控解决方案的 API。这为 Kubernetes 的监控和自动扩缩提供了一种统一和标准化的方法。

```
https://10.96.0.1/apis/metrics.k8s.io/v1beta1/namespaces/<namespace-name>/pods/<podname>
```

比如当我们访问上面的 API 的时候，我们就可以获取到该 Pod 的资源数据，这些数据其实是来自于 kubelet的 Summary API 采集而来的。不过需要说明的是我们这里可以通过标准的 API 来获取资源监控数据，并不是因为 Metrics Server 就是 APIServer 的一部分，而是通过 Kubernetes 提供的 Aggregator 汇聚插件来实现的，是独立于 APIServer 之外运行的。

<img src="image/image-20241019223610282.png" alt="image-20241019223610282" style="zoom:67%;" />

## 聚合API

Aggregator 允许开发人员编写一个自己的服务，把这个服务注册到 Kubernetes 的 APIServer 里面去，这样我们就可以像原生的 APIServer 提供的 API 使用自己的 API 了，我们把自己的服务运行在 Kubernetes 集群里面，然后 Kubernetes 的 Aggregator 通过 Service 名称就可以转发到我们自己写的 Service 里面去了。这样这个聚合层就带来了很多好处：

- 增加了 API 的扩展性，开发人员可以编写自己的 API 服务来暴露他们想要的 API。
- 丰富了 API，核心 kubernetes 团队阻止了很多新的 API 提案，通过允许开发人员将他们的 API 作为单独的服务公开，这样就无须社区繁杂的审查了。
- 开发分阶段实验性 API，新的 API 可以在单独的聚合服务中开发，当它稳定之后，在合并会 APIServer 就很容易了。
- 确保新 API 遵循 Kubernetes 约定，如果没有这里提出的机制，社区成员可能会被迫推出自己的东西，这样很可能造成社区成员和社区约定不一致。



## 安装

所以现在我们要使用 HPA，就需要在集群中安装 Metrics Server 服务，要安装 Metrics Server 就需要开启 Aggregator，因为 Metrics Server 就是通过该代理进行扩展的，不过我们集群是通过 Kubeadm 搭建的，默认已经开启了，如果是二进制方式安装的集群，需要单独配置 kube-apsierver 添加如下所示的参数：

```shell
--requestheader-client-ca-file=<path to aggregator CA cert>
--requestheader-allowed-names=aggregator
--requestheader-extra-headers-prefix=X-Remote-Extra-
--requestheader-group-headers=X-Remote-Group
--requestheader-username-headers=X-Remote-User
--proxy-client-cert-file=<path to aggregator proxy cert>
--proxy-client-key-file=<path to aggregator proxy key>
```

如果 kube-proxy 没有和 APIServer 运行在同一台主机上，那么需要确保启用了如下 kube-apsierver 的参数：

```shell
--enable-aggregator-routing=true
```

对于这些证书的生成方式，我们可以查看官方文档：https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/master/docs/concepts/auth.md。

Aggregator 聚合层启动完成后，就可以来安装 Metrics Server 了，我们可以获取该仓库的官方安装资源清单：

```shell
# 官方仓库地址：https://github.com/kubernetes-sigs/metrics-server
☸ ➜ wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.2/components.yaml
```

在部署之前，修改 components.yaml 的镜像地址为：

```
hostNetwork: true  # 使用hostNetwork模式
containers:
- name: metrics-server
  image: cnych/metrics-server:v0.5.1
```

等部署完成后，可以查看 Pod 日志是否正常：

```shell
[root@master yamlDir]# kubectl apply -f components.yaml
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
[root@master yamlDir]# kubectl get apiservice
NAME                                   SERVICE                      AVAILABLE                  AGE

v1.storage.k8s.io                      Local                        True                       7d1h

v1beta1.metrics.k8s.io                 kube-system/metrics-server   False (MissingEndpoints)   5m50s
..
[root@master yamlDir]#  kubectl get pods -n kube-system -l k8s-app=metrics-server
NAME                             READY   STATUS    RESTARTS   AGE
metrics-server-df98fb78f-9b4dv   0/1     Running   0          85s
[root@master yamlDir]# kubectl logs -f metrics-server-df98fb78f-9b4dv -n kube-system
I0226 09:27:38.775961       1 serving.go:342] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key)
E0226 09:27:39.109894       1 scraper.go:140] "Failed to scrape node" err="Get \"https://192.168.220.147:10250/metrics/resource\": x509: cannot validate certificate for 192.168.220.147 because it doesn't contain any IP SANs" node="node1"
I0226 09:27:39.112388       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0226 09:27:39.112435       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0226 09:27:39.112487       1 secure_serving.go:266] Serving securely on [::]:4443
I0226 09:27:39.112512       1 dynamic_serving_content.go:131] "Starting controller" name="serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key"
I0226 09:27:39.112699       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0226 09:27:39.112709       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController
I0226 09:27:39.112727       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0226 09:27:39.112730       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0226 09:27:39.113642       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0226 09:27:39.113649       1 shared_informer.go:372] The sharedIndexInformer has started, run more than once is not allowed
E0226 09:27:39.129327       1 scraper.go:140] "Failed to scrape node" err="Get \"https://192.168.220.146:10250/metrics/resource\": x509: cannot validate certificate for 192.168.220.146 because it doesn't contain any IP SANs" node="master"
E0226 09:27:39.147903       1 scraper.go:140] "Failed to scrape node" err="Get \"https://192.168.220.148:10250/metrics/resource\": x509: cannot validate certificate for 192.168.220.148 because it doesn't contain any IP SANs" node="node2"
I0226 09:27:39.212650       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0226 09:27:39.212769       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0226 09:27:39.212874       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController
E0226 09:27:54.122512       1 scraper.go:140] "Failed to scrape node" err="Get \"https://192.168.220.146:10250/metrics/resource\": x509: cannot validate certificate for 192.168.220.146 because it doesn't contain any IP SANs" node="master"
E0226 09:27:54.124524       1 scraper.go:140] "Failed to scrape node" err="Get \"https://192.168.220.147:10250/metrics/resource\": x509: cannot validate certificate for 192.168.220.147 because it doesn't contain any IP SANs" node="node1"
E0226 09:27:54.129561       1 scraper.go:140] "Failed to scrape node" err="Get \"https://192.168.220.148:10250/metrics/resource\": x509: cannot validate certificate for 192.168.220.148 because it doesn't contain any IP SANs" node="node2"
I0226 09:28:03.655805       1 server.go:187] "Failed probe" probe="metric-storage-ready" err="no metrics to serve"
E0226 09:28:09.119735       1 scraper.go:140] "Failed to scrape node" err="Get \"https://192.168.220.148:10250/metrics/resource\": x509: cannot validate certificate for 192.168.220.148 because it doesn't contain any IP SANs" node="node2"
```

因为部署集群的时候，CA 证书并没有把各个节点的 IP 签上去，所以这里 Metrics Server 通过 IP 去请求时，提示签的证书没有对应的 IP（错误：x509: cannot validate certificate for xxxx because it doesn't contain any IP SANs），我们可以添加一个 --kubelet-insecure-tls 参数跳过证书校验：

```shell
args:
- --cert-dir=/tmp
- --secure-port=443
- --kubelet-insecure-tls
- --kubelet-preferred-address-types=InternalIP
```

然后再重新安装即可成功！可以通过如下命令来验证：

```shell
➜  ~ kubectl apply -f components.yaml
[root@master yamlDir]#  kubectl get pods -n kube-system -l k8s-app=metrics-server
NAME                              READY   STATUS    RESTARTS   AGE
metrics-server-7c5487d558-sr5vp   1/1     Running   0          3m13s
[root@master yamlDir]# kubectl get apiservice
NAME                                   SERVICE                      AVAILABLE   AGE
v1beta1.metrics.k8s.io                 kube-system/metrics-server   True        15m
...
```

现在我们可以通过 kubectl top 命令来获取到资源数据了，证明 Metrics Server 已经安装成功了。

# HPA

现在我们用 Deployment 来创建一个 Nginx Pod，然后利用 HPA 来进行自动扩缩容。资源清单如下所示：

```yaml
# hpa-demo.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpa-demo
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 80
```

然后直接创建 Deployment，注意一定先把之前创建的具有 app=nginx 的 Pod 先清除掉：

```shell
➜  ~ kubectl apply -f hpa-demo.yaml
deployment.apps/hpa-demo created
➜  ~ kubectl get pods -l app=nginx
NAME                        READY   STATUS    RESTARTS   AGE
hpa-demo-7848d4b86f-khndb   1/1     Running   0          56s
```

现在我们来创建一个 HPA 资源对象，可以使用 kubectl autoscale 命令来创建：

```shell
➜  ~ kubectl autoscale deployment hpa-demo --cpu-percent=10 --min=1 --max=10
horizontalpodautoscaler.autoscaling/hpa-demo autoscaled
➜  ~ kubectl get hpa
NAME       REFERENCE             TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   <unknown>/10%   1         10        0          6s
```

此命令创建了一个关联资源 hpa-demo 的 HPA，最小的 Pod 副本数为 1，最大为 10。HPA 会根据设定的 cpu 使用率（10%）动态的增加或者减少 Pod 数量。使用 kubectl autoscale 命令创建的 Horizontal Pod Autoscaler (HPA) 对象默认会继承被扩缩资源的名称。因此，如果您没有显式指定 HPA 对象的名称，HPA 的名称将与目标 Deployment 的名称相同。如果您想要指定不同于 Deployment 名称的 HPA 对象名称，可以通过创建 HPA 的 YAML 配置文件来实现。然而，使用 kubectl autoscale 命令将不提供这样的选项，它简化了 HPA 创建流程，使用目标资源的名称作为 HPA 对象的名称。

当然我们依然还是可以通过创建 YAML 文件的形式来创建 HPA 资源对象。如果我们不知道怎么编写的话，可以查看上面命令行创建的 HPA 的 YAML 文件：

```shell
[root@master yamlDir]# kubectl get hpa hpa-demo -o yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: "2024-02-26T10:01:27Z"
  name: hpa-demo
  namespace: default
  resourceVersion: "857928"
  uid: 5eea4cf9-14ec-45d1-aef0-a2a2c4f48b5e
spec:
  maxReplicas: 10
  metrics:
  - resource:
      name: cpu
      target:
        averageUtilization: 10
        type: Utilization
    type: Resource
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: hpa-demo
status:
  conditions:
  - lastTransitionTime: "2024-02-26T10:01:42Z"
    message: the HPA controller was able to get the target's current scale
    reason: SucceededGetScale
    status: "True"
    type: AbleToScale
  - lastTransitionTime: "2024-02-26T10:01:42Z"
    message: 'the HPA was unable to compute the replica count: failed to get cpu utilization:
      missing request for cpu'
    reason: FailedGetResourceMetric
    status: "False"
    type: ScalingActive
  currentMetrics: null
  currentReplicas: 1
  desiredReplicas: 0
[root@master yamlDir]#
```

然后我们可以根据上面的 YAML 文件就可以自己来创建一个基于 YAML 的 HPA 描述文件了。但是我们发现上面信息里面出现了一些 Fail 信息，我们来查看下这个 HPA 对象的信息：

```shell
[root@master yamlDir]# kubectl describe hpa hpa-demo
Warning: autoscaling/v2beta2 HorizontalPodAutoscaler is deprecated in v1.23+, unavailable in v1.26+; use autoscaling/v2 HorizontalPodAutoscaler
Name:                                                  hpa-demo
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Mon, 26 Feb 2024 05:01:27 -0500
Reference:                                             Deployment/hpa-demo
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  <unknown> / 10%
Min replicas:                                          1
Max replicas:                                          10
Deployment pods:                                       1 current / 0 desired
Conditions:
  Type           Status  Reason                   Message
  ----           ------  ------                   -------
  AbleToScale    True    SucceededGetScale        the HPA controller was able to get the target's current scale
  ScalingActive  False   FailedGetResourceMetric  the HPA was unable to compute the replica count: failed to get cpu utilization: missing request for cpu
Events:
  Type     Reason                        Age                   From                       Message
  ----     ------                        ----                  ----                       -------
  Warning  FailedComputeMetricsReplicas  47s (x12 over 3m32s)  horizontal-pod-autoscaler  invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu
  Warning  FailedGetResourceMetric       32s (x13 over 3m32s)  horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu
[root@master yamlDir]#

```

我们可以看到上面的事件信息里面出现了 failed to get cpu utilization: missing request for cpu 这样的错误信息。这是因为我们上面创建的 Pod 对象没有添加 request 资源声明，这样导致 HPA 读取不到 CPU 指标信息，所以如果要想让 HPA 生效，对应的 Pod 资源必须添加 requests 资源声明，更新我们的资源清单文件：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpa-demo
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 80
          resources:
            requests:
              memory: 50Mi
              cpu: 50m
```

然后重新更新 Deployment，重新创建 HPA 对象：

```shell
[root@master yamlDir]# kubectl apply -f hpa-demo.yaml
deployment.apps/hpa-demo configured

[root@master yamlDir]# kubectl get pods -o wide -l app=nginx
NAME                      READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES
hpa-demo-f5597c99-cx56j   1/1     Running   0          21s   10.244.1.24   node1   <none>           <none>
[root@master yamlDir]# kubectl delete hpa hpa-demo
horizontalpodautoscaler.autoscaling "hpa-demo" deleted
[root@master yamlDir]# kubectl autoscale deployment hpa-demo --cpu-percent=10 --min=1 --max=10
horizontalpodautoscaler.autoscaling/hpa-demo autoscaled
[root@master yamlDir]# kubectl describe hpa hpa-demo
Warning: autoscaling/v2beta2 HorizontalPodAutoscaler is deprecated in v1.23+, unavailable in v1.26+; use autoscaling/v2 HorizontalPodAutoscaler
Name:                                                  hpa-demo
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Mon, 26 Feb 2024 05:12:38 -0500
Reference:                                             Deployment/hpa-demo
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  0% (0) / 10%
Min replicas:                                          1
Max replicas:                                          10
Deployment pods:                                       1 current / 1 desired
Conditions:
  Type            Status  Reason            Message
  ----            ------  ------            -------
  AbleToScale     True    ReadyForNewScale  recommended size matches current size
  ScalingActive   True    ValidMetricFound  the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  True    TooFewReplicas    the desired replica count is less than the minimum replica count
Events:           <none>
[root@master yamlDir]#
```

现在可以看到 HPA 资源对象已经正常了，现在我们来增大负载进行测试，我们来创建一个 busybox 的 Pod，并且循环访问上面创建的 Pod：

```shell
➜  ~ kubectl run -it --image busybox test-hpa --restart=Never --rm /bin/sh
If you don't see a command prompt, try pressing enter.
/ # while true; do wget -q -O- http://10.244.1.24; done
```

然后观察 Pod 列表，可以看到，HPA 已经开始工作：

```shell

[root@master ~]# kubectl get hpa
NAME       REFERENCE             TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   324%/10%   1         10        1          3m31s

[root@master ~]# kubectl get pods -l app=nginx --watch
NAME                      READY   STATUS              RESTARTS   AGE
hpa-demo-f5597c99-2g2td   0/1     ContainerCreating   0          6s
hpa-demo-f5597c99-5b26r   0/1     ContainerCreating   0          6s
hpa-demo-f5597c99-8tf7c   0/1     ContainerCreating   0          6s
hpa-demo-f5597c99-cx56j   1/1     Running             0          4m44s
hpa-demo-f5597c99-jcb8q   0/1     ContainerCreating   0          6s
hpa-demo-f5597c99-ktqw2   1/1     Running             0          21s
hpa-demo-f5597c99-ntt9g   1/1     Running             0          21s
hpa-demo-f5597c99-qdkpd   0/1     ContainerCreating   0          21s
```

我们可以看到已经自动拉起了很多新的 Pod，最后会定格在了我们上面设置的 10 个 Pod，同时查看资源 hpa-demo 的副本数量，副本数量已经从原来的 1 变成了 10 个：

```shell
[root@master ~]# kubectl get deployment hpa-demo
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
hpa-demo   10/10   10           10          20m
```

查看 HPA 资源的对象了解工作过程：

```shell

[root@master ~]# kubectl describe hpa hpa-demo
Warning: autoscaling/v2beta2 HorizontalPodAutoscaler is deprecated in v1.23+, unavailable in v1.26+; use autoscaling/v2 HorizontalPodAutoscaler
Name:                                                  hpa-demo
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Mon, 26 Feb 2024 05:12:38 -0500
Reference:                                             Deployment/hpa-demo
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  0% (0) / 10%
Min replicas:                                          1
Max replicas:                                          10
Deployment pods:                                       10 current / 10 desired
Conditions:
  Type            Status  Reason               Message
  ----            ------  ------               -------
  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation
  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  True    TooManyReplicas      the desired replica count is more than the maximum replica count
Events:
  Type    Reason             Age    From                       Message
  ----    ------             ----   ----                       -------
  Normal  SuccessfulRescale  4m30s  horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  4m15s  horizontal-pod-autoscaler  New size: 8; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  4m     horizontal-pod-autoscaler  New size: 10; reason: cpu resource utilization (percentage of request) above target
```

同样的这个时候我们来关掉 busybox 来减少负载，然后等待一段时间观察下 HPA 和 Deployment 对象：

```shell
[root@master ~]# kubectl get deployment hpa-demo
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
hpa-demo   1/1     1            1           30m
[root@master ~]# kubectl get hpa
NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   0%/10%    1         10        1          16m
```

从 Kubernetes v1.12 版本开始我们可以通过设置 kube-controller-manager 组件的--horizontal-pod-autoscaler-downscale-stabilization 参数来设置一个持续时间，用于指定在当前操作完成后，HPA 必须等待多长时间才能执行另一次缩放操作。默认为 5 分钟，也就是默认需要等待 5 分钟后才会开始自动缩放。

可以看到副本数量已经由 10 变为 1，当前我们只是演示了 CPU 使用率这一个指标，在后面的课程中我们还会学习到根据自定义的监控指标来自动对 Pod 进行扩缩容。

# 内存







# 扩缩容行为





# VPA




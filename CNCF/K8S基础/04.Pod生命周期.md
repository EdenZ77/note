# Pod生命周期

前面了解了 Pod 的设计原理，接下来了解 Pod 的生命周期。下图展示了一个 Pod 的完整生命周期过程，其中包含 Init Container、Pod Hook、健康检查三个主要部分，接下来就来分别介绍影响 Pod 生命周期的部分。

<img src="image/2024-04-13-06-23-46.png" alt="img" style="zoom:67%;" />

## Pod的基本用法

参考《Kubernetes权威指南 第6版 上》3.2节 Pod的基本用法

```shell
ctr -n k8s.io image ls -q
ctr -n k8s.io image rm docker.io/kubeguide/redis-master:latest
docker pull kubeguide/guestbook-php-frontend:localredis
ctr -n k8s.io --debug=true i pull --hosts-dir=/etc/containerd/certs.d registry.aliyuncs.com/kubeguide/guestbook-php-frontend:localredis

ctr -n k8s.io --debug=true i pull --hosts-dir=/etc/containerd/certs.d docker.io/kubeguide/guestbook-php-frontend:localredis
```



## Pod 的生命周期管理

参考《Kubernetes权威指南 第6版 上》3.7 节



## Pod重启策略

```shell
[root@master ~]# kubectl explain pod.spec.restartPolicy
KIND:     Pod
VERSION:  v1

FIELD:    restartPolicy <string>

DESCRIPTION:
     Restart policy for all containers within the pod. One of Always, OnFailure,
     Never. Default to Always. More info:
     https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy

     Possible enum values:
     - `"Always"`
     - `"Never"`
     - `"OnFailure"`
```

Pod 的 `spec` 中包含一个 `restartPolicy` 字段，其可能取值包括 Always、OnFailure 和 Never。默认值是 Always。

`restartPolicy` 应用于 Pod 中的[应用容器](https://kubernetes.io/zh-cn/docs/reference/glossary/?all=true#term-app-container)和常规的 [Init 容器](https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/init-containers/)。 [Sidecar 容器](https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/sidecar-containers/)忽略 Pod 级别的 `restartPolicy` 字段：在 Kubernetes 中，Sidecar 被定义为 `initContainers` 内的一个条目，其容器级别的 `restartPolicy` 被设置为 `Always`。 对于因错误而退出的 Init 容器，如果 Pod 级别 `restartPolicy` 为 `OnFailure` 或 `Always`， 则 kubelet 会重新启动 Init 容器。

- `Always`：只要容器终止就自动重启容器。
- `OnFailure`：只有在容器错误退出（退出状态非零）时才重新启动容器。
- `Never`：不会自动重启已终止的容器。

当 kubelet 根据配置的重启策略处理容器重启时，仅适用于同一 Pod 内替换容器并在同一节点上运行的重启。当 Pod 中的容器退出时，`kubelet` 会以指数级回退延迟机制（10 秒、20 秒、40 秒......）重启容器， 上限为 300 秒（5 分钟）。一旦容器顺利执行了 10 分钟， kubelet 就会重置该容器的重启延迟计时器。 

# Pod中的容器

## Init容器

了解 Pod 状态后，首先来了解 Pod 中最先启动的 Init Container，也就是常说的初始化容器。Init Container 就是用来做初始化工作的容器，可以是一个或者多个，如果有多个的话，这些容器会按定义的顺序依次执行。

一个 Pod 里面的所有容器是共享数据卷（要主动声明）和 Network Namespace 的，所以 Init Container 里面产生的数据可以被主容器使用到。从上面的 Pod 生命周期的图中可以看出初始化容器是独立与主容器之外的，只有所有的初始化容器执行完之后，主容器才会被启动。那么初始化容器有哪些应用场景呢：

- 等待其他模块 Ready：这个可以用来解决服务之间的依赖问题，比如我们有一个 Web 服务，该服务又依赖于另外一个数据库服务，但是在我们启动这个 Web 服务的时候我们并不能保证依赖的这个数据库服务就已经就绪了，所以可能会出现一段时间内 Web 服务连接数据库异常。要解决这个问题的话就可以在 Web 服务的 Pod 中使用一个 InitContainer，在这个初始化容器中去检查数据库是否已经准备好了，准备好之后初始化容器就结束退出，然后主容器的 Web 服务才开始启动，这个时候去连接数据库就不会有问题了。
- 做初始化配置：比如检测所有已经存在的成员节点，为主容器准备好集群的配置信息，这样主容器起来后就能用这个配置信息加入集群。
- 其它场景：比如将 Pod 注册到一个中央数据库、配置中心等。

比如现在来实现一个功能，在 Nginx Pod 启动之前去重新初始化首页内容，如下所示的资源清单(init-pod.yaml）：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: init-demo
spec:
  volumes:
    - name: workdir
      emptyDir: {}
  initContainers:
    - name: install
      image: busybox
      command:
        - wget
        - "-O"
        - "/work-dir/index.html"
        - http://www.baidu.com # https
      volumeMounts:
        - name: workdir
          mountPath: "/work-dir"
  containers:
    - name: web
      image: nginx
      ports: # ports 字段是一个数组，每个元素表示一个容器中需要暴露的网络端口。不指定端口并不会阻止该端口被暴露。
        - containerPort: 80
      volumeMounts:
        - name: workdir
          mountPath: /usr/share/nginx/html
```

上面的资源清单中首先在 Pod 顶层声明了一个名为 `workdir` 的 Volume，这里使用的是一个 `emptyDir{}` 类型的数据卷，这个是一个临时的目录，数据会保存在 kubelet 的工作目录下面，生命周期等同于 Pod 的生命周期。

然后定义了一个初始化容器，该容器会下载一个网页的 html 代码文件到 `/work-dir` 目录下面，但是由于我们又将该目录声明挂载到了全局的 Volume，同样的主容器 nginx 也将目录 `/usr/share/nginx/html` 声明挂载到了全局的 Volume，所以在主容器的该目录下面会同步初始化容器中创建的 `index.html` 文件。

直接创建上面的 Pod：

```shell
➜  ~ kubectl apply -f init-pod.yaml
```

创建完成后可以查看该 Pod 的状态：

```shell
➜  ~ kubectl get pods
NAME                            READY   STATUS     RESTARTS   AGE
init-demo                       0/1     Init:0/1   0          4s
```

可以发现 Pod 现在的状态处于 `Init:0/1` 状态，意思就是现在第一个初始化容器还在执行过程中，此时我们可以查看 Pod 的详细信息：

```shell
[root@localhost ~]# kubectl describe pod init-demo
Name:             init-demo
Namespace:        default
Priority:         0
Service Account:  default
Node:             demo-worker/172.19.0.4
Start Time:       Tue, 20 Feb 2024 22:19:16 -0500
Labels:           <none>
Annotations:      <none>
Status:           Running
IP:               10.244.2.4
IPs:
  IP:  10.244.2.4
Init Containers: # Init容器详细信息如下：
  install: # 容器名称
    Container ID:  containerd://f63a72b687506c246b82c01e1bc063738e1a99534319d58dcb74e784bf6eb6b4
    Image:         busybox
    Image ID:      docker.io/library/busybox@sha256:6d9ac9237a84afe1516540f40a0fafdc86859b2141954b4d643af7066d598b74
    Port:          <none>
    Host Port:     <none>
    Command:
      wget
      -O
      /work-dir/index.html
      http://www.baidu.com
    State:          Terminated # 容器状态为Terminated
      Reason:       Completed  # 具体原因是Completed
      Exit Code:    0          # 退出码为0,表明是正常退出
      Started:      Tue, 20 Feb 2024 22:19:34 -0500
      Finished:     Tue, 20 Feb 2024 22:19:34 -0500
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-446wm (ro)
      /work-dir from workdir (rw)
Containers:
  web: # 容器名称
    Container ID:   containerd://005bd732d4372e6d6499c79c369edad369f66be50ce3aec150b4b9763fad4065
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:c26ae7472d624ba1fafd296e73cecc4f93f853088e6a9c13c0d52f6ca5865107
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 20 Feb 2024 22:19:41 -0500
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /usr/share/nginx/html from workdir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-446wm (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  workdir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime) # 卷的类型
    Medium:
    SizeLimit:  <unset>
  kube-api-access-446wm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  39s   default-scheduler  Successfully assigned default/init-demo to demo-worker
  Normal  Pulling    37s   kubelet            Pulling image "busybox"
  Normal  Pulled     22s   kubelet            Successfully pulled image "busybox" in 15.723733725s
  Normal  Created    22s   kubelet            Created container install
  Normal  Started    21s   kubelet            Started container install
  Normal  Pulling    20s   kubelet            Pulling image "nginx"
  Normal  Pulled     15s   kubelet            Successfully pulled image "nginx" in 5.228256821s
  Normal  Created    15s   kubelet            Created container web
  Normal  Started    14s   kubelet            Started container web
[root@localhost ~]#
```

从上面的描述信息可以看到初始化容器已经启动了，初始化容器执行完成后退出会变成 `Completed` 状态，然后才会启动主容器。待到主容器也启动完成后，Pod 就会变成 `Running` 状态，然后去访问下 Pod 主页，验证下是否有初始化容器中下载的页面信息：

```shell
[root@master yamlDir]# kubectl get pod -owide
NAME        READY   STATUS    RESTARTS   AGE    IP           NODE    NOMINATED NODE   READINESS GATES
init-demo   1/1     Running   0          133m   10.244.1.5   node1   <none>           <none>
[root@master yamlDir]# curl 10.244.1.5
<!DOCTYPE html>
<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>百度一下，你就知道</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class="bg s_ipt_wr"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class="bg s_btn_wr"><input type=submit id=su value=百度一下 class="bg s_btn"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>新闻</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>地图</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>视频</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>贴吧</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>登录</a> </noscript> <script>document.write('<a href="http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u='+ encodeURIComponent(window.location.href+ (window.location.search === "" ? "?" : "&")+ "bdorz_come=1")+ '" name="tj_login" class="lb">登录</a>');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style="display: block;">更多产品</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>关于百度</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>©2017 Baidu <a href=http://www.baidu.com/duty/>使用百度前必读</a>  <a href=http://jianyi.baidu.com/ class=cp-feedback>意见反馈</a> 京ICP证030173号  <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>
[root@master yamlDir]#
```

可以看到主容器内容已经变成初始化容器中下载的网页内容。

### 理解Init容器

每个 Pod 中可以包含多个容器， 应用运行在这些容器里面，同时 Pod 也可以有一个或多个先于应用容器启动的 Init 容器。

Init 容器与普通的容器非常像，除了如下两点：

- 它们总是运行到完成。（理解这点很重要，它们通常不能定义长时间运行的任务，因为它们的设计目的是在完成其任务后立即退出）
- 每个都必须在下一个启动之前成功完成。

如果 Pod 的 Init 容器失败，kubelet 会不断地重启该 Init 容器直到该容器成功为止。 然而，如果 Pod 对应的 `restartPolicy` 值为 "Never"，并且 Pod 的 Init 容器失败， 则 Kubernetes 会将整个 Pod 状态设置为失败。

**与普通容器的不同之处**

Init 容器支持应用容器的全部字段和特性。 然而，Init 容器对资源请求和限制的处理稍有不同， 在下面[容器内的资源共享](https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/init-containers/#resource-sharing-within-containers)节有说明。

常规的 Init 容器（即不包括边车容器）不支持 `livenessProbe`、`readinessProbe` 或 `startupProbe` 字段。Init 容器必须在 Pod 准备就绪之前完成运行；而边车容器在 Pod 的生命周期内继续运行， 它支持这些探针。

如果为一个 Pod 指定了多个 Init 容器，这些容器会按顺序逐个运行。 每个 Init 容器必须运行成功，下一个才能够运行。当所有的 Init 容器运行完成时， Kubernetes 才会为 Pod 初始化应用容器并像平常一样运行。



### 使用 Init 容器

下面的例子定义了一个具有 2 个 Init 容器的简单 Pod。 第一个等待 `myservice` 启动， 第二个等待 `mydb` 启动。 一旦这两个 Init 容器都启动完成，Pod 将启动 `spec` 节中的应用容器。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app.kubernetes.io/name: MyApp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    # until ...; do ...; done   这是一个 Shell 循环结构。until 循环会一直执行其内部命令直到条件成功（即命令返回状态为 0 时）。
    command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]
```

你通过运行下面的命令启动 Pod：

```shell
kubectl apply -f myapp.yaml

# 输出类似于：
pod/myapp-pod created
```

使用下面的命令检查其状态：

```shell
kubectl get -f myapp.yaml

# 输出类似于：
NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m
```

或者查看更多详细信息：

```shell
kubectl describe -f myapp.yaml
```

输出类似于：

```shell
Name:          myapp-pod
Namespace:     default
[...]
Labels:        app.kubernetes.io/name=MyApp
Status:        Pending # 此时pod的状态为Pending
[...]
Init Containers:
  init-myservice: # 第一个init容器
[...]
    State:         Running  # 此时第一个init容器状态为 Running
[...]
  init-mydb:      # 第二个init容器
[...]
    State:         Waiting # 由于第一个init容器为Running，所以第二个init容器状态为 Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Containers:
  myapp-container:
[...]
    State:         Waiting # 由于init容器并未完成，应用容器状态就为 Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Events:
  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
  ---------    --------    -----    ----                      -------------                           --------      ------        -------
  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image "busybox"
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image "busybox"
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container init-myservice
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container init-myservice
```

如需查看 Pod 内 Init 容器的日志，请执行：

```shell
kubectl logs myapp-pod -c init-myservice # 查看第一个 Init 容器
kubectl logs myapp-pod -c init-mydb      # 查看第二个 Init 容器
```

在这一刻，Init 容器将会等待至发现名称为 `mydb` 和 `myservice` 的[服务](https://kubernetes.io/zh-cn/docs/concepts/services-networking/service/)。

如下为创建这些 Service 的配置文件：

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
---
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9377
```

创建 `mydb` 和 `myservice` 服务的命令：

```shell
kubectl apply -f services.yaml

# 输出类似于：
service/myservice created
service/mydb created
```

这样你将能看到这些 Init 容器执行完毕，随后 `my-app` 的 Pod 进入 `Running` 状态：

```shell
kubectl get -f myapp.yaml

# 输出类似于：
NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
```

这个简单例子应该能为你创建自己的 Init 容器提供一些启发。



## Sidecar容器

Kubernetes 将边车容器作为 Init 容器的一个特例来实现， Pod 启动后，边车容器仍保持运行状态。 本文档使用术语"常规 Init 容器"来明确指代仅在 Pod 启动期间运行的容器。

Kubernetes 1.28 在 Init 容器中添加了一个新的 `restartPolicy` 字段， 该字段在 `SidecarContainers` 特性门控启用时可用（该特性自 Kubernetes v1.29 起默认启用）。

```yaml
apiVersion: v1
kind: Pod
spec:
  initContainers:
  - name: secret-fetch
    image: secret-fetch:1.0
  - name: network-proxy
    image: network-proxy:1.0
    restartPolicy: Always
  containers:
  ...
```

该字段是可选的，如果对其设置，则唯一有效的值为 Always。设置此字段会更改 Init 容器的行为，如下所示：

- 如果容器退出则会重新启动
- 任何后续的 Init 容器在 startupProbe 成功完成后立即启动，而不是等待可重新启动 Init 容器退出
- 由于可重新启动的 Init 容器资源现在添加到主容器的资源请求总和中，所以 Pod 使用的资源计算发生了变化。

可重新启动的 Init 容器的以下属性使其非常适合边车部署模式：

- 无论你是否设置 `restartPolicy`，初始化容器都有一个明确定义的启动顺序， 因此你可以确保你的边车在其所在清单中声明的后续任何容器之前启动。
- 边车容器不会延长 Pod 的生命周期，因此你可以在短生命周期的 Pod 中使用它们，而不会对 Pod 生命周期产生改变。
- 边车容器在退出时将被重新启动，这提高了弹性，并允许你使用边车来为主容器提供更可靠地服务。



**与应用容器的区别**

边车容器与同一 Pod 中的应用容器并行运行。不过边车容器不执行主应用逻辑，而是为主应用提供支持功能。

边车容器具有独立的生命周期。它们可以独立于应用容器启动、停止和重启。 这意味着你可以更新、扩展或维护边车容器，而不影响主应用。

边车容器与主容器共享相同的网络和存储命名空间。这种共存使它们能够紧密交互并共享资源。

### 与sidecar模式对比

1. Sidecar 模式：这是一个设计模式，一种架构思想。它描述了一种“一个主容器 + 一个或多个辅助容器”的设计方式，这些容器共享网络和存储，协同工作。在很长一段时间里，Kubernetes 并没有为这种模式提供原生支持。大家是通过在 Pod 中定义多个常规容器来实现的。这些辅助容器和主容器一样，在 Pod 的整个生命周期内运行。这种实现方式简单直观，是目前最普遍的做法。
2. 边车容器：这是 Kubernetes 官方提供的一种新的、特定的实现方式。它从技术上将“边车”归类为一种特殊的 Init 容器。这种 Init 容器不像“常规 Init 容器”那样在启动后退出，而是通过设置 `restartPolicy: Always`来保持持续运行。这是一种“语法糖”或更精确的原生支持，让 Sidecar 模式的定义变得更加清晰和可控。

为了更直观，制作了下面这个对比表格：

| 特性     | 传统 Sidecar 容器 (社区模式)                     | 官方边车容器 (Kubernetes v1.28+)                         |
| :------- | :----------------------------------------------- | :------------------------------------------------------- |
| 定义位置 | Pod 规约中的 `spec.containers`数组里             | Pod 规约中的 `spec.initContainers`数组里                 |
| 容器类型 | 常规容器                                         | 一种特殊的 Init 容器                                     |
| 重启策略 | 遵循 Pod 的 `restartPolicy`                      | 拥有独立的 **`restartPolicy`** 字段（需设置为 `Always`） |
| 生命周期 | 与主容器完全平行的生命周期，同时启动、同时终止。 | 启动顺序仍在主容器之前，但启动后不会退出，会持续运行。   |
| 目的     | 扩展主容器功能（日志、监控、代理等）             | 为主容器提供依赖服务，但该依赖服务需要持续运行。         |
| 优势     | 概念简单，应用广泛，易于理解。                   | 生命周期管理更精细。                                     |





## 容器生命周期

Pod 是 Kubernetes 集群中的最小单元，而 Pod 是由容器组成的，所以在讨论 Pod 的生命周期的时候我们可以先来讨论下容器的生命周期。实际上 Kubernetes 为容器提供了生命周期的钩子，就是我们说的 Pod Hook，Pod Hook 是由 kubelet 发起的，当容器中的进程启动之前或者容器中的进程终止之前运行，可以同时为 Pod 中的所有容器都配置 hook。

Kubernetes 为我们提供了两种钩子函数：

* PostStart：Kubernetes 在容器创建后立即发送 postStart 事件，然而 postStart 处理函数的调用不保证早于容器的入口点（entrypoint） 的执行。Kubernetes 的容器管理逻辑会一直阻塞等待 postStart 处理函数执行完毕，只有 postStart 处理函数执行完毕，Pod的状态才会变成 RUNNING。
* PreStop：这个钩子在容器终止之前立即被调用。它是阻塞的，所以它必须在删除容器的调用发出之前完成。主要用于优雅关闭应用程序、通知其他系统等。在容器因 API 请求或者管理事件（诸如存活态探针、启动探针失败、资源抢占、资源竞争等） 而被终止之前，此回调会被调用。 如果容器已经处于已终止或者已完成状态，则对 preStop 回调的调用将失败。 在用来停止容器的 TERM 信号被发出之前，回调必须执行结束。 Pod 的终止宽限周期在 `PreStop` 回调被执行之前即开始计数， 所以无论回调函数的执行结果如何，容器最终都会在 Pod 的终止宽限期内被终止。 

我们应该让钩子函数尽可能的轻量，当然有些情况下，长时间运行命令是合理的，比如在停止容器之前预先保存状态。

另外我们有两种方式来实现上面的钩子函数：

* Exec - 用于执行一段特定的命令，不过要注意的是该命令消耗的资源会被计入容器。
* HTTP - 对容器上的特定的端点执行 HTTP 请求。

以下示例中，定义了一个 Nginx Pod，其中设置了 PostStart 钩子函数，即在容器创建成功后，写入一句话到 `/usr/share/message` 文件中：

```yaml
# pod-poststart.yaml
apiVersion: v1
kind: Pod
metadata:
  name: hook-demo1
spec:
  containers:
  - name: hook-demo1
    image: nginx
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
```

直接创建上面的 Pod：

```shell
[root@master yamlDir]# kubectl apply -f pod-poststart.yaml
pod/hook-demo1 created
[root@master yamlDir]# kubectl get pods
NAME         READY   STATUS    RESTARTS   AGE
hook-demo1   1/1     Running   0          35s
[root@master yamlDir]#
```

创建成功后可以查看容器中 `/usr/share/message` 文件是否内容正确：

```shell
[root@master yamlDir]# kubectl exec hook-demo1 -- cat /usr/share/message
Hello from the postStart handler
```

以下示例中，定义了一个 Nginx Pod，其中设置了 PreStop 钩子函数，即在容器退出之前，优雅的关闭 Nginx：

```yaml
# pod-prestop.yaml
apiVersion: v1
kind: Pod
metadata:
  name: hook-demo2
spec:
  containers:
    - name: hook-demo2
      image: nginx
      lifecycle:
        preStop:
          exec:
            command: ["/usr/sbin/nginx", "-s", "quit"] # 优雅退出
---
apiVersion: v1
kind: Pod
metadata:
  name: hook-demo3
spec:
  volumes:
    - name: message
      hostPath:
        path: /tmp
  containers:
    - name: hook-demo2
      image: nginx
      ports:
        - containerPort: 80
      volumeMounts:
        - name: message
          mountPath: /usr/share/
      lifecycle:
        preStop:
          exec:
            command:
              [
                "/bin/sh",
                "-c",
                "echo Hello from the preStop Handler > /usr/share/message",
              ]
```

上面定义的两个 Pod，一个是利用 preStop 来进行优雅删除，另外一个是利用 preStop 来做一些信息记录的事情，同样直接创建上面的 Pod：

```shell
[root@master yamlDir]# kubectl apply -f pod-prestop.yaml
pod/hook-demo2 created
pod/hook-demo3 created
[root@master yamlDir]# kubectl get pods -owide
NAME         READY   STATUS    RESTARTS   AGE     IP           NODE    NOMINATED NODE   READINESS GATES
hook-demo2   1/1     Running   0          3m23s   10.244.2.4   node2   <none>           <none>
hook-demo3   1/1     Running   0          3m23s   10.244.2.5   node2   <none>           <none>
```

创建完成后，可以直接删除 hook-demo2 这个 Pod，在容器删除之前会执行 preStop 里面的优雅关闭命令，这个用法在后面滚动更新的时候用来保证应用零宕机非常有用。第二个 Pod 我们声明了一个 `hostPath` 类型的 Volume，在容器里面声明挂载到了这个 Volume，所以当删除 Pod，退出容器之前，在容器里面输出的信息也会同样的保存到宿主机（Pod 被调度到的目标节点）的 `/tmp` 目录下面。

现在来删除 hook-demo3 这个 Pod，按照设定在容器退出之前会执行 preStop 里面的命令，也就是会往 message 文件中输出一些信息：

```shell
➜  ~ kubectl delete pod hook-demo3
pod "hook-demo3" deleted
➜  ~ ls /tmp/
message
➜  ~ cat /tmp/message
Hello from the preStop Handler
```

另外 Hook 调用的日志没有暴露给 Pod，如果处理程序由于某种原因失败，它将产生一个事件。对于 PostStart，这是 FailedPostStartHook 事件，对于 PreStop，是 FailedPreStopHook 事件，比如修改下面的 `lifecycle-events.yaml` 文件，将 postStart 命令更改为 badcommand 并应用它。

```yaml
# lifecycle-events.yaml
apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-demo
spec:
  containers:
    - name: lifecycle-demo-container
      image: nginx
      lifecycle:
        postStart:
          exec:
            command:
              [
                "badcommand",
                "-c",
                "echo Hello from the postStart handler > /usr/share/message",
              ]
        preStop:
          exec:
            command:
              [
                "/bin/sh",
                "-c",
                "nginx -s quit; while killall -0 nginx; do sleep 1; done",
              ]
```

应用后可以通过运行 `kubectl describe pod lifecycle-demo` 后来查看一些结果事件的示例输出：

```shell
Events:
  Type     Reason               Age                From               Message
  ----     ------               ----               ----               -------
  Normal   Scheduled            11s                default-scheduler  Successfully assigned default/lifecycle-demo to node1
  Normal   Pulled               11s                kubelet            Successfully pulled image "nginx" in 365.440243ms
  Normal   Created              11s                kubelet            Created container lifecycle-demo-container
  Normal   Started              11s                kubelet            Started container lifecycle-demo-container
  # 可以看到下面的 FailedPostStartHook 事件
  Warning  FailedPostStartHook  11s                kubelet            Exec lifecycle hook ([badcommand -c echo Hello from the postStart handler > /usr/share/message]) for Container "lifecycle-demo-container" in Pod "lifecycle-demo_default(e3b248fd-dd8e-4756-8ab1-deadcd066f02)" failed - error: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "6ded53e6bc13fd97f7f5dd6e08199b241b124d9aff9768c1ef6c7ceee4d7b414": OCI runtime exec failed: exec failed: unable to start container process: exec: "badcommand": executable file not found in $PATH: unknown, message: ""
  Normal   Killing              11s                kubelet            FailedPostStartHook
  Normal   Pulling              10s (x2 over 11s)  kubelet            Pulling image "nginx"
[root@master yamlDir]#
```

# Pod重建

简单来说：只有当 Pod 被彻底删除并重新创建（这个过程称为“重建”）时，Pod 的 IP 才会改变。

Pod 的 IP 地址是在其被调度到某个节点上之后，由该节点上的 CNI (容器网络接口) 插件分配的。它是 Pod 生命周期的一部分。因此，任何不涉及 Pod 重建的操作，都不会改变其 IP。

下面我们详细分析哪些情况会导致 Pod 重建（IP 改变），哪些情况不会（IP 不变）。

**会导致 Pod 重建（IP 改变）的情况**

这些情况的核心是 “旧 Pod 被删除，一个新 Pod 被创建来替代它”。

1. 手动删除 Pod命令：`kubectl delete pod <pod-name>`这是最直接的方式。如果你手动删除一个 Pod，并且它是由一个控制器（如 Deployment、StatefulSet）管理的，那么控制器会立刻检测到“期望状态”和“实际状态”不符，从而创建一个全新的 Pod 来替代被删除的。新 Pod 会获得全新的 IP。
2. 更新 Deployment/DaemonSet 等的 Pod 模板（PodTemplate）操作：修改 Deployment 的 YAML 文件，例如更新容器镜像标签、修改环境变量等，然后应用更改 (`kubectl apply ...`)。**解释**：Kubernetes 的控制器会根据更新策略来用新配置的 Pod 替换掉所有旧配置的 Pod。每一个被替换掉的旧 Pod 都会被删除，然后创建一个新 Pod，新 Pod 拥有新的 IP。
3. 节点故障或节点被驱逐（Node Eviction）场景：Pod 所在的节点宕机、网络断开、或资源不足。**解释**：控制平面会将该节点标记为 `NotReady`或 `Unreachable`。之后，Deployment 等控制器会认为运行在该节点上的 Pod 已经失败，并在另一个健康节点上调度并创建一个全新的 Pod。新 Pod 拥有新 IP。
4. Init 容器运行失败（且 Pod 配置了重启策略）解释：常规 Init 容器的失败被视为 Pod 启动失败。如果 Pod 的 `restartPolicy`是 `Always`或 `OnFailure`，Kubernetes 会尝试重启整个 Pod。这个“重启”在底层机制上，实际上是删除旧的失败 Pod 并创建一个新的 Pod 来重试，因此 IP 会改变。
5. 主动或被动驱逐（Eviction）场景：节点资源（如内存、磁盘）不足时，kubelet 会主动选择并驱逐某些 Pod 以释放资源。**解释**：驱逐过程是优雅地删除 Pod。被驱逐的 Pod 会被删除，如果它有控制器管理，控制器会在别处（如果资源允许）重建一个新的 Pod。

**不会导致 Pod 重建（IP 不变）的情况**

这些情况的核心是 “Kubelet 在原有 Pod 内部重启容器”。Pod 本身这个“壳”没有动，所以 IP 地址得以保留。

1. 常规应用容器失败退出（并由重启策略重启）场景：你的应用程序（比如一个 Web 服务器）崩溃了。**解释**：Kubelet 会根据 Pod 的 `restartPolicy`，在同一个 Pod 内部直接重启这个失败的容器。Pod 本身没有被删除和重建，因此所有属性（包括 IP、名称、命名空间）都保持不变。
2. 边车容器（Sidecar）失败退出解释：对于设置了 `restartPolicy: Always`的边车容器，它的失败和重启行为与常规应用容器完全一致。Kubelet 只会重启这个边车容器，而不会触动整个 Pod，所以 Pod IP 不变。
3. Pod 的 `livenessProbe`或 `startupProbe`失败解释：探针失败意味着容器被认为不健康。Kubelet 的应对策略是 `kill`掉这个容器并根据 `restartPolicy`在 Pod 内部重启它，而不是重建整个 Pod。

# Pod 健康检查

在 Kubernetes 集群当中，可以通过配置 `liveness probe`（存活探针）、`readiness probe`（就绪探针） 以及 `startup probe`（启动探针） 来影响容器的生命周期：

* kubelet 使用 liveness probe 来确定容器是否正在运行，即是否还活着。如果存活态探测失败，则 kubelet 会杀死容器， 并且容器将根据其重启策略决定未来。如果容器不提供存活探针， 则默认状态为 `Success`。
* kubelet 使用 readiness probe 来确定容器是否已经就绪可以接收流量过来了。只有当 Pod 中的容器都处于就绪状态的时候 kubelet 才会认定该 Pod 处于就绪状态，因为一个 Pod 下面可能会有多个容器。如果就绪态探测失败，端点控制器将从与 Pod 匹配的所有服务的端点列表中删除该 Pod 的 IP 地址，这样流量就不会被路由到这个 Pod 。如果容器不提供就绪态探针，则默认状态为 `Success`。
* kubelet 使用 startup probe 来指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被禁用，直到该探针成功为止。如果启动探测失败，kubelet 将杀死容器，而容器依其重启策略进行重启。如果容器没有提供启动探测，则默认状态为 Success。如果探测成功，Kubernetes 认为容器启动完成并健康，随后会开始执行存活探针和就绪探针（如果配置了），一旦启动探针（startup probe）成功，Kubernetes 就不再继续执行启动探针的周期性探测。

**何时使用存活探针**

如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃，则不一定需要存活态探针；kubelet 将根据 Pod 的 restartPolicy 自动执行修复操作。如果你希望容器在探测失败时被杀死并重新启动，那么请指定一个存活态探针， 并指定 restartPolicy 为 Always 或 OnFailure。

**何时使用就绪探针**

如果要仅在探测成功时才开始向 Pod 发送请求流量，请指定就绪态探针。 在这种情况下，就绪态探针可能与存活态探针相同，但是规约中的就绪态探针的存在意味着 Pod 将在启动阶段不接收任何数据，并且只有在探针探测成功后才开始接收数据。

如果你希望容器能够自行进入维护状态，也可以指定一个就绪态探针， 检查某个特定于就绪态的因此不同于存活态探测的端点。

如果你的应用程序对后端服务有严格的依赖性，你可以同时实现存活态和就绪态探针。 当应用程序本身是健康的，存活态探针检测通过后，就绪态探针会额外检查每个所需的后端服务是否可用。 这可以帮助你避免将流量导向只能返回错误信息的 Pod。

如果你的容器需要在启动期间加载大型数据、配置文件或执行迁移， 你可以使用启动探针。 

**何时使用启动探针**

如果容器需要在启动期间加载大型数据、配置文件等操作，那么这时我们可以使用启动探针。该探针在 Kubernetes v1.20 版本才变成稳定状态，对于所包含的容器需要较长时间才能启动就绪的 Pod 而言，启动探针是有用的。你不再需要配置一个较长的存活态探测时间间隔，只需要设置另一个独立的配置选项，对启动期间的容器执行探测，从而允许使用远远超出存活态时间间隔所允许的时长。

> 建议先看“探针参数”这小结，了解参数基本含义，才有利于理解后面的说明。

如果不使用启动探针而只依赖存活探针（Liveness Probe），可能会导致一些问题，尤其是在容器需要较长时间启动的情况下。这些问题包括：

1. 频繁重启：如果存活探针的initialDelaySeconds设置得过短，存活探针可能会在容器还未完成启动和初始化过程时就开始检查容器的健康状态。如果此时容器未能通过健康检查，Kubernetes会认为容器不健康，导致Pod被重启。这种过早的重启会打断容器的启动过程，可能导致容器永远无法成功启动。
2. 延长启动时间：为了避免频繁重启，您可能需要将存活探针的initialDelaySeconds设置得非常长，以确保容器有足够的时间完成启动。然而，这意味着在容器实际启动完成后，Kubernetes需要更长时间才能开始对其进行健康检查，这会延迟发现容器在启动后出现的任何问题。

因此，启动探针（Startup Probe）的出现帮助解决了这些问题。它允许容器在启动期间有一个较长的时间窗口来完成复杂的初始化，而不会受到存活探针的干扰。一旦容器启动并通过了启动探针的检查，存活探针和就绪探针将接管后续的健康检查工作，它们通常有更严格的检查频率和失败阈值，以确保容器在运行过程中维持健康状态。这样，即使您需要容器在启动时加载大量数据或进行复杂配置，也可以有效地避免不必要的重启，并且在容器运行时可以及时响应任何潜在问题。

如果你的容器启动时间通常超出存活态探针的 `initialDelaySeconds + failureThreshold × periodSeconds` 总值，你应该设置一个启动探针，对存活态探针所使用的同一端点执行检查。 periodSeconds 的默认值是 10 秒，还应该将其 failureThreshold 设置得足够高，以便容器有充足的时间完成启动，并且避免更改存活态探针所使用的默认值。 

## 探针参数

### initialDelaySeconds

initialDelaySeconds 参数在启动探针（Startup Probe）、存活探针（Liveness Probe）和就绪探针（Readiness Probe）中都存在，并且它们各自的含义是类似的。

对于所有这三种类型的探针，initialDelaySeconds 指定了容器启动后，探针开始执行检查之前的等待时间。这个参数的主要目的是给容器足够的时间来完成启动过程中的初始化工作，例如加载配置文件、连接数据库、预热缓存等，以避免在容器尚未准备好时就开始进行健康检查。

下面分别解释每种探针的 initialDelaySeconds 参数含义：

* 启动探针（Startup Probe）: initialDelaySeconds 设置了容器启动后到启动探针开始执行第一次检查之间的延迟时间。如果定义了启动探针，则存活探针和就绪探针的延迟将在启动探针已成功之后才开始计算。 默认是 0 秒，最小值是 0。
* 存活探针（Liveness Probe）: initialDelaySeconds 在这里设置的是容器启动后到存活探针开始工作之间的延迟时间（如果你的容器有一个启动探针，存活探针的 initialDelaySeconds 计时将从启动探针成功后的那个时刻开始）。
* 就绪探针（Readiness Probe）: initialDelaySeconds 定义了从容器启动后到就绪探针开始检查容器是否准备好接受流量之间的延迟时间。

在设置这些参数时，你需要根据容器启动和初始化的时间来调整 initialDelaySeconds 的值，以确保探针检查既不会过早导致失败，也不会过晚才开始执行，影响服务的响应速度。

### periodSeconds

periodSeconds 参数指定了探针执行检查的频率，即探针之间的时间间隔。这个参数同样适用于启动探针（Startup Probe）、存活探针（Liveness Probe）和就绪探针（Readiness Probe）。

### 实践两种参数

以下是两个示例 YAML 文件，分别展示了 `initialDelaySeconds` 和 `periodSeconds` 的不同设置，以及如何在 Kubernetes 中观察它们的行为。

**示例 1: `initialDelaySeconds` 小于 `periodSeconds`**

在这个例子中，`initialDelaySeconds` 小于 `periodSeconds`。首次探针检查会在 `initialDelaySeconds` 之后开始，然后每隔 `periodSeconds` 进行检查。

假设 `initialDelaySeconds` = 5 秒，`periodSeconds` = 10 秒：

- 容器启动。
- 等待 5 秒（`initialDelaySeconds`），然后探针第一次执行。
- 接下来每隔 10 秒（`periodSeconds`）再次执行探针。
- 执行的时间点：5，15，25，35，45......

发现上面的描述是错误的，下面是官方文档的说明：

> https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes

`initialDelaySeconds`：容器启动后要等待多少秒后才启动启动、存活和就绪探针。 如果定义了启动探针，则存活探针和就绪探针的延迟将在启动探针已成功之后才开始计算。 如果 `periodSeconds` 的值大于 `initialDelaySeconds`，则 `initialDelaySeconds` 将被忽略。默认是 0 秒，最小值是 0。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: probe-test-working
spec:
  terminationGracePeriodSeconds: 0
  containers:
  - name: test
    image: busybox:musl
    command: ["/bin/sh", "-c"]
    args:
      - |
        # 1. 确保目录存在
        mkdir -p /data

        # 2. 记录绝对启动时间戳
        echo $(date +%s) > /data/start_time
        echo "Container STARTED at: $(date)" > /data/probe.log

        # 3. 持续运行（不使用 tail -f）
        while true; do sleep 10000; done
    livenessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - |
            # 4. 读取启动时间
            start_time=$(cat /data/start_time)
            current_time=$(date +%s)
            elapsed=$((current_time - start_time))

            # 5. 写入日志（使用 echo -n 避免换行问题）
            echo -n "Probe EXECUTED at: $(date) | " >> /data/probe.log
            echo "Elapsed: ${elapsed}s" >> /data/probe.log
      initialDelaySeconds: 50
      periodSeconds: 100
      failureThreshold: 5
```

**示例 2: `initialDelaySeconds` 大于 `periodSeconds`**

在这个例子中，`initialDelaySeconds` 大于 `periodSeconds`。首次探针检查仍然会在 `initialDelaySeconds` 之后进行，然后每隔 `periodSeconds` 进行检查。

假设 `initialDelaySeconds` = 10 秒，`periodSeconds` = 5 秒：

- 容器启动。
- 等待 10 秒（`initialDelaySeconds`），然后探针第一次执行。
- 接下来每隔 5 秒（`periodSeconds`）再次执行探针。



### failureThreshold

failureThreshold 参数也适用于启动探针（Startup Probe）、存活探针（Liveness Probe）和就绪探针（Readiness Probe）。这个参数定义了在Kubernetes认定探针失败之前，探针允许连续失败的最大次数。

### timeoutSeconds

timeoutSeconds 参数在 Kubernetes 中用于设置探针等待响应的超时时间。这个参数对于启动探针（Startup Probe）、存活探针（Liveness Probe）和就绪探针（Readiness Probe）都有相同的含义和作用。当 kubelet 执行任何一种类型的探针时，它会等待指定的 timeoutSeconds 时间长度以接收探针的响应。

如果在这个时间段内，探针没有得到容器的响应，探针就会被认为是失败的。这个参数帮助 Kubernetes 管理探针的超时行为，确保在合理的时间内得到探针的结果，避免因为等待响应而导致的长时间挂起。

### successThreshold

successThreshold 参数在 Kubernetes 探针配置中定义了探针必须连续成功的最小次数，才能被认为是成功的。

## 探针实践

### 存活探针-EXEC

许多长时间运行的应用最终会进入损坏状态，除非重新启动，否则无法被恢复。 Kubernetes 提供了存活探针来发现并处理这种情况。

在本练习中会创建一个 Pod，其中运行一个基于 `registry.k8s.io/busybox` 镜像的容器。 下面是这个 Pod 的配置文件。（[`pods/probe/exec-liveness.yaml`](https://raw.githubusercontent.com/kubernetes/website/main/content/zh-cn/examples/pods/probe/exec-liveness.yaml)）

```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: registry.k8s.io/busybox
    args: # 使用 args 替换掉镜像默认的启动命令 CMD [/bin/sh]
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
```

在这个配置文件中，可以看到 Pod 中只有一个 `Container`。 `periodSeconds` 字段指定了 kubelet 应该每 5 秒执行一次存活探测。 `initialDelaySeconds` 字段告诉 kubelet 在执行第一次探测前应该等待 5 秒。 kubelet 在容器内执行命令 `cat /tmp/healthy` 来进行探测。 如果命令执行成功并且返回值为 0，kubelet 就会认为这个容器是健康存活的。 如果这个命令返回非 0 值，kubelet 会杀死这个容器并重新启动它。

当容器启动时，执行如下的命令：

```shell
/bin/sh -c "touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600"
```

这个容器生命的前 30 秒，`/tmp/healthy` 文件是存在的。 所以在这最开始的 30 秒内，执行命令 `cat /tmp/healthy` 会返回成功代码。 30 秒之后，执行命令 `cat /tmp/healthy` 就会返回失败代码。

创建 Pod：

```shell
kubectl apply -f https://k8s.io/examples/pods/probe/exec-liveness.yaml
```

在 30 秒内，查看 Pod 的事件：

```shell
kubectl describe pod liveness-exec

# 下面是部分输出
Containers:
  liveness:
    Container ID:  containerd://ec2b163a31db8761f6bb0d1ffd14f82b9355d158ebfacc5191c157ff83b0e4c6
    Image:         registry.k8s.io/busybox
    Image ID:      sha256:e7d168d7db455c45f4d0315d89dbd18806df4784f803c3cc99f8a2e250585b5b
    Port:          <none>
    Host Port:     <none>
    Args: # 启动命令
      /bin/sh
      -c
      touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600
    State:          Running
      Started:      Wed, 16 Oct 2024 05:35:51 -0400
    Ready:          True
    Restart Count:  0
    # 这里存活探针，说明了默认的successThreshold=1次，failureThreshold=3次
    Liveness:       exec [cat /tmp/healthy] delay=5s timeout=1s period=5s #success=1 #failure=3
    Environment:    <none>
```

输出结果表明还没有存活探针失败：

```shell
Type    Reason     Age   From               Message
----    ------     ----  ----               -------
Normal  Scheduled  11s   default-scheduler  Successfully assigned default/liveness-exec to node01
Normal  Pulling    9s    kubelet, node01    Pulling image "registry.k8s.io/busybox"
Normal  Pulled     7s    kubelet, node01    Successfully pulled image "registry.k8s.io/busybox"
Normal  Created    7s    kubelet, node01    Created container liveness
Normal  Started    7s    kubelet, node01    Started container liveness
```

35 秒之后，再来看 Pod 的事件，在输出结果的最下面，有信息显示存活探针失败了，这个失败的容器被杀死并且被重建了。

```shell
Type     Reason     Age                From               Message
----     ------     ----               ----               -------
Normal   Scheduled  57s                default-scheduler  Successfully assigned default/liveness-exec to node01
Normal   Pulling    55s                kubelet, node01    Pulling image "registry.k8s.io/busybox"
Normal   Pulled     53s                kubelet, node01    Successfully pulled image "registry.k8s.io/busybox"
Normal   Created    53s                kubelet, node01    Created container liveness
Normal   Started    53s                kubelet, node01    Started container liveness
# 可以看到失败3次了
Warning  Unhealthy  10s (x3 over 20s)  kubelet, node01    Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory
# 达到失败阈值，准备重启
Normal   Killing    10s                kubelet, node01    Container liveness failed liveness probe, will be restarted
```

再等 30 秒，确认这个容器被重启了：

```shell
kubectl get pod liveness-exec
```

输出结果显示 `RESTARTS` 的值增加了 1。 请注意，一旦失败的容器恢复为运行状态，`RESTARTS` 计数器就会增加 1：

```none
NAME            READY     STATUS    RESTARTS   AGE
liveness-exec   1/1       Running   1          1m
```

重启后，观察一下哪些值变了：

```shell
[root@master probe]# kubectl describe pod liveness-exec
Name:             liveness-exec
Namespace:        default
Priority:         0
Service Account:  default
Node:             node1/192.168.220.147 # 被调度的node未变
Start Time:       Wed, 16 Oct 2024 05:35:50 -0400
Labels:           test=liveness
Annotations:      <none>
Status:           Running
IP:               10.244.1.218 # ip未变
IPs:
  IP:  10.244.1.218
Containers:
  liveness:
    # 由于容器重启了，所以容器ID改变了
    Container ID:  containerd://18e7445466b3a285af77fa718f6a4b83d35e17d0632f1a7035a8836b2da65d02
    Image:         registry.k8s.io/busybox
    Image ID:      sha256:e7d168d7db455c45f4d0315d89dbd18806df4784f803c3cc99f8a2e250585b5b
```



### 存活探针-HTTP

另外一种类型的存活探测方式是使用 HTTP GET 请求。 下面是一个 Pod 的配置文件，其中运行一个基于 `registry.k8s.io/e2e-test-images/agnhost` 镜像的容器。([`pods/probe/http-liveness.yaml`](https://raw.githubusercontent.com/kubernetes/website/main/content/zh-cn/examples/pods/probe/http-liveness.yaml))

```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness
    image: registry.k8s.io/e2e-test-images/agnhost:2.40
    imagePullPolicy: IfNotPresent
    args:
    - liveness
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
        - name: Custom-Header
          value: Awesome
      initialDelaySeconds: 3
      periodSeconds: 3
```

在这个配置文件中，可以看到 Pod 只有一个容器。 `periodSeconds` 字段指定了 kubelet 每隔 3 秒执行一次存活探测。 `initialDelaySeconds` 字段告诉 kubelet 在执行第一次探测前应该等待 3 秒。 kubelet 会向容器内运行的服务（服务在监听 8080 端口）发送一个 HTTP GET 请求来执行探测。 如果服务器上 `/healthz` 路径下的处理程序返回成功代码，则 kubelet 认为容器是健康存活的。 如果处理程序返回失败代码，则 kubelet 会杀死这个容器并将其重启。

返回大于或等于 200 并且小于 400 的任何代码都标示成功，其它返回代码都标示失败。

你可以访问 [server.go](https://github.com/kubernetes/kubernetes/blob/master/test/images/agnhost/liveness/server.go) 阅读服务的源码。 容器存活期间的最开始 10 秒中，`/healthz` 处理程序返回 200 的状态码。 之后处理程序返回 500 的状态码。

```go
http.HandleFunc("/healthz", func(w http.ResponseWriter, r *http.Request) {
    duration := time.Now().Sub(started)
    if duration.Seconds() > 10 {
        w.WriteHeader(500)
        w.Write([]byte(fmt.Sprintf("error: %v", duration.Seconds())))
    } else {
        w.WriteHeader(200)
        w.Write([]byte("ok"))
    }
})
```

kubelet 在容器启动之后 3 秒开始执行健康检查。所以前几次健康检查都是成功的。 但是 10 秒之后，健康检查会失败，并且 kubelet 会杀死容器再重新启动容器。

创建一个 Pod 来测试 HTTP 的存活检测：

```shell
kubectl apply -f https://k8s.io/examples/pods/probe/http-liveness.yaml
```

10 秒之后，通过查看 Pod 事件来确认存活探针已经失败，并且容器被重新启动了。

```shell
kubectl describe pod liveness-http
```



### 存活探针-TCP

第三种类型的存活探测是使用 TCP 套接字。 使用这种配置时，kubelet 会尝试在指定端口和容器建立套接字链接。 如果能建立连接，这个容器就被看作是健康的，如果不能则这个容器就被看作是有问题的。([`pods/probe/tcp-liveness-readiness.yaml`](https://raw.githubusercontent.com/kubernetes/website/main/content/zh-cn/examples/pods/probe/tcp-liveness-readiness.yaml))

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: registry.k8s.io/goproxy:0.1
    imagePullPolicy: IfNotPresent
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 10
```

如你所见，TCP 检测的配置和 HTTP 检测非常相似。 下面这个例子同时使用就绪探针和存活探针。kubelet 会在容器启动 15 秒后运行第一次存活探测。 此探测会尝试连接 `goproxy` 容器的 8080 端口。 如果此存活探测失败，容器将被重启。kubelet 将继续每隔 10 秒运行一次这种探测。

除了存活探针，这个配置还包括一个就绪探针。 kubelet 会在容器启动 15 秒后运行第一次就绪探测。 与存活探测类似，就绪探测会尝试连接 `goproxy` 容器的 8080 端口。 如果就绪探测失败，Pod 将被标记为未就绪，且不会接收来自任何服务的流量。

要尝试 TCP 存活检测，运行以下命令创建 Pod：

```shell
kubectl apply -f https://k8s.io/examples/pods/probe/tcp-liveness-readiness.yaml
```

15 秒之后，通过查看 Pod 事件来检测存活探针：

```shell
kubectl describe pod goproxy
```



### 命名端口

对于 HTTP 和 TCP 存活检测可以使用命名的 [`port`](https://kubernetes.io/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#ports)（gRPC 探针不支持使用命名端口）。

例如：

```yaml
ports:
- name: liveness-port
  containerPort: 8080

livenessProbe:
  httpGet:
    path: /healthz
    port: liveness-port
```



### 启动探针

有时候，会有一些现有的应用在启动时需要较长的初始化时间。解决办法是使用相同的命令来设置启动探测，针对 HTTP 或 TCP 检测，可以通过将 `failureThreshold * periodSeconds` 参数设置为足够长的时间来应对最糟糕情况下的启动时间。

这样，前面的例子就变成了：

```yaml
ports:
- name: liveness-port
  containerPort: 8080

livenessProbe:
  httpGet:
    path: /healthz
    port: liveness-port
  failureThreshold: 1
  periodSeconds: 10

startupProbe:
  httpGet:
    path: /healthz
    port: liveness-port
  failureThreshold: 30
  periodSeconds: 10
```

幸亏有启动探测，应用将会有最多 5 分钟（30 * 10 = 300s）的时间来完成其启动过程。 一旦启动探测成功一次，存活探测任务就会接管对容器的探测，对容器死锁作出快速响应。 如果启动探测一直没有成功，容器会在 300 秒后被杀死，并且根据 `restartPolicy` 来执行进一步处置。

# Pod 的终止

由于 Pod 所代表的是在集群中节点上运行的进程，当不再需要这些进程时允许其体面地终止是很重要的。 一般不应武断地使用 `KILL` 信号终止它们，导致这些进程没有机会完成清理操作。

设计的目标是令你能够请求删除进程，并且知道进程何时被终止，同时也能够确保删除操作终将完成。 当你请求删除某个 Pod 时，集群会记录并跟踪 Pod 的体面终止周期， 而不是直接强制地杀死 Pod。在存在强制关闭设施的前提下， [kubelet](https://kubernetes.io/docs/reference/generated/kubelet) 会尝试体面地终止 Pod。

通常 Pod 体面终止的过程为：kubelet 先发送一个带有体面超时限期的 TERM（又名 SIGTERM） 信号到每个容器中的主进程，将请求发送到容器运行时来尝试停止 Pod 中的容器。 

停止容器的这些请求由容器运行时以异步方式处理。 这些请求的处理顺序无法被保证。许多容器运行时遵循容器镜像内定义的 `STOPSIGNAL` 值， 如果不同，则发送容器镜像中配置的 STOPSIGNAL，而不是 TERM 信号。 

一旦超出了体面终止限期，容器运行时会向所有剩余进程发送 KILL 信号，之后 Pod 就会被从 [API 服务器](https://kubernetes.io/zh-cn/docs/concepts/architecture/#kube-apiserver)上移除。 如果 `kubelet` 或者容器运行时的管理服务在等待进程终止期间被重启， 集群会从头开始重试，赋予 Pod 完整的体面终止限期。

Pod 终止流程，如下例所示：

1. 你使用 `kubectl` 工具手动删除某个特定的 Pod，而该 Pod 的体面终止限期是默认值（30 秒）。

2. API 服务器中的 Pod 对象被更新，记录涵盖体面终止限期在内 Pod 的最终死期，超出所计算时间点则认为 Pod 已死（dead）。 如果你使用 `kubectl describe` 来查验你正在删除的 Pod，该 Pod 会显示为 "Terminating" （正在终止）。 在 Pod 运行所在的节点上：`kubelet` 一旦看到 Pod 被标记为正在终止（已经设置了体面终止限期），`kubelet` 即开始本地的 Pod 关闭过程。

   1. 如果 Pod 中的容器之一定义了 `preStop` [回调](https://kubernetes.io/zh-cn/docs/concepts/containers/container-lifecycle-hooks)， `kubelet` 开始在容器内运行该回调逻辑。如果超出体面终止限期时， `preStop` 回调逻辑仍在运行，`kubelet` 会请求给予该 Pod 的宽限期一次性增加 2 秒钟。

   
> **说明：**如果 `preStop` 回调所需要的时间长于默认的体面终止限期，你必须修改 `terminationGracePeriodSeconds` 属性值来使其正常工作。

2. `kubelet` 接下来触发容器运行时发送 TERM 信号给每个容器中的进程 1。
   
   如果 Pod 中定义了[Sidecar 容器](https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/sidecar-containers/)， 则存在[特殊排序](https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#termination-with-sidecars)。否则，Pod 中的容器会在不同的时间和任意的顺序接收 TERM 信号。如果关闭顺序很重要，考虑使用 `preStop` 钩子进行同步（或者切换为使用 Sidecar 容器）。
   
1. 在 `kubelet` 启动 Pod 的体面关闭逻辑的同时，控制平面会评估是否将关闭的 Pod 从对应的 EndpointSlice（和端点）对象中移除，过滤条件是 Pod 被对应的[服务](https://kubernetes.io/zh-cn/docs/concepts/services-networking/service/)以某 [选择算符](https://kubernetes.io/zh-cn/docs/concepts/overview/working-with-objects/labels/)选定。 [ReplicaSet](https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/replicaset/) 和其他工作负载资源不再将关闭进程中的 Pod 视为合法的、能够提供服务的副本。

   关闭动作很慢的 Pod 不应继续处理常规服务请求，而应开始终止并完成对打开的连接的处理。 一些应用程序不仅需要完成对打开的连接的处理，还需要更进一步的体面终止逻辑 - 比如：排空和完成会话。

   任何正在终止的 Pod 所对应的端点都不会立即从 EndpointSlice 中被删除，EndpointSlice API（以及传统的 Endpoints API）会公开一个状态来指示其处于 [终止状态](https://kubernetes.io/zh-cn/docs/concepts/services-networking/endpoint-slices/#conditions)。 正在终止的端点始终将其 `ready` 状态设置为 `false`（为了向后兼容 1.26 之前的版本）， 因此负载均衡器不会将其用于常规流量。

1. kubelet 确保 Pod 被关闭和终止
   1. 超出终止宽限期限时，如果 Pod 中仍有容器在运行，kubelet 会触发强制关闭过程。 容器运行时会向 Pod 中所有容器内仍在运行的进程发送 `SIGKILL` 信号。 `kubelet` 也会清理隐藏的 `pause` 容器，如果容器运行时使用了这种容器的话。
   2. `kubelet` 将 Pod 转换到终止阶段（`Failed` 或 `Succeeded`，具体取决于其容器的结束状态）。
   3. kubelet 通过将宽限期设置为 0（立即删除），触发从 API 服务器强制移除 Pod 对象的操作。
   4. API 服务器删除 Pod 的 API 对象，从任何客户端都无法再看到该对象。



## 强制终止 Pod

默认情况下，所有的删除操作都会附有 30 秒钟的宽限期限。 `kubectl delete` 命令支持 `--grace-period=<seconds>` 选项，允许你重载默认值， 设定自己希望的期限值。

将宽限期限强制设置为 `0` 意味着立即从 API 服务器删除 Pod。 如果 Pod 仍然运行于某节点上，强制删除操作会触发 `kubelet` 立即执行清理操作。

使用 kubectl 时，你必须在设置 `--grace-period=0` 的同时额外设置 `--force` 参数才能发起强制删除请求。

执行强制删除操作时，API 服务器不再等待来自 `kubelet` 的、关于 Pod 已经在原来运行的节点上终止执行的确认消息。 API 服务器直接删除 Pod 对象，这样新的与之同名的 Pod 即可以被创建。 在节点侧，被设置为立即终止的 Pod 仍然会在被强行杀死之前获得一点点的宽限时间。
# 亲和性调度
一般情况下部署的 Pod 是通过集群的自动调度策略来选择节点，默认情况下调度器考虑的是资源足够，并且负载尽量平均，但是有时需要能够更加细粒度的去控制 Pod 的调度。比如希望机器学习的应用只跑在有 GPU 的节点上；有时服务之间交流比较频繁，又希望能够将这服务的 Pod 都调度到同一个的节点上。这就需要使用一些调度方式来控制 Pod 的调度，主要有两个概念：亲和性和反亲和性，亲和性又分成节点亲和性(nodeAffinity)和 Pod 亲和性(podAffinity)。

# nodeSelector
在了解亲和性之前，先来了解一个非常常用的调度方式：nodeSelector。label 标签是 kubernetes 中一个非常重要的概念，用户可以非常灵活的利用 label 来管理集群中的资源，比如最常见的 Service 对象通过 label 去匹配 Pod 资源，而 Pod 的调度也可以根据节点的 label 来进行调度。

通过下面的命令查看 node 的 label：
```sh
[root@master scheduler]# kubectl get nodes --show-labels
NAME     STATUS   ROLES           AGE   VERSION   LABELS
master   Ready    control-plane   32d   v1.25.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
node1    Ready    <none>          32d   v1.25.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux
node2    Ready    <none>          32d   v1.25.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux
[root@master scheduler]#
```
现在给节点 node2 增加一个`com=youdianzhishi`的标签，命令如下：
```sh
[root@master scheduler]# kubectl label nodes node2 com=youdianzhishi
node/node2 labeled
```
可以通过上面的 `--show-labels` 参数可以查看上述标签是否生效。当节点被打上了相关标签后，在调度的时候就可以使用这些标签，只需要在 Pod 的 spec 字段中添加 nodeSelector 字段，里面是需要被调度的节点的 label 标签，比如，下面的 Pod 要强制调度到 node2 这个节点上去，就可以使用 nodeSelector 来表示：
```yaml
# node-selector-demo.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: busybox-pod
  name: test-busybox
spec:
  containers:
    - command:
        - sleep
        - "3600"
      image: busybox
      imagePullPolicy: Always
      name: test-busybox
  nodeSelector:
    com: youdianzhishi
```
然后我们可以通过 describe 命令查看调度结果：
```sh
[root@master scheduler]# kubectl apply -f node-selector-demo.yaml
pod/test-busybox created
[root@master scheduler]# kubectl describe pod test-busybox
Name:             test-busybox
Namespace:        default
Priority:         0
Service Account:  default
Node:             node2/192.168.220.148
Start Time:       Fri, 22 Mar 2024 22:58:49 -0400
Labels:           app=busybox-pod
Annotations:      <none>
Status:           Running
IP:               10.244.2.76
...
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  75s   default-scheduler  Successfully assigned default/test-busybox to node2
  Normal  Pulling    73s   kubelet            Pulling image "busybox"
  Normal  Pulled     58s   kubelet            Successfully pulled image "busybox" in 15.482858678s
  Normal  Created    58s   kubelet            Created container test-busybox
  Normal  Started    58s   kubelet            Started container test-busybox
```
可以看到 Events 下面的信息，Pod 通过默认的 `default-scheduler` 调度器被绑定到 node2 节点。不过需要注意的是 nodeSelector 属于强制性的，如果目标节点没有可用的资源，Pod 就会一直处于 Pending 状态。

通过上面的例子可以感受到 nodeSelector 的方式比较直观，但是还不够灵活，控制粒度偏大，接下来再和大家了解下更加灵活的方式：节点亲和性。

# 亲和性和反亲和性调度
当 Pod 创建后，Kubernetes 的调度器会经过一系列的调度算法将 Pod 调度到最合适的节点上去，但有时需要根据一些场景来自己控制 Pod 的调度，这个时候就要到 nodeAffinity(节点亲和性)、podAffinity(pod 亲和性) 以及 podAntiAffinity(pod 反亲和性)。

亲和性调度可以分成软策略和硬策略两种方式:
- 软策略就是如果现在没有满足调度要求的节点的话，Pod 就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有的话也无所谓。
- 硬策略就比较强硬了，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然就不干了。

对于亲和性和反亲和性都有这两种规则可以设置： `preferredDuringSchedulingIgnoredDuringExecution` 和`requiredDuringSchedulingIgnoredDuringExecution`，前面的就是软策略，后面的就是硬策略。

## 节点亲和性
节点亲和性（nodeAffinity）主要是用来控制 Pod 要部署在哪些节点上，以及不能部署在哪些节点上的，它可以进行一些简单的逻辑组合，而不只是简单的相等匹配。

比如现在用一个 Deployment 来管理 8 个 Pod 副本，现在来控制下这些 Pod 的调度，如下例子：
```yaml
# node-affinity-demo.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-affinity
  labels:
    app: node-affinity
spec:
  replicas: 8
  selector:
    matchLabels:
      app: node-affinity
  template:
    metadata:
      labels:
        app: node-affinity
    spec:
      containers:
        - name: nginx
          image: nginx:1.7.9
          ports:
            - containerPort: 80
              name: nginxweb
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution: # 硬策略
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.io/hostname
                    operator: NotIn
                    values:
                      - master
          preferredDuringSchedulingIgnoredDuringExecution: # 软策略
            - weight: 1
              preference:
                matchExpressions:
                  - key: com
                    operator: In
                    values:
                      - youdianzhishi
```
上面这个 Pod 首先是要求不能运行在 master 节点上，如果有节点满足 `com=youdianzhishi` 的话就优先调度到这个节点上。

由于上面 node2 节点打上了 `com=youdianzhishi` 这个label 标签，所以按要求会优先调度到这个节点来的，现在来创建这个 Pod，然后查看具体的调度情况是否满足我们的要求。
```sh
[root@master scheduler]# kubectl apply -f node-affinity-demo.yaml
deployment.apps/node-affinity created
[root@master scheduler]# kubectl get pods -l app=node-affinity -o wide
NAME                             READY   STATUS    RESTARTS   AGE     IP            NODE    NOMINATED NODE   READINESS GATES
node-affinity-6b6f54966b-4v4gd   1/1     Running   0          2m12s   10.244.2.83   node2   <none>           <none>
node-affinity-6b6f54966b-7z2zx   1/1     Running   0          2m12s   10.244.2.81   node2   <none>           <none>
node-affinity-6b6f54966b-dlwh9   1/1     Running   0          2m12s   10.244.2.77   node2   <none>           <none>
node-affinity-6b6f54966b-pwcpg   1/1     Running   0          2m12s   10.244.2.79   node2   <none>           <none>
node-affinity-6b6f54966b-qt6w8   1/1     Running   0          2m12s   10.244.2.80   node2   <none>           <none>
node-affinity-6b6f54966b-rlbmd   1/1     Running   0          2m12s   10.244.2.82   node2   <none>           <none>
node-affinity-6b6f54966b-rpr9w   1/1     Running   0          2m12s   10.244.2.84   node2   <none>           <none>
node-affinity-6b6f54966b-w42j2   1/1     Running   0          2m12s   10.244.2.78   node2   <none>           <none>
```
看到 8 个 Pod 都被部署到了 node2 节点上，并没有一个 Pod 被部署到 master 节点上，因为硬策略就是不允许部署到该节点上，而 node2 是软策略，所以会尽量满足。这里的匹配逻辑是 label 标签的值在某个列表中，现在 Kubernetes 提供的操作符有下面的几种：
- In：label 的值在某个列表中
- NotIn：label 的值不在某个列表中
- Gt：label 的值大于某个值
- Lt：label 的值小于某个值
- Exists：某个 label 存在
- DoesNotExist：某个 label 不存在

但是需要注意的是，在 Kubernetes 的 Node Affinity 中，`requiredDuringSchedulingIgnoredDuringExecution` 部分的 `nodeSelectorTerms` 是一个数组它包含多个 `nodeSelectorTerm` 对象。当你定义了多个 `nodeSelectorTerm` 对象时，Pod可以调度到满足**任意一个** `nodeSelectorTerm` 条件的节点上。这意味着每个 `nodeSelectorTerm` 在逻辑上“或”（OR）的关系。

每个 `nodeSelectorTerm` 内部的 `matchExpressions` 则是“且”（AND）的关系，意味着一个 `nodeSelectorTerm` 中所有的 `matchExpressions` 必须都满足，节点才能匹配该 `nodeSelectorTerm`。

因此，如果你有两个互斥的 `nodeSelectorTerm`，实际上是给调度器提供了两种选择的可能性，它会尝试找到满足第一个 `nodeSelectorTerm` 或第二个nodeSelectorTerm` 的节点。如果集群中有符合任一条件的节点存在，Pod 就可以被调度到对应的节点上。

例如：
```yaml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: disktype
          operator: In
          values:
          - ssd
      - matchExpressions:
        - key: disktype
          operator: NotIn
          values:
          - hdd
```
在这个例子中，Pod 可以调度到标签为 `disktype=ssd` 的节点上（第一个 `nodeSelectorTerm`），或者调度到标签不是 `disktype=hdd` 的任何节点上（第二个 `nodeSelectorTerm`）。只要集群中存在符合任一条件的节点，Pod 就不会处于 Pending 状态。

## Pod 亲和性
Pod 亲和性（podAffinity）主要解决 Pod 可以和哪些 Pod 部署在同一个拓扑域中的问题（其中拓扑域用主机标签实现，可以是单个主机，也可以是多个主机组成的 cluster、zone 等等），而 Pod 反亲和性主要是解决 Pod 不能和哪些 Pod 部署在同一个拓扑域中的问题，它们都是处理的 Pod 与 Pod 之间的关系。比如一个 Pod 在一个节点上了，那么我这个也得在这个节点，或者你这个 Pod 在节点上了，那么我就不想和你待在同一个节点上。

由于这里只有一个集群，并没有区域或者机房的概念，所以这里直接使用主机名来作为拓扑域，把 Pod 创建在同一个主机上面。同样，还是针对上面的资源对象来测试下 Pod 的亲和性：
```yaml
# pod-affinity-demo.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-affinity
  labels:
    app: pod-affinity
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pod-affinity
  template:
    metadata:
      labels:
        app: pod-affinity
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 80
              name: nginxweb
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution: # 硬策略
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - busybox-pod
              topologyKey: kubernetes.io/hostname
```
上面这个例子中的 Pod 需要调度到某个指定的节点上，并且该节点上运行了一个带有 `app=busybox-pod` 标签的 Pod。可以查看有标签 `app=busybox-pod` 的 pod 列表：
```sh
[root@master scheduler]# kubectl get pod --show-labels -owide
NAME                                     READY   STATUS    RESTARTS   AGE     IP             NODE    NOMINATED NODE   READINESS GATES   LABELS
test-busybox                             1/1     Running   0          22s     10.244.2.85    node2   <none>           <none>            app=busybox-pod
```
看到此 Pod 运行在 node2 节点上，所以按照亲和性来说，上面部署的 3 个 Pod 副本也应该运行在 node2 节点上：
```sh
[root@master scheduler]# kubectl apply -f pod-affinity-demo.yaml
deployment.apps/pod-affinity created
[root@master scheduler]# kubectl get pods -o custom-columns='NAME:metadata.name,NODE:spec.nodeName,STATUS:status.phase,IP:status.podIP' -l app=pod-affinity
NAME                           NODE    STATUS    IP
pod-affinity-76d4d7bfd-9h2gm   node2   Running   10.244.2.87
pod-affinity-76d4d7bfd-9pks5   node2   Running   10.244.2.88
pod-affinity-76d4d7bfd-c9d28   node2   Running   10.244.2.86
```
如果把上面的 `test-busybox` 和 `pod-affinity` 这个 Deployment 都删除，然后重新创建 `pod-affinity` 这个资源，看看能不能正常调度呢：
```sh
[root@master scheduler]# kubectl delete -f node-selector-demo.yaml
pod "test-busybox" deleted
[root@master scheduler]# kubectl delete -f pod-affinity-demo.yaml
deployment.apps "pod-affinity" deleted
[root@master scheduler]# kubectl apply -f pod-affinity-demo.yaml
deployment.apps/pod-affinity created
[root@master scheduler]# kubectl get pods -l app=busybox-pod
No resources found in default namespace.
[root@master scheduler]# kubectl get pods -o custom-columns='NAME:metadata.name,NODE:spec.nodeName,STATUS:status.phase,IP:status.podIP' -l app=pod-affinity
NAME                           NODE     STATUS    IP
pod-affinity-76d4d7bfd-bqr8p   <none>   Pending   <none>
pod-affinity-76d4d7bfd-jhlzf   <none>   Pending   <none>
pod-affinity-76d4d7bfd-mhmn4   <none>   Pending   <none>
```
可以看到都处于 Pending 状态，这是因为现在没有一个节点上面拥有 `app=busybox-pod` 这个标签的 Pod，而上面调度使用的是硬策略，所以就没办法进行调度了，大家可以去尝试下重新将 `test-busybox` 这个 Pod 调度到其他节点上，观察下上面的 3 个副本会不会也被调度到对应的节点上去。

这个地方使用的是 kubernetes.io/hostname 这个拓扑域，意思就是当前调度的 Pod 要和目标的 Pod 处于同一个主机上面，因为要处于同一个拓扑域下面，为了说明这个问题，把拓扑域改成 kubernetes.io/os，同样当前调度的 Pod 要和目标的 Pod 处于同一个拓扑域中，目标的 Pod 是拥有 kubernetes.io/os=linux 的标签，而这里所有节点都有这样的标签，这也就意味着所有节点都在同一个拓扑域中，所以这里的 Pod 可以被调度到任何一个节点，重新运行上面的 `app=busybox-pod` 的 Pod，然后再更新下这里的资源对象：
```sh
[root@master scheduler]# kubectl get pods -o custom-columns='NAME:metadata.name,NODE:spec.nodeName,STATUS:status.phase,IP:status.podIP' -l app=busybox-pod
NAME           NODE    STATUS    IP
test-busybox   node2   Running   10.244.2.89
[root@master scheduler]# kubectl get pods -o custom-columns='NAME:metadata.name,NODE:spec.nodeName,STATUS:status.phase,IP:status.podIP' -l app=pod-affinity
NAME                            NODE    STATUS    IP
pod-affinity-6c57c6cf75-5jkjx   node2   Running   10.244.2.94
pod-affinity-6c57c6cf75-75vx4   node2   Running   10.244.2.96
pod-affinity-6c57c6cf75-857hz   node1   Running   10.244.1.142
```
可以看到现在是分别运行在 2 个节点下面的，因为他们都属于 kubernetes.io/os 这个拓扑域。

## Pod 反亲和性
Pod 反亲和性（podAntiAffinity）则是反着来的，比如一个节点上运行了某个 Pod，那么模板 Pod 则不希望被调度到这个节点上面去了。把上面的 podAffinity 直接改成 podAntiAffinity：
```yaml
# pod-antiaffinity-demo.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-antiaffinity
  labels:
    app: pod-antiaffinity
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pod-antiaffinity
  template:
    metadata:
      labels:
        app: pod-antiaffinity
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 80
              name: nginxweb
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution: # 硬策略
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - busybox-pod
              topologyKey: kubernetes.io/hostname
```
这里的意思就是如果一个节点上面有 `app=busybox-pod` 这样的 Pod，那么我们的 Pod 就别调度到这个节点上面来，上面我们把`app=busybox-pod`这个 Pod 固定到了 node2 这个节点上面的，所以正常来说我们这里的 Pod 不会出现在该节点上：
```sh
[root@master scheduler]# kubectl apply -f pod-antiaffinity-demo.yaml
deployment.apps/pod-antiaffinity created
[root@master scheduler]# kubectl get pods -o custom-columns='NAME:metadata.name,NODE:spec.nodeName,STATUS:status.phase,IP:status.podIP' -l app=pod-antiaffinity
NAME                                NODE    STATUS    IP
pod-antiaffinity-776dd65b6d-6d5xz   node1   Running   10.244.1.147
pod-antiaffinity-776dd65b6d-q6bkz   node1   Running   10.244.1.145
pod-antiaffinity-776dd65b6d-t4fjf   node1   Running   10.244.1.146
```
可以看到没有被调度到 node1 节点上，因为这里使用的是 Pod 反亲和性。大家可以思考下，如果这里将拓扑域更改成 kubernetes.io/os 会怎么样呢？会调度失败 Pod 为 Pending 状态。

# 污点与容忍
对于 nodeAffinity 无论是硬策略还是软策略方式，都是调度 Pod 到预期节点上，而污点（Taints）恰好与之相反，如果一个节点标记为 Taints ，除非 Pod 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度 Pod。

比如用户希望把 master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 Pod，则污点就很有用了，Pod 不会再被调度到 taint 标记过的节点。使用 kubeadm（kind）搭建的集群默认就给 master 节点添加了一个污点标记，所以看到平时的 Pod 都没有被调度到 master 上去：
```sh
[root@master scheduler]# kubectl describe node master
Name:               master
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=master
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"8e:fe:b7:c1:50:14"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 192.168.220.146
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 19 Feb 2024 03:25:13 -0500
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
...
```
可以使用上面的命令查看 master 节点的信息，其中有一条关于 Taints 的信息：`node-role.kubernetes.io/control-plane:NoSchedule`，就表示 master 节点打了一个污点的标记，其中影响的参数是 NoSchedule，表示 Pod 不会被调度到标记为 taints 的节点，除了 NoSchedule 外，还有另外两个选项：
- PreferNoSchedule：NoSchedule 的软策略版本，表示尽量不调度到污点节点上去
- NoExecute：该选项意味着一旦 Taint 生效，如该节点内正在运行的 Pod 没有对应容忍（Tolerate）设置，则会直接被逐出

污点 taint 标记节点的命令如下：
```sh
[root@master scheduler]# kubectl taint nodes node2 test=demo:NoSchedule
node/node2 tainted
[root@master scheduler]# kubectl describe node node2
Name:               node2
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    com=youdianzhishi
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node2
                    kubernetes.io/os=linux
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"42:bb:d0:93:6c:d4"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 192.168.220.148
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 19 Feb 2024 04:34:22 -0500
Taints:             test=demo:NoSchedule
...
```
上面将 node2 节点标记为了污点，影响策略是 NoSchedule，只会影响新的 Pod 调度，如果仍然希望某个 Pod 调度到 taint 节点上，则必须在 Spec 中做出 Toleration 定义，才能调度到该节点，比如现在想要将一个 Pod 调度到 master 节点：
```yaml
# taint-demo.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: taint
  labels:
    app: taint
spec:
  replicas: 3
  selector:
    matchLabels:
      app: taint
  template:
    metadata:
      labels:
        app: taint
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - name: http
              containerPort: 80
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
```
由于 master 节点被标记为了污点，所以这里要想 Pod 能够调度到该节点去，就需要增加容忍的声明：
```yaml
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
```
然后创建上面的资源，查看结果：
```sh
[root@master scheduler]# kubectl apply -f taint-demo.yaml
deployment.apps/taint created
[root@master scheduler]# kubectl get pods -o custom-columns='NAME:metadata.name,NODE:spec.nodeName,STATUS:status.phase,IP:status.podIP' -l app=taint
NAME                     NODE     STATUS    IP
taint-5cb5dcbf55-5qnsq   master   Running   10.244.0.2
taint-5cb5dcbf55-6xdvl   node1    Running   10.244.1.149
taint-5cb5dcbf55-x7jnq   node1    Running   10.244.1.148
```
可以看到有一个 Pod 副本被调度到了 master 节点，这就是容忍的使用方法。

对于 tolerations 属性的写法，其中的 key、value、effect 与 Node 的 Taints 设置需保持一致， 还有以下几点说明：
- 如果 operator 的值是 Exists，则 value 属性可省略
- 如果 operator 的值是 Equal，则表示其 key 与 value 之间的关系是 equal(等于)
- 如果不指定 operator 属性，则默认值为 Equal

另外，还有两个特殊值：
- 空的 key 如果再配合 Exists 就能匹配所有的 key 与 value，也就是能容忍所有节点的所有 Taints
- 空的 effect 匹配所有的 effect

最后如果要取消节点的污点标记，可以使用下面的命令：
```sh
[root@master scheduler]# kubectl taint nodes node2 test-
node/node2 untainted
```
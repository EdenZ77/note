接下来给大家介绍另外一类资源对象： Job ，在日常的工作中经常都会遇到一些需要进行批量数据处理和分析的需求，当然也会有按时间来进行调度的工作，在 Kubernetes 集群中提供了 Job 和 CronJob 两种资源对象来应对这种需求。

Job 负责处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。而 CronJob 则就是在 Job上加上了时间调度。

# Job

用 Job 这个资源对象来创建一个如下所示的任务，该任务负责计算 π 小数点后 2000 位，并将结果打印出来，此计算大约需要 10 秒钟完成。对应的资源清单如下所示：

```yaml
# job-pi.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
        - name: pi
          image: perl:5.34.0
          imagePullPolicy: IfNotPresent
          command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 4
```

可以看到 Job 中也是一个 Pod 模板，和之前的 Deployment、StatefulSet 之类的是一致的，只是 Pod 中的容器要求是一个任务，而不是一个常驻的进程了，因为需要退出；另外值得注意的是 Job 的 `RestartPolicy` 仅支持 `Never` 和 `OnFailure` 两种，不支持 `Always`。我们知道 Job 就相当于来执行一个批处理任务，执行完就结束了，如果支持 `Always` 的话是不是就陷入了死循环了？

直接创建这个 Job 对象：

```shell
[root@master yamlDir]# kubectl apply -f job-pi.yaml
job.batch/pi created
[root@master yamlDir]# kubectl get job
NAME   COMPLETIONS   DURATION   AGE
pi     0/1           5s         5s
[root@master controller]# kubectl get pods
NAME                                     READY   STATUS              RESTARTS        AGE
pi-lbjxd                                 0/1     ContainerCreating   0               23s
```

Job 对象创建成功后，可以查看下对象的详细描述信息：

```shell
[root@master yamlDir]# kubectl describe job pi
Name:             pi
Namespace:        default
# 注意这个选择器
Selector:         controller-uid=38738e0d-bb61-4cd1-b7cc-44f60f31d858
Labels:           controller-uid=38738e0d-bb61-4cd1-b7cc-44f60f31d858
                  job-name=pi
Annotations:      batch.kubernetes.io/job-tracking:
Parallelism:      1
Completions:      1
Completion Mode:  NonIndexed
Start Time:       Sat, 24 Feb 2024 17:50:55 -0500
Completed At:     Sat, 24 Feb 2024 17:52:18 -0500
Duration:         83s
Pods Statuses:    0 Active (0 Ready) / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  controller-uid=38738e0d-bb61-4cd1-b7cc-44f60f31d858
           job-name=pi
  Containers:
   pi:
    Image:      perl:5.34.0
    Port:       <none>
    Host Port:  <none>
    Command:
      perl
      -Mbignum=bpi
      -wle
      print bpi(2000)
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  94s   job-controller  Created pod: pi-j6zg5
  Normal  Completed         11s   job-controller  Job completed
```

可以看到，Job 对象在创建后，它的 Pod 模板，被自动加上了一个 `controller-uid=< 一个随机字符串 >` 这样的 Label 标签，而这个 Job 对象本身，则被自动加上了这个 Label 对应的 `Selector`，从而保证了 Job 与它所管理的 Pod 之间的匹配关系。而 Job 控制器之所以要使用这种携带了 UID 的 Label，就是为了避免不同 Job 对象所管理的 Pod 发生重合。

可以看到隔一会儿后 Pod 变成了 `Completed` 状态，这是因为容器的任务执行完成正常退出了，可以查看对应的日志：

```shell
[root@master yamlDir]# kubectl get pods
NAME       READY   STATUS      RESTARTS   AGE
pi-j6zg5   0/1     Completed   0          7m17s

[root@master yamlDir]# kubectl logs pi-j6zg5
3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421......
[root@master controller]# kubectl get job
#                    job持续的时间
NAME   COMPLETIONS   DURATION   AGE
pi     1/1           9s         15m
```

上面这里的 Job 任务对应的 Pod 在运行结束后，会变成 `Completed` 状态，但是如果执行任务的 Pod 因为某种原因一直没有结束怎么办呢？同样可以在 Job 对象中通过设置字段 `spec.activeDeadlineSeconds` 来限制任务运行的最长时间，比如：

```yaml
spec:
 activeDeadlineSeconds: 100
```

在 Kubernetes Job 的配置中，`spec.activeDeadlineSeconds` 字段允许设置一个时间限制，表示该 Job 可以运行的最长时间（以秒为单位）。如果设置了这个字段，Job 运行的时间超过了这个秒数，系统就会尝试终止这个 Job。这个时间限制包括了 Job 的启动时间、运行时间以及任何重试的时间。那么当 Job 运行超过了 100s 后，这个 Job 的所有 Pod 都会被终止，并且Job的终止原因会变成 `DeadlineExceeded`。

如果任务执行失败了，会怎么处理呢，这个和定义的 `restartPolicy` 有关系，比如定义如下所示的 Job 任务，定义 `restartPolicy: Never` 的重启策略：

```yaml
# job-failed-demo.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: job-failed-demo
spec:
  template:
    spec:
      containers:
      - name: test-job
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["echo123", "test failed job!"]
      restartPolicy: Never
```

直接创建上面的资源对象：

```shell
➜  ~ kubectl apply -f job-failed-demo.yaml
job.batch/job-failed-demo created
[root@master ~]# kubectl get pod -w
# 可以看到 RESTARTS 都是0
NAME                                     READY   STATUS              RESTARTS       AGE
job-failed-demo-tsdvs                    0/1     Pending             0              0s
job-failed-demo-tsdvs                    0/1     ContainerCreating   0              0s
job-failed-demo-tsdvs                    0/1     StartError          0              1s
job-failed-demo-86gcx                    0/1     Pending             0              0s
job-failed-demo-86gcx                    0/1     ContainerCreating   0              0s
job-failed-demo-86gcx                    0/1     StartError          0              1s
job-failed-demo-gwmtt                    0/1     Pending             0              0s
job-failed-demo-gwmtt                    0/1     ContainerCreating   0              0s
job-failed-demo-gwmtt                    0/1     StartError          0              1s
job-failed-demo-8hlpv                    0/1     Pending             0              0s
job-failed-demo-8hlpv                    0/1     ContainerCreating   0              0s
job-failed-demo-8hlpv                    0/1     StartError          0              3s
job-failed-demo-gn884                    0/1     Pending             0              0s
job-failed-demo-gn884                    0/1     ContainerCreating   0              0s
job-failed-demo-gn884                    0/1     StartError          0              3s
job-failed-demo-2lmsx                    0/1     Pending             0              0s
job-failed-demo-2lmsx                    0/1     ContainerCreating   0              0s
job-failed-demo-2lmsx                    0/1     StartError          0              3s
job-failed-demo-94p9j                    0/1     Pending             0              0s
job-failed-demo-94p9j                    0/1     ContainerCreating   0              0s
job-failed-demo-94p9j                    0/1     StartError          0              3s
# 共7次失败
[root@master controller]# kubectl describe job job-failed-demo
Name:             job-failed-demo
Namespace:        default
Selector:         controller-uid=5eac4705-0216-45c4-b57a-f66df82b104b
Labels:           controller-uid=5eac4705-0216-45c4-b57a-f66df82b104b
                  job-name=job-failed-demo
Annotations:      batch.kubernetes.io/job-tracking:
Parallelism:      1
Completions:      1
Completion Mode:  NonIndexed
Start Time:       Mon, 21 Oct 2024 23:09:24 -0400
Pods Statuses:    0 Active (0 Ready) / 0 Succeeded / 7 Failed # 可以看到7次失败
Pod Template:
  Labels:  controller-uid=5eac4705-0216-45c4-b57a-f66df82b104b
           job-name=job-failed-demo
  Containers:
   test-job:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Command:
      echo123
      test failed job!
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type     Reason                Age    From            Message
  ----     ------                ----   ----            -------
  # job-controller 重启了6个pod，第一个不算
  Normal   SuccessfulCreate      2m11s  job-controller  Created pod: job-failed-demo-tsdvs
  Normal   SuccessfulCreate      2m8s   job-controller  Created pod: job-failed-demo-86gcx
  Normal   SuccessfulCreate      2m5s   job-controller  Created pod: job-failed-demo-gwmtt
  Normal   SuccessfulCreate      2m2s   job-controller  Created pod: job-failed-demo-8hlpv
  Normal   SuccessfulCreate      119s   job-controller  Created pod: job-failed-demo-gn884
  Normal   SuccessfulCreate      116s   job-controller  Created pod: job-failed-demo-2lmsx
  Normal   SuccessfulCreate      113s   job-controller  Created pod: job-failed-demo-94p9j
  # 重启次数达到限制
  Warning  BackoffLimitExceeded  110s   job-controller  Job has reached the specified backoff limit
```

可以看到当设置成 `Never` 重启策略的时候，Job 任务执行失败后会不断创建新的 Pod，但是不会一直创建下去，会根据 `spec.backoffLimit` 参数进行限制，默认为 `6`，通过该字段可以定义重建 Pod 的次数。

但是如果设置的 `restartPolicy: OnFailure` 重启策略，则当 Job 任务执行失败后不会创建新的 Pod 出来，只会不断重启 Pod，比如将上面的 Job 任务 `restartPolicy` 更改为 `OnFailure` 后查看 Pod：

```shell
[root@master ~]# kubectl get pod -w -owide
NAME                                     READY   STATUS              RESTARTS       AGE    IP             NODE     NOMINATED NODE   READINESS GATES
job-failed-demo-j65zq                    0/1     Pending             0              0s     <none>         <none>   <none>           <none>
job-failed-demo-j65zq                    0/1     ContainerCreating   0              0s     <none>         node1    <none>           <none>
job-failed-demo-j65zq                    0/1     RunContainerError   0 (1s ago)     2s     10.244.1.6     node1    <none>           <none>
job-failed-demo-j65zq                    0/1     RunContainerError   1 (1s ago)     3s     10.244.1.6     node1    <none>           <none>
job-failed-demo-j65zq                    0/1     CrashLoopBackOff    1 (2s ago)     4s     10.244.1.6     node1    <none>           <none>
job-failed-demo-j65zq                    0/1     RunContainerError   2 (0s ago)     16s    10.244.1.6     node1    <none>           <none>
job-failed-demo-j65zq                    0/1     CrashLoopBackOff    2 (15s ago)    31s    10.244.1.6     node1    <none>           <none>
job-failed-demo-j65zq                    0/1     RunContainerError   3 (0s ago)     44s    10.244.1.6     node1    <none>           <none>
[root@master controller]# kubectl describe pod job-failed-demo-j65zq
Name:             job-failed-demo-j65zq
Namespace:        default
Priority:         0
Service Account:  default
Node:             node1/192.168.220.147
Start Time:       Mon, 21 Oct 2024 23:22:21 -0400
Labels:           controller-uid=4676fa3b-a60e-4cba-a4d8-f0640cf8bf49
                  job-name=job-failed-demo
Annotations:      <none>
Status:           Running
IP:               10.244.1.6
IPs:
  IP:           10.244.1.6
# 被谁控制
Controlled By:  Job/job-failed-demo
Containers:
  test-job:
    Container ID:  containerd://42842151aeef9147b546320e034d1355d1f038f0b999efc7ee950734cc948843
    Image:         busybox
    Image ID:      sha256:beae173ccac6ad749f76713cf4440fe3d21d1043fe616dfbe30775815d1d0f6a
    Port:          <none>
    Host Port:     <none>
    Command:
      echo123
      test failed job!
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated # 上次状态
      Reason:       StartError # 具体原因
      Message:      failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: "echo123": executable file not found in $PATH: unknown
      Exit Code:    128
      Started:      Wed, 31 Dec 1969 19:00:00 -0500
      Finished:     Mon, 21 Oct 2024 23:25:21 -0400
    Ready:          False
    Restart Count:  5 # 重启次数
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-447s6 (ro)
```

除此之外，还可以通过设置 `spec.parallelism` 参数来进行并行控制，该参数定义了一个 Job 在任意时间最多可以有多少个 Pod 同时运行。并行性请求（`.spec.parallelism`）可以设置为任何非负整数，如果未设置，则默认为 1，如果设置为 0，则 Job 相当于启动之后便被暂停，直到此值被增加。

`spec.completions` 参数可以定义 Job 至少要完成的 Pod 数目。如下所示创建一个新的 Job 任务，设置允许并行数为 2，至少要完成的 Pod 数为 8：

```yaml
# job-para-demo.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: job-para-test
spec:
  parallelism: 2
  completions: 8
  template:
    spec:
      containers:
        - name: test-job
          image: busybox
          imagePullPolicy: IfNotPresent
          command: ["echo", "test paralle job!"]
      restartPolicy: Never
```

创建完成后查看任务状态：

```shell
[root@master controller]# kubectl get job
NAME              COMPLETIONS   DURATION   AGE
job-para-test     8/8           17s        40s

[root@master ~]# kubectl get pod -w -owide
NAME                                     READY   STATUS              RESTARTS         AGE    IP             NODE     NOMINATED NODE   READINESS GATES
job-para-test-c87rq                      0/1     Pending             0                0s     <none>         node1    <none>           <none>
job-para-test-fk2mt                      0/1     Pending             0                0s     <none>         node1    <none>           <none>
job-para-test-c87rq                      0/1     ContainerCreating   0                0s     <none>         node1    <none>           <none>
job-para-test-fk2mt                      0/1     ContainerCreating   0                0s     <none>         node1    <none>           <none>
job-para-test-c87rq                      0/1     Completed           0                4s     10.244.1.7     node1    <none>           <none>
job-para-test-fk2mt                      0/1     Completed           0                4s     10.244.1.8     node1    <none>           <none>
job-para-test-w7h4q                      0/1     Pending             0                0s     <none>         node1    <none>           <none>
job-para-test-pbrs8                      0/1     Pending             0                0s     <none>         <none>   <none>           <none>
job-para-test-w7h4q                      0/1     ContainerCreating   0                0s     <none>         node1    <none>           <none>
job-para-test-c87rq                      0/1     Completed           0                5s     10.244.1.7     node1    <none>           <none>
job-para-test-fk2mt                      0/1     Completed           0                5s     10.244.1.8     node1    <none>           <none>
job-para-test-pbrs8                      0/1     ContainerCreating   0                0s     <none>         node1    <none>           <none>
job-para-test-w7h4q                      0/1     Completed           0                3s     10.244.1.9     node1    <none>           <none>
job-para-test-pbrs8                      0/1     Completed           0                3s     10.244.1.10    node1    <none>           <none>
job-para-test-fhvzv                      0/1     Pending             0                0s     <none>         <none>   <none>           <none>
job-para-test-4cgp6                      0/1     Pending             0                0s     <none>         <none>   <none>           <none>
job-para-test-w7h4q                      0/1     Completed           0                4s     10.244.1.9     node1    <none>           <none>
job-para-test-pbrs8                      0/1     Completed           0                4s     10.244.1.10    node1    <none>           <none>
job-para-test-fhvzv                      0/1     ContainerCreating   0                0s     <none>         node1    <none>           <none>
job-para-test-4cgp6                      0/1     ContainerCreating   0                0s     <none>         node1    <none>           <none>
job-para-test-4cgp6                      0/1     Completed           0                3s     10.244.1.12    node1    <none>           <none>
job-para-test-fhvzv                      0/1     Completed           0                3s     10.244.1.11    node1    <none>           <none>
job-para-test-9blws                      0/1     Pending             0                0s     <none>         <none>   <none>           <none>
job-para-test-wzcsp                      0/1     Pending             0                0s     <none>         node1    <none>           <none>
job-para-test-9blws                      0/1     ContainerCreating   0                0s     <none>         node1    <none>           <none>
job-para-test-wzcsp                      0/1     ContainerCreating   0                0s     <none>         node1    <none>           <none>
job-para-test-wzcsp                      0/1     Completed           0                4s     10.244.1.14    node1    <none>           <none>
job-para-test-9blws                      0/1     Completed           0                4s     10.244.1.13    node1    <none>           <none>
```

可以看到一次可以有 2 个 Pod 同时运行，需要 8 个 Pod 执行成功，如果不是 8 个成功，那么会根据 `restartPolicy` 的策略进行处理，可以认为是一种检查机制。

此外带有确定完成计数的 Job，即 `.spec.completions` 不为 `nil` 的 Job（设置为 `nil` 表示只要有一个 Pod 成功即可）， 都可以在其 `.spec.completionMode` 中设置完成模式：

- `NonIndexed`（默认值）：当成功完成的 Pod 个数达到 `.spec.completions` 所设值时认为 Job 已经完成。换言之，每个 Job 完成事件都是独立无关且同质的。要注意的是，当 `.spec.completions` 取值为 `nil` 时，Job 被默认处理为 `NonIndexed` 模式。
- `Indexed`：当设置为 `Indexed` 时，Job 中的每个 Pod 都会获得一个从 0 到 (`.spec.completions - 1`) 的连续索引，这个索引会以注解的形式（`batch.kubernetes.io/job-completion-index`）呈现在 Pod 上。Job 在每个索引都有至少一个成功完成的 Pod 时被认为是完成的。此模式下，必须指定 `.spec.completions`，且 `.spec.parallelism` 必须小于或等于 `10^5`。此外，在这种模式下，Pod 的名字会采用 `$(job-name)-$(index)-$(random-string)` 格式，Pod 的主机名会采用 `$(job-name)-$(index)` 格式。索引可以通过四种方式获取：
  - Pod 注解 `batch.kubernetes.io/job-completion-index`。
  - Pod 标签 `batch.kubernetes.io/job-completion-index`（适用于 v1.28 及更高版本）。 请注意，必须启用 `PodIndexLabel` 特性门控才能使用此标签，默认被启用。
  - 作为 Pod 主机名的一部分，遵循模式 `$(job-name)-$(index)`。 当你同时使用带索引的 Job（Indexed Job）与 [服务（Service）](https://kubernetes.io/zh-cn/docs/concepts/services-networking/service/)， Job 中的 Pod 可以通过 DNS 使用确切的主机名互相寻址。 有关如何配置的更多信息，请参阅[带 Pod 间通信的 Job](https://kubernetes.io/zh-cn/docs/tasks/job/job-with-pod-to-pod-communication/)。
  - 对于容器化的任务，在环境变量 `JOB_COMPLETION_INDEX` 中。

当每个索引都有对应一个成功完成的 Pod 时，Job 被认为是已完成的。

## 索引完成模式

下面将运行一个使用多个并行工作进程的 Kubernetes Job。 Pod 具有控制平面自动设置的索引编号（index number）， 这些编号使得每个 Pod 能识别出要处理整个任务的哪个部分。

Pod 索引在注解 `batch.kubernetes.io/job-completion-index` 中呈现，具体表示为一个十进制值字符串。为了让容器化的任务进程获得此索引，可以使用 Downward API 机制来获取注解的值，而且控制平面会自动设置环境变量`JOB_COMPLETION_INDEX` 来暴露索引。

这里创建一个如下所示的 Job 资源清单文件：

```yaml
# job-indexed.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: indexed-job
spec:
  completions: 5
  parallelism: 3
  completionMode: Indexed
  template:
    spec:
      restartPolicy: Never
      initContainers:
        - name: "input"
          image: "bash"
          imagePullPolicy: IfNotPresent
          command:
            - "bash"
            - "-c"
            - |
              items=(foo bar baz qux xyz)
              echo ${items[$JOB_COMPLETION_INDEX]} > /input/data.txt
          volumeMounts:
            - mountPath: /input
              name: input
      containers:
        - name: "worker"
          image: "busybox"
          imagePullPolicy: IfNotPresent
          command:
            - "rev"
            - "/input/data.txt"
          volumeMounts:
            - mountPath: /input
              name: input
      volumes:
        - name: input
          emptyDir: {}
```

在该资源清单中设置了 `completionMode: Indexed`，表示这是一个 `Indexed` 完成模式的 Job 任务。这里还使用了 Job 控制器为 Pod 的所有容器设置的内置 `JOB_COMPLETION_INDEX` 环境变量。 Init 容器将索引映射到一个静态值，并将其写入一个文件，该文件通过 `emptyDir` 卷与运行 `worker` 的容器共享。

> rev（reverse）命令用于将文件中的每行内容以字符为单位反序输出，即第一个字符最后输出，最后一个字符最先输出，以此类推。

直接创建该资源对象即可：

```shell
[root@master yamlDir]# kubectl apply -f job-indexed.yaml
job.batch/indexed-job created

[root@master ~]# kubectl get pod -w -owide
NAME                                     READY   STATUS              RESTARTS      AGE    IP             NODE     NOMINATED NODE   READINESS GATES
indexed-job-0-p97vx                      0/1     Pending             0             0s     <none>         <none>   <none>           <none>
indexed-job-2-qjjwf                      0/1     Pending             0             0s     <none>         <none>   <none>           <none>
indexed-job-1-wmt47                      0/1     Pending             0             0s     <none>         <none>   <none>           <none>
indexed-job-0-p97vx                      0/1     Init:0/1            0             0s     <none>         node1    <none>           <none>
indexed-job-2-qjjwf                      0/1     Init:0/1            0             0s     <none>         node1    <none>           <none>
indexed-job-1-wmt47                      0/1     Init:0/1            0             0s     <none>         node1    <none>           <none>
indexed-job-1-wmt47                      0/1     PodInitializing     0             61s    10.244.1.17    node1    <none>           <none>
indexed-job-1-wmt47                      0/1     Completed           0             62s    10.244.1.17    node1    <none>           <none>
indexed-job-3-bcsrv                      0/1     Pending             0             0s     <none>         <none>   <none>           <none>
indexed-job-1-wmt47                      0/1     Completed           0             65s    10.244.1.17    node1    <none>           <none>
indexed-job-3-bcsrv                      0/1     Init:0/1            0             0s     <none>         node1    <none>           <none>
indexed-job-3-bcsrv                      0/1     PodInitializing     0             2s     10.244.1.18    node1    <none>           <none>
indexed-job-3-bcsrv                      0/1     Completed           0             5s     10.244.1.18    node1    <none>           <none>
indexed-job-4-54bjb                      0/1     Pending             0             0s     <none>         <none>   <none>           <none>
indexed-job-4-54bjb                      0/1     Init:0/1            0             0s     <none>         node1    <none>           <none>
indexed-job-3-bcsrv                      0/1     Completed           0             6s     10.244.1.18    node1    <none>           <none>
indexed-job-4-54bjb                      0/1     PodInitializing     0             2s     10.244.1.19    node1    <none>           <none>
indexed-job-2-qjjwf                      0/1     PodInitializing     0             75s    10.244.1.15    node1    <none>           <none>
indexed-job-4-54bjb                      0/1     Completed           0             6s     10.244.1.19    node1    <none>           <none>
indexed-job-2-qjjwf                      0/1     Completed           0             79s    10.244.1.15    node1    <none>           <none>
indexed-job-0-p97vx                      0/1     Init:0/1            0             80s    10.244.1.16    node1    <none>           <none>
indexed-job-0-p97vx                      0/1     PodInitializing     0             81s    10.244.1.16    node1    <none>           <none>
indexed-job-0-p97vx                      0/1     Completed           0             85s    10.244.1.16    node1    <none>           <none>
```

创建此 Job 时，控制平面会创建一系列 Pod。 `.spec.parallelism` 的值决定了一次可以运行多少个 Pod， 而 `.spec.completions` 决定了 Job 总共创建了多少个 Pod。因为 `.spec.parallelism` 小于 `.spec.completions`， 所以控制平面在启动更多 Pod 之前，将等待第一批的某些 Pod 完成。

创建 Job 后，稍等片刻，就能检查进度：

```shell
[root@master controller]# kubectl describe job indexed-job
Name:               indexed-job
Namespace:          default
Selector:           controller-uid=dd3f3091-3001-4e88-9d2a-1c6e0623043a
Labels:             controller-uid=dd3f3091-3001-4e88-9d2a-1c6e0623043a
                    job-name=indexed-job
Annotations:        batch.kubernetes.io/job-tracking:
Parallelism:        3
Completions:        5
Completion Mode:    Indexed
Start Time:         Tue, 22 Oct 2024 05:50:14 -0400
Completed At:       Tue, 22 Oct 2024 05:51:39 -0400
Duration:           85s
Pods Statuses:      0 Active (0 Ready) / 5 Succeeded / 0 Failed
Completed Indexes:  0-4
Pod Template:
  Labels:  controller-uid=dd3f3091-3001-4e88-9d2a-1c6e0623043a
           job-name=indexed-job
  Init Containers:
   input:
  ......
  Containers:
   worker:
  ......
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  13m   job-controller  Created pod: indexed-job-0-p97vx
  Normal  SuccessfulCreate  13m   job-controller  Created pod: indexed-job-1-wmt47
  Normal  SuccessfulCreate  13m   job-controller  Created pod: indexed-job-2-qjjwf
  Normal  SuccessfulCreate  11m   job-controller  Created pod: indexed-job-3-bcsrv
  Normal  SuccessfulCreate  11m   job-controller  Created pod: indexed-job-4-54bjb
  Normal  Completed         11m   job-controller  Job completed
```

这里使用每个索引的自定义值运行 Job，可以检查其中 Pod 的输出：

```shell
[root@master yamlDir]# kubectl logs indexed-job-0-p8np2
Defaulted container "worker" out of: worker, input (init)
oof
[root@master yamlDir]# kubectl logs indexed-job-1-zvpsm
Defaulted container "worker" out of: worker, input (init)
rab
```

在初始化容器中执行了如下所示的命令：

```shell
items=(foo bar baz qux xyz)
echo ${items[$JOB_COMPLETION_INDEX]} > /input/data.txt
```

当 `JOB_COMPLETION_INDEX=3` 的时候表示将 `items[3]` 的 qux 值写入到了 `/input/data.txt` 文件中，然后通过 `volume` 共享，在主容器中通过 `rev` 命令将其反转，所以输出结果就是 `xuq` 。

上面这个示例中每个 Pod 只做一小部分工作（反转一个字符串），在实际工作中肯定比这复杂，比如可能会创建一个基于场景数据制作 60 秒视频任务的 Job，此视频渲染 Job 中的每个工作项都将渲染该视频的特定帧，索引完成模式意味着 Job 中的每个 Pod 都知道通过从视频开始计算帧数，来确定渲染和发布哪一帧，这样就可以大大提高工作任务的效率。

## Pod间通信的Job

> 参考资料：https://kubernetes.io/zh-cn/docs/tasks/job/job-with-pod-to-pod-communication/







# CronJob

CronJob 其实就是在 Job 的基础上加上了时间调度，我们可以在给定的时间点运行一个任务，也可以周期性地在给定时间点运行。这个实际上和我们 Linux 中的 crontab 就非常类似了。

crontab 的格式为：分 时 日 月 星期 要运行的命令 。

```shell
# ┌───────────── 分钟 (0 - 59)
# │ ┌───────────── 小时 (0 - 23)
# │ │ ┌───────────── 月的某天 (1 - 31)
# │ │ │ ┌───────────── 月份 (1 - 12)
# │ │ │ │ ┌───────────── 周的某天 (0 - 6)（周日到周一；在某些系统上，7 也是星期日）
# │ │ │ │ │             或者是 sun，mon，tue，web，thu，fri，sat
# │ │ │ │ │
# │ │ │ │ │
# * * * * *
```

- 第 1 列分钟 0 ～ 59
- 第 2 列小时 0 ～ 23
- 第 3 列日 1 ～ 31
- 第 4 列月 1 ～ 12
- 第 5 列星期 0 ～ 7（0 和 7 表示星期天）
- 第 6 列要运行的命令

所有 CronJob 的 schedule 时间都是基于 kube-controller-manager 的时区，如果你的控制平面在 Pod 中运行了 kube-controller-manager， 那么为该容器所设置的时区将会决定 CronJob 的控制器所使用的时区。官方并不支持设置如 CRON_TZ 或者 TZ 等变量，这两个变量是用于解析和计算下一个 Job 创建时间所使用的内部库中一个实现细节，不建议在生产集群中使用它。

但是如果在 kube-controller-manager 中启用了 CronJobTimeZone 这个 Feature Gates，那么我们就可以为 CronJob 指定一个时区（如果你没有启用该特性门控，或者你使用的是不支持试验性时区功能的 Kubernetes 版本，集群中所有 CronJob 的时区都是未指定的）。启用该特性后，你可以将 spec.timeZone 设置为有效时区名称。

现在，我们用 CronJob 来管理我们上面的 Job 任务，定义如下所示的资源清单：

```yaml
# cronjob-demo.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cronjob-demo
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
            - name: hello
              image: busybox
              args:
                - "bin/sh"
                - "-c"
                - "for i in 9 8 7 6 5 4 3 2 1; do echo $i; done"
```

这里的 Kind 变成了 CronJob 了，要注意的是 .spec.schedule 字段是必须填写的，用来指定任务运行的周期，格式就和 crontab 一样，另外一个字段是 .spec.jobTemplate, 用来指定需要运行的任务，格式当然和 Job 是一致的。

还有一些值得我们关注的字段 .spec.successfulJobsHistoryLimit(默认为 3) 和 .spec.failedJobsHistoryLimit(默认为 1)，表示历史限制，是可选的字段，指定可以保留多少完成和失败的 Job。然而，当运行一个 CronJob 时，Job 可以很快就堆积很多，所以一般推荐设置这两个字段的值，如果设置限制的值为 0，那么相关类型的 Job 完成后将不会被保留。

我们直接新建上面的资源对象：

```
➜  ~ kubectl apply -f cronjob-demo.yaml
cronjob "cronjob-demo" created
```

然后可以查看对应的 Cronjob 资源对象：

```
[root@master yamlDir]# kubectl get cronjob -owide
NAME           SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE     CONTAINERS   IMAGES    SELECTOR
cronjob-demo   */1 * * * *   False     0        60s             2m15s   hello        busybox   <none>
```

稍微等一会儿查看可以发现多了几个 Job 资源对象，这个就是因为上面我们设置的 CronJob 资源对象，每 1 分钟执行一个新的 Job：

```shell
[root@master yamlDir]# kubectl get job -owide
NAME                    COMPLETIONS   DURATION   AGE     CONTAINERS   IMAGES    SELECTOR
cronjob-demo-28480384   1/1           20s        2m39s   hello        busybox   controller-uid=d5065b99-9896-476a-a219-d9600eed0d80
cronjob-demo-28480385   1/1           19s        99s     hello        busybox   controller-uid=e01b4708-5f31-4368-a514-31854f164cf8
cronjob-demo-28480386   1/1           19s        39s     hello        busybox   controller-uid=aa297cbc-f77f-4f2d-bc8e-6d83536243ba

[root@master yamlDir]# kubectl get pod
NAME                          READY   STATUS      RESTARTS   AGE
cronjob-demo-28480384-gcrgh   0/1     Completed   0          2m46s
cronjob-demo-28480385-48m62   0/1     Completed   0          106s
cronjob-demo-28480386-v5q2w   0/1     Completed   0          46s
[root@master yamlDir]#
```

这个就是 CronJob 的基本用法，一旦不再需要 CronJob，我们可以使用 kubectl 命令删除它：

```shell
➜  ~ kubectl delete cronjob cronjob-demo
cronjob "cronjob-demo" deleted
```

不过需要注意的是这将会终止正在创建的 Job，但是运行中的 Job 将不会被终止，不会删除它们的 Job 或 Pod。

> 思考：那如果我们想要在每个节点上去执行一个 Job 或者 Cronjob 又该怎么来实现呢？
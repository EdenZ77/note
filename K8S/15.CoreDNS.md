# 服务发现
前面我们讲解了 Service 的用法，我们可以通过 Service 生成的 ClusterIP(VIP) 来访问 Pod 提供的服务，但是在使用的时候还有一个问题：我们怎么知道某个应用的 VIP 呢？比如我们有两个应用，一个是 api 应用，一个是 db 应用，两个应用都是通过 Deployment 进行管理的，并且都通过 Service 暴露出了端口提供服务。api 需要连接到 db 这个应用，我们只知道 db 应用的名称和 db 对应的 Service 的名称，但是并不知道它的 VIP 地址，如果我们知道了 VIP 的地址是不是就行了？

# 环境变量
为了解决上面的问题，在之前的版本中，Kubernetes 采用了环境变量的方法，每个 Pod 启动的时候，会通过环境变量设置所有服务的 IP 和 port 信息，这样 Pod 中的应用可以通过读取环境变量来获取依赖服务的地址信息，这种方法使用起来相对简单，但是有一个很大的问题就是依赖的服务必须在 Pod 启动之前就存在，不然是不会被注入到环境变量中的。比如我们首先创建一个 Nginx 服务：
```yaml
# test-nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.7.9
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  labels:
    name: nginx-service
spec:
  ports:
    - port: 5000
      targetPort: 80
  selector:
    app: nginx
```
创建上面的服务：
```sh
[root@master dns]# kubectl apply -f test-nginx.yaml
deployment.apps/nginx-deploy created
service/nginx-service created
[root@master dns]# kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
nginx-deploy-7759cfdc55-mpslb   1/1     Running   0          80s
nginx-deploy-7759cfdc55-zxn57   1/1     Running   0          80s
[root@master dns]# kubectl get svc
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
kubernetes      ClusterIP   10.96.0.1       <none>        443/TCP    15d
nginx-service   ClusterIP   10.98.134.163   <none>        5000/TCP   87s
```
我们可以看到两个 Pod 和一个名为 nginx-service 的服务创建成功了，该 Service 监听的端口是 5000，同时它会把流量转发给它代理的所有 Pod（我们这里就是拥有 app: nginx 标签的两个 Pod）。

现在我们再来创建一个普通的 Pod，观察下该 Pod 中的环境变量是否包含上面的 nginx-service 的服务信息：
```yaml
# test-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
    - name: test-service-pod
      image: busybox
      command: ["/bin/sh", "-c", "env"]
```
然后创建该测试的 Pod：
```sh
[root@master dns]# kubectl apply -f test-pod.yaml
pod/test-pod created
```
等 Pod 创建完成后，我们查看日志信息：
```sh
[root@master dns]# kubectl logs test-pod
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_SERVICE_PORT=443
HOSTNAME=test-pod
SHLVL=1
HOME=/root
NGINX_SERVICE_PORT_5000_TCP_ADDR=10.98.134.163
NGINX_SERVICE_PORT_5000_TCP_PORT=5000
NGINX_SERVICE_PORT_5000_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
NGINX_SERVICE_SERVICE_HOST=10.98.134.163
NGINX_SERVICE_PORT_5000_TCP=tcp://10.98.134.163:5000
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_PROTO=tcp
NGINX_SERVICE_PORT=tcp://10.98.134.163:5000
NGINX_SERVICE_SERVICE_PORT=5000
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_SERVICE_HOST=10.96.0.1
PWD=/
```
我们可以看到打印了很多环境变量信息，其中就包括我们刚刚创建的 nginx-service 这个服务，有 HOST、PORT、PROTO、ADDR 等，也包括其他已经存在的 Service 的环境变量，现在如果我们需要在这个 Pod 里面访问 nginx-service 的服务，我们是不是可以直接通过 NGINX_SERVICE_SERVICE_HOST 和 NGINX_SERVICE_SERVICE_PORT 就可以了，但是如果这个 Pod 启动起来的时候 nginx-service 服务还没启动起来，在环境变量中我们是无法获取到这些信息的，当然我们可以通过 initContainer 之类的方法来确保 nginx-service 启动后再启动 Pod，但是这种方法毕竟增加了 Pod 启动的复杂性，所以这不是最优的方法，局限性太多了。

# DNS
这部分可以看CS_Base/Network/应用层/DNS.md文章，详细介绍了DNS这门技术。

## dig 域名
DNS 解析器从根域名服务器查找到顶级域名服务器的 IP 地址，又从顶级域名服务器查找到权威域名服务器的 IP 地址，最终从权威域名服务器查出了对应服务的 IP 地址。
```sh
C:\Users\zhuqiqi>dig -t A youdianzhishi.com +trace

; <<>> DiG 9.11.3 <<>> -t A youdianzhishi.com +trace
;; global options: +cmd
.                       30      IN      NS      g.root-servers.net.
.                       30      IN      NS      d.root-servers.net.
.                       30      IN      NS      h.root-servers.net.
.                       30      IN      NS      e.root-servers.net.
.                       30      IN      NS      b.root-servers.net.
.                       30      IN      NS      i.root-servers.net.
.                       30      IN      NS      f.root-servers.net.
.                       30      IN      NS      l.root-servers.net.
.                       30      IN      NS      j.root-servers.net.
.                       30      IN      NS      c.root-servers.net.
.                       30      IN      NS      a.root-servers.net.
.                       30      IN      NS      m.root-servers.net.
.                       30      IN      NS      k.root-servers.net.
;; Received 443 bytes from 172.30.3.22#53(172.30.3.22) in 6 ms

;; Received 58 bytes from 192.33.4.12#53(c.root-servers.net) in 5 ms
```
可以使用 dig 命令来追踪下 youdianzhishi.com 域名对应的 IP 地址是如何被解析出来的，可以看到首先会向预置的 13 个根域名服务器发出请求获取顶级域名的地址。根域名服务器是 DNS 中最高级别的域名服务器，域名的格式从上面返回的结果可以看到是 .root-servers.net，每个根域名服务器中只存储了顶级域服务器的 IP 地址。

在这里，我们获取到了以下的 13 条 NS 记录，也就是 13 台 com. 顶级域名 DNS 服务器：
```sh
com. 172800 IN NS a.gtld-servers.net.
com. 172800 IN NS b.gtld-servers.net.
com. 172800 IN NS c.gtld-servers.net.
com. 172800 IN NS d.gtld-servers.net.
com. 172800 IN NS e.gtld-servers.net.
com. 172800 IN NS f.gtld-servers.net.
com. 172800 IN NS g.gtld-servers.net.
com. 172800 IN NS h.gtld-servers.net.
com. 172800 IN NS i.gtld-servers.net.
com. 172800 IN NS j.gtld-servers.net.
com. 172800 IN NS k.gtld-servers.net.
com. 172800 IN NS l.gtld-servers.net.
com. 172800 IN NS m.gtld-servers.net.
```
当 DNS 解析器从根域名服务器中查询到了顶级域名 .com 服务器的地址之后，就可以访问这些顶级域名服务器其中的一台 g.gtld-servers.net 获取权威 DNS 的服务器的地址了：
```sh
youdianzhishi.com. 172800 IN NS dns21.hichina.com.
youdianzhishi.com. 172800 IN NS dns22.hichina.com.
```
这里的权威 DNS 服务是我在域名提供商进行配置的，当有客户端请求 youdianzhishi.com 域名对应的 IP 地址时，其实会从我使用的 DNS 服务商（阿里云）处请求服务的 IP 地址：
```sh
youdianzhishi.com. 600 IN A 39.106.22.102
;; Received 62 bytes from 120.76.107.40#53(dns21.hichina.com) in 26 ms
```
最终，DNS 解析器从 dns21.hichina.com 服务中获取网站的 IP 地址，浏览器或者其他设备就能够通过该 IP 向服务器获取请求的内容了。

# CoreDNS
到这其实能够发现 DNS 就是一种最早的服务发现手段，虽然服务器 IP 地址可能会变动，但是通过相对不会变动的域名，总是可以找到提供对应服务的服务器。

在微服务架构中，确实存在两种常见的服务注册和发现机制：配置管理中心（如 Zookeeper 和 etcd）和 DNS 服务。让我们来简单地介绍一下它们各自的工作方式和优缺点：
1. 配置管理中心（例如 Zookeeper、etcd、Consul）：
   - 这些系统通常具备分布式的、一致性的键值存储，用于存储和管理服务的元数据，包括服务的位置（IP地址和端口）、健康检查信息、配置设置等。
   - 服务在启动时将自己的信息注册到配置管理中心，消费者服务通过查询配置管理中心来发现服务的当前位置。
   - 这些工具通常提供服务健康检测、故障转移、服务同步、分布式锁等高级功能。
   - 它们通常对网络分区和系统故障有很好的容忍度，并能提供一致性保证。
   - 缺点可能包括运维复杂性高，因为需要维护另一个分布式系统，以及可能的性能瓶颈，因为所有服务注册和发现操作都要通过它来完成。

2. DNS 服务（例如 CoreDNS、SkyDNS）：
   - DNS 服务注册机制相对简单。服务通过DNS记录（通常是A记录或SRV记录）注册自己的存在和网络位置。
   - 当服务消费者需要连接到某个服务时，它会进行DNS查询来解析服务的实际地址。
   - DNS服务通常易于集成和使用，大多数语言和环境都内置了DNS解析功能，且基础设施通常已经有了对DNS的支持。
   - DNS缓存可以提高效率，减少对实际DNS服务器的查询次数。
   - 缺点可能包括DNS缓存可能导致服务位置信息过时，以及没有内置的健康检查和服务同步机制。

在选择服务注册和发现机制时，需要考虑到应用场景、系统的复杂性、团队的运维能力、对一致性和容错性的要求等因素。例如，如果系统需要对注册信息的一致性和及时更新有很高的要求，可能会倾向于使用Zookeeper或etcd。如果系统可以容忍一些延迟，并且希望简化运维，使用DNS可能是一个更简单的选择。

由于上面介绍的环境变量这种方式的局限性，我们需要一种更加智能的方案，其实可以思考一种比较理想的方案：那就是可以直接使用 Service 的名称，因为 Service 的名称不会变化，不需要去关心分配的 ClusterIP 的地址，因为这个地址并不是固定不变的，所以如果直接使用 Service 的名字，然后对应 ClusterIP 地址的转换能够自动完成就很好了。名字和 IP 直接的转换是不是和我们平时访问的网站非常类似啊？他们之间的转换功能通过 DNS 就可以解决了，同样的，Kubernetes 也提供了 DNS 的方案来解决上面的服务发现问题。

CoreDNS 其实就是一个 DNS 服务，而 DNS 作为一种常见的服务发现手段，所以很多开源项目以及工程师都会使用 CoreDNS 为集群提供服务发现的功能，Kubernetes 就在集群中使用 CoreDNS 解决服务发现的问题。CoreDNS 是基于 Go 编写的 HTTP/2 Web 服务器 Caddy 构建的，其大多数功能都是由插件来实现的，插件和服务本身都使用了 Caddy 提供的一些功能。现在的 K8s 集群默认直接安装的就是 CoreDNS ：
```sh
[root@master dns]# kubectl get pods -n kube-system -l k8s-app=kube-dns
NAME                       READY   STATUS    RESTARTS       AGE
coredns-7b884d5cb7-lrl72   1/1     Running   2 (5d5h ago)   15d
coredns-7b884d5cb7-nw9hl   1/1     Running   2 (5d5h ago)   15d
```
CoreDNS 内部采用插件机制，所有功能都是插件形式编写，用户也可以扩展自己的插件，以下是 Kubernetes 部署 CoreDNS 时的默认配置：
```sh
➜  ~ kubectl get cm coredns -n kube-system -o yaml
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors # 启用错误记录，错误信息输出到 stdout
        health { # 启用健康检查检查端点，8080:health
            lameduck 5s
        }
        ready # 插件状态报告，8181:ready，可用来做可读性检查
        # 处理 K8s 域名解析，提供集群内服务解析能力，基于服务和 Pod 的 IP 来应答 DNS 查询
        kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure # 为了与 kube-dns 向后兼容，还可以使用 pods verified 选项，可以使得仅在相同名字空间中存在具有匹配 IP 的 Pod 时才返回 A 记录。
            fallthrough in-addr.arpa ip6.arpa
            ttl 30
        }
        prometheus :9153 # 启用 metrics 指标，9153:metrics
        forward . /etc/resolv.conf {
            # 默认情况下任何不属于 Kubernetes 集群内部的域名，其 DNS 请求都将指向 forward 指定的DNS 服务器地址，这里面第一个 “.” 代表所有域名，后面 “/etc/resolv.conf” 表示使用宿主机的域名解析服务器。
            max_concurrent 1000
        }
        cache 30 # 启用缓存
        loop # 环路检测，如果检测到环路，则停止 CoreDNS。
        reload # 允许自动重新加载已更改的 Corefile，编辑 ConfigMap 配置后，需要等待一段时间生效
        loadbalance # 负载均衡，默认 round_robin
    }
kind: ConfigMap
metadata:
  creationTimestamp: "2019-11-08T11:59:49Z"
  name: coredns
  namespace: kube-system
  resourceVersion: "188"
  selfLink: /api/v1/namespaces/kube-system/configmaps/coredns
  uid: 21966186-c2d9-467a-b87f-d061c5c9e4d7
```
- 每个 {} 代表一个 zone，格式是 Zone:port{}，其中 . 代表默认 zone
- {} 内的每个名称代表插件的名称，只有配置的插件才会启用，当解析域名时，会先匹配 zone（都未匹配会执行默认 zone），然后 zone 内的插件从上到下依次执行(这个顺序并不是配置文件内谁在前面的顺序，而是代码 core/dnsserver/zdirectives.go 内的顺序)，
- 对于每个 DNS 查询，CoreDNS 会从上到下依次调用每个配置的插件，直到某个插件能够产生响应或者整个插件链都被执行完毕。当某个插件处理 DNS 查询并生成响应后，该响应通常会直接返回给客户端。如果没有插件能够处理查询，CoreDNS 会返回一个错误响应。如果插件生成了响应，它还可能由之前执行的插件进行后处理。例如，cache 插件可能会缓存响应，供后续相同的查询使用。

## CoreDNS 扩展配置
我们可以根据自己的实际需求，针对不同场景来扩展 CoreDNS 的配置。

### 开启日志服务
如果需将 CoreDNS 每次域名解析的日志打印出来，我们可以开启 Log 插件，只需要在 Corefile 里加上 log 即可，示例配置如下：
```yaml
Corefile: |
  .:53 {
      errors
      log # 启用 log 插件
      health {
          lameduck 15s
      }
      ready
      kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
          ttl 30
      }
      prometheus :9153
      forward . /etc/resolv.conf {
          prefer_udp
      }
      cache 30
      loop
      reload
      loadbalance
  }
```

### 特定域名使用自定义 DNS 服务器
如果 example.com 类型后缀的域名需要使用自建 DNS 服务器（比如 10.10.0.10）进行解析的话，我们可为域名配置一个单独的服务块，示例配置如下：
```yaml
Corefile: |
  .:53 {
      # 省略......
  }
  example.com:53 {
      errors
      cache 30
      forward . 10.10.0.10
      prefer_udp
  }
```

### 自定义 Hosts
如果需要为特定域名指定 hosts 映射，如为 www.example.com 指定 IP 为 127.0.0.1，那么可以使用 Hosts 插件来配置，示例配置如下：
```yaml
Corefile: |
  .:53 {
      errors
      health {
          lameduck 15s
      }
      ready
      hosts { # 使用 hosts 插件
          127.0.0.1 www.example.com
          fallthrough
      }
      kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
          ttl 30
      }
      prometheus :9153
      forward . /etc/resolv.conf {
          prefer_udp
      }
      cache 30
      loop
      reload
      loadbalance
  }
```

### 集群外部访问集群内服务
两种不同的方法来使集群外的进程能够访问集群内的服务：
1. 通过修改 `/etc/resolv.conf` 文件:
   - 将集群外节点的 DNS 解析器 (`nameserver`) 设置为 Kubernetes 集群内的 DNS 服务（如 kube-dns 或 CoreDNS）的 ClusterIP。
   - 这种方法的确可以让集群外的进程解析集群内服务的 DNS 名称，但它具有几个缺点：
     - 它假设集群外节点可以直接访问到 Kubernetes 集群内的网络。
     - 如果集群DNS服务不可用，它会影响到集群外节点的DNS解析能力。
     - 它增加了集群内外网络的耦合，可能带来安全风险。

因此，这种方法通常**不推荐**用于生产环境。

2. 通过内网负载均衡器和云服务商的 DNS 解析服务:
   - 将集群内的服务通过内网负载均衡器（SLB）暴露给集群外。
   - 使用云服务商提供的 DNS 解析服务（例如阿里云的 PrivateZone）来添加指向内网负载均衡器的 A 记录。
   - 这种方法更加稳定和安全，因为它不依赖于集群内的DNS服务，同时也遵循了安全的网络分离原则。

使用云服务商提供的 SLB 和 DNS 服务具有以下优势：
- **安全性**：保持了网络隔离，不会直接暴露集群内部的网络。
- **高可用性**：大多数云服务商的负载均衡器和 DNS 服务都设计为高可用的。
- **易用性**：简化了解决方案的管理和维护，因为配置和运维大多可以通过云服务商的控制台完成。
- **可扩展性**：随着服务流量的增加，可以方便地扩展 SLB 的性能和处理能力。

总结来说，对于云环境下的 Kubernetes 集群，推荐使用云服务商提供的负载均衡和 DNS 解析服务来允许集群外的进程访问集群内的服务。这种方法更加适合生产环境，因为它提供了更好的可靠性、安全性和易用性。

### 统一域名访问服务
我们可以实现在公网、内网和集群内部通过统一域名 foo.example.com 访问你的服务，原理如下：
- 集群内的服务 foo.default.svc.cluster.local 通过公网 SLB 进行了暴露，且有域名 foo.example.com 解析到该公网 SLB 的 IP。
- 集群内服务 foo.default.svc.cluster.local 通过内网 SLB 进行了暴露，且通过云解析 PrivateZone（比如在阿里云）在 VPC 内网中将 foo.example.com 解析到该内网 SLB 的 IP。
- 在集群内部，可以通过 Rewrite 插件将 foo.example.com CNAME 到 foo.default.svc.cluster.local。

示例配置如下：
```yaml
Corefile: |
  .:53 {
      errors
      health {
        lameduck 15s
      }
      ready
      rewrite stop {
        name regex foo.example.com foo.default.svc.cluster.local
        answer name foo.default.svc.cluster.local foo.example.com
      }
      kubernetes cluster.local in-addr.arpa ip6.arpa {
        pods insecure
        fallthrough in-addr.arpa ip6.arpa
        ttl 30
      }
      prometheus :9153
      forward . /etc/resolv.conf {
        prefer_udp
      }
      cache 30
      loop
      reload
      loadbalance
  }
```

### 禁止对 IPv6 类型的 AAAA 记录查询返回
如果 K8s 节点没有禁用 IPV6 的话，容器内进程请求 CoreDNS 时的默认行为是同时发起 IPV4 和 IPV6 解析，而通常我们只需要用到 IPV4，当容器请求某个域名时，CoreDNS 解析不到 IPV6 记录，就会 forward 到 upstream 去解析，如果到 upstream 需要经过较长时间(比如跨公网，跨机房专线)，就会拖慢整个解析流程的速度，业务层面就会感知 DNS 解析慢。

CoreDNS 有一个 template 的插件，可以用它来禁用 IPV6 的解析，只需要给 CoreDNS 加上如下的配置:
```yaml
Corefile: |
  .:53 {
      errors
      health {
          lameduck 15s
      }
      template IN AAAA .
      # ......
  }
```
该配置的含义是在 CoreDNS 中将 AAAA 记录类型拦截，返回域名不存在，以减少不必要的网络通信。

## 使用
CoreDNS 的 Service 地址一般情况下是固定的，类似于 kubernetes 这个 Service 地址一般就是第一个 IP 地址 10.96.0.1，CoreDNS 的 Service 地址就是 10.96.0.10，该 IP 被分配后，kubelet 会将使用 `--cluster-dns=<dns-service-ip>` 参数配置的 DNS 传递给每个容器。DNS 名称也需要域名，本地域可以使用参数`--cluster-domain = <default-local-domain>` 在 kubelet 中配置：
```sh

```
我们前面说了如果我们建立的 Service 如果支持域名形式进行解析，就可以解决我们的服务发现的功能，那么利用 CoreDNS 可以将 Service 生成怎样的 DNS 记录呢？
- 普通的 Service：会生成 servicename.namespace.svc.cluster.local 的域名，会解析到 Service 对应的 ClusterIP 上，在 Pod 之间的调用可以简写成 servicename.namespace，如果处于同一个命名空间下面，甚至可以只写成 servicename 即可访问
- Headless Service：无头服务，就是把 clusterIP 设置为 None 的，会被解析为指定 Pod 的 IP 列表，同样还可以通过 podname.servicename.namespace.svc.cluster.local 访问到具体的某一个 Pod。

接下来我们来使用一个简单 Pod 来测试下 CoreDNS 的使用。
```sh

```
直接应用上面的资源清单即可：
```sh

```
一旦 Pod 处于运行状态，我们就可以在该环境里执行 nslookup 命令，如果你看到类似下列的内容，则表示 DNS 是正常运行的。
```sh

```
如果 nslookup 命令执行失败，可以先检查下本地的 DNS 配置，查看 resolv.conf 文件的内容：
```sh

```
正常该文件内容如下所示：
```sh

```
可以看到 nameserver 的地址 10.96.0.10，该 IP 地址即是在安装 CoreDNS 插件的时候集群分配的一个固定的 IP 地址，我们可以通过下面的命令进行查看：
```sh

```
也就是说我们这个 Pod 现在默认的 nameserver 就是 kube-dns 的地址，现在我们来访问下前面我们创建的 nginx-service 服务，先在 Pod 中添加 wget 命令：
```sh

```
然后我们去访问下 nginx-service 服务：
```sh

```
但是上面我们使用 wget 命令去访问 nginx-service 服务域名的时候被 hang 住了，没有得到期望的结果，这是因为上面我们建立 Service 的时候暴露的端口是 5000：
```sh

```
加上 5000 端口，就正常访问到服务，再试一试访问：nginx-service.default.svc、nginx-service.default、nginx-service，不出意外这些域名都可以正常访问到期望的结果。

到这里我们是不是就实现了在集群内部通过 Service 的域名形式进行互相通信了，大家下去试着看看访问不同 namespace 下面的服务呢？

我们再来总结一下，在 Kubernetes 集群内部，域名的全称是 servicename.namespace.svc.cluster.local，服务名就是 K8s 的 Service 名称，比如当我们执行 curl s 的时候，必须就有一个名为 s 的 Service 对象，在容器内部，会根据 /etc/resolve.conf 进行解析，使用 nameserver 10.96.0.10 进行解析，然后用字符串 s 依次带入 search 域进行查找，分别是：s.default.svc.cluster.local -> s.svc.cluster.local -> s.cluster.local，直到找到为止。

所以，我们执行 curl s，或者执行 curl s.default，都可以完成 DNS 请求，这 2 个不同的操作，会分别进行不同的 DNS 查找步骤：
```sh

```
那么问题来了，curl s 要比 curl s.default 效率高吗？

答案是肯定的，因为 curl s.default 多经过了一次 DNS 查询，当执行 curl s.default，也就使用了带有命名空间的内部域名时，容器的第一个 DNS 请求是：
```sh

```
当请求不到 DNS 结果时，使用下面的搜索域
```sh

```
进行请求，此时才可以得到正确的 DNS 解析。
### ndots

### 补充说明
我们这里是给 CoreDNS 添加的一个 log 插件来观察的解析日志，在实际的工作中可能我们可能也需要对一些应用进行抓包观察，可能单纯通过日志没办法获取相关信息，而该应用镜像中又不包含抓包需要的 tcpdump 命令，这个时候应该如何来进行抓包呢？

我们这里还是以 CoreDNS 为例，要抓去 DNS 容器的包，就需要先进入到该容器的网络中去。
```sh

```
比如 coredns-565d847f94-kc6qc Pod 位于节点 demo-control-plane，我们可以先进入该节点，使用 crictl 命令找到该 Pod 对应的容器 ID：
```sh

```
我们这里对应的容器 ID 就是 68ab2eb5bbf1b，通过下面的命令找到它的 Pid：
```sh

```
然后进入该容器的网络 Namespace 中去：
```sh

```
然后使用 tcpdump 命令抓取 DNS 网络包：
```sh

```
同样在上面的容器中进行 youdianzhishi.com 域名查找：
```sh

```
此时就可以看到对应的数据包：
```sh

```
当然也可以利用 kubectl-debug 这样的工具进行辅助调试，但是进入容器网络命名空间进行抓包是最基本的知识点，需要我们掌握。

# Pod 的 DNS 策略

# Pod 的 DNS 配置